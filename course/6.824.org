#+title: 6.824

#+EXPORT_FILE_NAME: ../latex/6.824/6.824.tex
#+LATEX_HEADER: \graphicspath{{../../books/}}
#+LATEX_HEADER: \input{../preamble.tex}
#+LATEX_HEADER: \makeindex
#+LATEX_HEADER: \usepackage{minted}

* map reduce

** programming model
    the computation takes a set of *input* key/value pairs, and produces a set of *output* key/value
    pairs. The user of the MapReduce library expresses the computation as two functions: *Map* and
    *Reduce*

    *Map*, written by the user, takes an input pair and produces a set of *intermediate* key/value
    pairs. The MapReduce library groups together all intermediate values associated with the same
    intermediate key \(I\) and passes them to the *Reduce* function

    The *Reduce* function, also written by the user, accepts an intermediate key \(I\)  and a set of
    values for that key. It merges together these values to form a possibly smaller set of values.
    Typically just zero or one output value is produced per Reduce invocation.

*** example
#+begin_src python
map(String key, String value):
  // key: document name
  // value: document contents
  for each word w in value:
    EmitIntermediate(w, "1")

reduce(String key, Iterator values):
  // key: a word
  // values: a list of counts
  int result = 0;
  for each v in values:
    results += ParseInt(v)
  Emit(AsString(result))
#+end_src
    The ~map~ function emits each word plus an associated count of occurrences. The ~reduce~ function
    sums together all counts emitted for a particular word

*** Types
    \begin{alignat*}{3}
    &\text{map}&&\texttt{(k1,v1)}&&\to\texttt{list(k2,v2)}\\
    &\text{reduce}\quad&&\texttt{(k2,list(v2))}&&\to\texttt{list(v2)}\\
    \end{alignat*}

*** More examples
    *Distributed Grep*: the map function emits a line if it matches a supplied pattern. The reduce
    function is an identity function that just copies the supplied intermediate data to the output

    *Count of URL Access Frequency*: the map function processes logs of web page requests an outputs
    \(\la\texttt{URL,1}\ra\). The reduce function adds together all values for the same URL and emits
    a \(\la\texttt{URL,total count}\ra\) pair

    *Term-vector per Host*: A term vector summarizes the most important words that occur in a document
     or a set of documents as a list of \(\la word,frequency\ra\) pairs. The map function emits a
     \(\la\texttt{hostname,term vector}\ra\) pair for each input document. The reduce function is passed
     all per-document term vectors for a given host. It add these term vectors together, throwing
     away infrequent terms, and then emits a final \(\la\texttt{hostname,term vector}\ra\) pair

** Implementation
*** Execution Overview
    The /Map/ invocations are distributed across multiple machines by automatically partitioning the
    input data into a set of \(M\) /splits/. The input splits can be processed in parallel by
    different machines. /Reduce/ invocations are distributed by partitioning the intermediate key
    space into \(R\) pieces using a partitioning function (e.g., \(hash(key)\mod R\)). The number of
    partitions and the partitioning function are specified by the user

    #+ATTR_LATEX: :width \textwidth
    #+NAME:
    #+CAPTION:
    [[../images/6.824/1.png]]

    When the user program calls the ~MapReduce~ function, the following sequence of actions occurs
    1. the MapReduce library in the user program first splits the input files into \(M\) pieces and
       starts up many copies of the program on a cluster of machines
    2. one of the copies of the program is special - the master. The rest are workers that are
       assigned work by the master. there are \(M\) map tasks and \(R\) reduce tasks to assign. The
       master picks idle workers and assigns each one a map task or a reduce task
    3. a worker who is assigned a map task reads the contents of the corresponding input split. It
       parses key/value pairs out of the input data and passes each pair to the user-defined /Map/
       function. The intermediate key/value pairs produced by the /Map/ function are buffered in memory
    4. periodically, the buffered pairs are written to local disk, partitioned into \(R\) regions by
       the partitioning function. The locations of these buffered pairs on the local disk are passed
       back to the master, who is responsible for forwarding these locations to the reduce workers
    5. when a reduce worker is notified by the master about these locations, it uses remote
       procedure calls to read the buffered data from the local disks of the map workers. When a
       reduce worker has read all intermediate data, it *sorts* it by the intermediate keys so that
       all occurrences of the same key are grouped together. The sorting is needed because typically
       many different keys map to the same reduce task. If the amount of intermediate data is too
       large to fit in memory, an external sort is used
    6. the reduce worker iterates over the sorted intermediate data and for each unique intermediate
       key encountered, it passes the key and the corresponding set of intermediate values to the
       user's /Reduce/ function. The output of the /Reduce/ function is appended to a final output file
       for this reduce partition
    7. when all map tasks and reduce tasks have been completed, the master wakes up the user
       program. At this point, the /MapReduce/ call in the user program returns back the user code
*** Master data structures
    for each map task and reduce task, it stores the state (/idle/, /in-progress/, or /completed/),
    identity of the worker machine.

    For each completed map task, the master stores the locations and sizes of the \(R\) intermediate
    file regions produced by the map task. Updates to this location and size information are
    received as map tasks are completed
*** Fault tolerance
**** worker failure
    The master pings every worker periodically. If no response is received from a worker in a
    certain amount of time, the master marks the worker as failed.

    Completed map tasks are re-executed on a failure because their output is stored on the local
    disk(s) of the failed machine and is therefore inaccessible
**** master failure
    It is easy to make the master write periodic checkpoints of the master data structures described
    above.
**** Semantics in the presence of failures
* Raft paper
    Raft is a consensus algorithm for managing a replicated log.
** Introduction
    Consensus algorithms allow a collection of machines to work as a coherent group that can survive
    the failures of some of its members
** Replicated state machines
    Replicated state machines are typically implemented using a replicated log, as shown in figure
    #+ATTR_LATEX: :width .7\textwidth
    #+NAME:
    #+CAPTION:
    [[../images/6.824/2.png]]
    Each server stores a log containing a series of commands, which its state machine executes in
    order. Each log contains the same commands in the same order, so each state machine processes
    the same sequence of commands. Since the state machines are deterministic, each computes the
    same state and the same sequence of outputs. Once commands are properly replicated, each
    serverâ€™s state machine processes them in log order, and the outputs are returned to clients.

    Consensus algorithms for practical systems typically have the following properties:
    * they ensure *safety* (never returning an incorrect result) under all non-Byzantine conditions,
      including network delays, partitions, and packet loss, duplication, and reordering
    * they are fully functional as long as any majority of the servers are operational and can
      communicate with each other and with clients. Thus a typical cluster of five servers can
      tolerate the failure of any two servers
    * they do not depend on timing to ensure the consistency of the logs
    * in the common case, a command can complete as soon as a majority of the cluster has responded
      to a single round of remote procedure calls
** The Raft consensus algorithm
    [[https://eli.thegreenplace.net/2020/implementing-raft-part-3-persistence-and-optimizations/][link]]

    #+NAME:
    #+CAPTION:
    [[../images/6.824/3.pdf]]


    Raft implements consensus by first electing a distinguished *leader*, then giving the leader
    complete responsibility for managing the replicated log. The leader accepts log entries from
    clients, replicates them on other servers, and tell servers when it is safe to apply log entries
    to their state machines.

    Given the leader approach, Raft decomposes the consensus problem into three relatively
    independent subproblems:
    * *leader election*
    * *log replication*
    * *safty* : the key safety property for Raft is the State Machine Safety Property: if any server
      has applied a particular log entry to its state machine, then no other server may apply a
      different command for the same log index
      * *Election Safety*: at most one leader can be elected
      * *Leader Append-Only*: a leader never overwrites or deletes entries in its log; it only append
        new entries
      * *Log Matching*: if two logs contain an entry with the same index and term, then logs are
        identical in all entries up through the given index
      * *Leader Completeness*: if a log entry is commited in a given term, then that entry will be
        present in the logs of the leaders for all higher-numbered terms
      * *State Machine Safety*: if a server has applied a log entry at a given index to its state
        machine, no other server will ever apply a different log entry for the same index
*** Raft basics
    At any given time each server is in one of three states: *leader*, *follower* or *candidate*. In
    normal operation there is exactly one leader and all of the other servers are followers.
    * Followers are passive: they issue no requests on their own but simply respond to requests from
      leaders and candidates.
    * The leader handles all client requests (if a client contacts a follower, the follower
      redirects it to the leader)
    * Candidate is used to elect a new leader
    #+ATTR_LATEX: :width .7\textwidth
    #+NAME:
    #+CAPTION:
    [[../images/6.824/4.png]]

    Raft divides time into *terms* of arbitrary length:
    #+ATTR_LATEX: :width .7\textwidth
    #+NAME:
    #+CAPTION:
    [[../images/6.824/5.png]]
    Terms are numbered with consecutive integers. Each term begins with an *election*, in which one or
    more candidates attempt to become leader. If a candidate wins the election, then it serves as
    leader for the rest of the term. In some situations an election will result in a split vote, in
    this case the term will end with no leader; a new term (with a new election) will begin shortly.
    Raft ensures that there is at most one leader in a given term

    Different severs may observe the transitions between terms at different times, and in some
    situations a server may not observe an election or even entire terms. Terms act as a logical
    clock in Raft, and they allow servers to detect obsolete information such as stable leaders.

    Each server stores a *current term* number, which increases monotonically over time. Current terms
    are exchanged whenever servers communicate; if one server's current term is smaller than the
    other's, then it updates its current term to the larger value. If a candidate or leader
    discovers that its term is out of data, it immediately reverts to follower state. If a server
    receives a request with a stale term number, it rejects the request.

    Raft servers communicate using remote procedure calls(RPCs), and the basic consensus algorithm
    requires only two types of RPCs. RequestVote RPCs are initiated by candidates during elections,
    and AppendEntries RPCs are initiated by leaders to replicate log entries and to provide a form
    of heartbeat
*** Leader Election
    Raft uses a heartbeat mechanism to trigger leader election.

    When servers start up, they begin as followers. A server remains in follower state as long as it
    receives valid RPCs from a leader or candidate. Leaders send periodic heartbeats (AppendEntries
    RPCs that carry no log entries) to all followers in order to maintain their authority. If a
    follower receives no communication over a period of time called the *election timeout*, then it
    assumes there is no viable leader and begins an election to choose a new leader

    To begin an election, a follower increments its current term and transitions to candidate state.
    It then votes for itself and issues RequestVote RPCs *in parallel* to each of the other servers in
    the cluster. A candidate continues in this state until one of three things happens:
    1. it wins the election
    2. another server establishes itself as leader
    3. a period of time goes by with no winner

    A candidate wins an election if it receives votes from a majority of the servers in the full
    cluster for the same term. Each server will vote for at most one candidate in a given term, on a
    first-come-first-served basis (5.4 adds an additional restriction on votes). The majority rules
    ensures that at most one candidate can win the election for a particular term (the *Election*
    *Safe Property*) Once a candidate wins the election, it becomes leader. It then sends heartbeat
    messages to all of the other servers to establish its authority and prevent new elections

    While waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming
    to be leader. If the leader's term is at least as large as the candidate's current term, then
    the candidate recognizes the leader as legitimate and returns to follower state. If the term in
    the RPC is smaller than the candidate's current term, then the candidate rejects the RPC and
    continues in candidate state.

    The third possible outcome is that a candidate neither wins nor loses the election: if many
    followers become candidates at the same time, votes could be split so that no candidate obtains
    a majority. When this happens, each candidate will time out and start a new election by
    incrementing its term and initiating another round of RequestVote RPCs. However, without extra
    measures split votes could repeat indefinitely.

    Raft uses randomized election timeouts to ensure that split votes are rare and that they are
    resolved quickly. To prevent split votes in the first place, election timeouts are chosen
    randomly from a fixed interval (e.g., 150-300ms). This spreads out the servers so that in most
    cases only a single server will time out; it wins the election and sends single server will time
    out.

    The same mechanism is used to handle split votes. Each candidate restarts its randomized
    election timeout at the start of an election, and it waits for that timeout to elapse before
    starting the next election;
*** Log replication [15/15]

    Check [[https://stackoverflow.com/questions/46376293/what-is-lastapplied-and-matchindex-in-raft-protocol-for-volatile-state-in-server][this]] for some explanation.

    Each client request contains a command to be executed by the replicated state machines.
    1. [X] The leader appends the command to its log as a new entry, then issues AppendEntries RPCs in
       parallel to each of the other servers to replicate the entry.
    2. [X] When the entry has been safely replicated, the leader applies the entry to its state machine
       and returns the result of that execution to the client.
    3. [X] If followers crash or run slowly, or if network packets are lost, the leader retries
       AppendEntries RPCs indefinitely (even after it has responded to the client) until all
       followers
    eventually store all log entries.

    Los are organized as below
    #+ATTR_LATEX: :width .7\textwidth
    #+NAME:
    #+CAPTION:
    [[../images/6.824/6.png]]
    Each log entry stores a state machine command along with the term number when the entry was
    received by the leader. The term numbers in log entries are used to detect inconsistencies
    between logs and to ensure some of the properties. Each log entry also has an integer index
    identifying its position in the log

    The leader decides when it is safe to apply a log entry to the state machines; such an entry is
    called *committed*.
    * [X] A log entry is committed once the leader that created the entry has replicated it on a
      majority of the servers. This also commits all preceding entries in the leader's log,
      including entries created by previous leaders.
    * [X] The leader keeps track of the highest index it knows to be committed, and it includes that
      index in future AppendEntries RPCs (including heartbeats) so that the other servers eventually
      find out.
    * [X] Once a follower learns that a log entry is committed, it applies the entry to its local state machine

    Raft maintains the following properties, which together constitute the Log Machine Property:
    * If two entries in different logs have the same index and term, then they store the same command
    * If two entries in different logs have the same index and term, then the logs are identical in
      all preceding entries

    The first property follows from the fact that a leader creates at most one entry with a given
    log index in a given term, and log entries never change their position in the log.

    The second property is guaranteed by a simple consistency check performed by AppendEntries.
    1. [X] When sending an AppendEntries RPC, the leader includes the index and the term in its log that
       immediately precedes the new entries.
    2. [X] If the follower does not find an entry in its log with the same index and term, then it
       refuses the new entries
    The consistency check acts as an induction

    During normal operation, the logs of the leader and followers stay consistent, so the
    AppendEntries consistency check never fails.

    However, leader crashes can leave the log inconsistent,
    #+ATTR_LATEX: :width .7\textwidth
    #+NAME:
    #+CAPTION:
    [[../images/6.824/7.png]]

    In Raft, the leader handles inconsistencies by forcing the follower' log to duplicate its own.
    This means that conflicting entries in follower logs will be overwritten with entries from the
    leader's log

    To bring a follower's log into consistency with its own, the leader must
    1. [X] find the latest log entry where the two logs agree
    2. [X] delete any entries in the follower's log after that point
    3. [X] send the follower all of the leader's entries after that point
    All of these actions happen in response to the consistency check performed by AppendEntries
    RPCs.

    The leader maintains a ~nextIndex~ for each follower, which is the index of the next log entry the
    leader will send to that follower
    1. [X] When a leader first comes to power, it initializes all ~nextIndex~ values to the index just
      after the last one in its log (11 in figure)
    2. [X] If a follower's log is inconsistent with the leader's, the AppendEntries consistency check
      will fail in the next AppendEntries RPC.
    3. [X] After a rejection, the leader decrements ~nextIndex~ and retries the AppendEntries RPC.
      Eventually ~nextIndex~ will reach a point where the leader and follower logs match.
    4. [X] When this happens, AppendEntries will succeed, which removes any conflicting entries in the
      follower's log and appends entries from the leader's log.

    Once AppendEntries succeeds, the follower's log is consistent with the leader's, and it will
    remain that way for the rest of the term

    If desired, the protocol can be optimized to reduce the number of rejected AppendEntries RPCs.
    * When rejecting an AppendEntries request, the follower can include the term of the conflicting
      entry and the first index it stores for that term. With this information, the leader can
      decrement ~nextIndex~ to bypass all of the conflicting entries in that term
    * One AppendEntries RPC will be required for each term with conflicting entries, rather than one
      RPC per entry
** Safety
*** election restriction
    Raft uses the voting process to prevent a candidate from winning an election unless its log
    contains all committed entries.

    A candidate must contact a majority of the cluster in order to be elected, which means that
    every committed entry must be present in at least one of those servers. If the candidate's log
    is at least as up-to-date as any other log in that majority, then it will hold all the committed
    entries.

    The RequestVote RPC implements this restriction: the RPC includes information about candidate's
    log, and the voter denies its vote if its own log is more up-to-date than that of the candidate

    Raft determines which of two logs is more up-to-date by comparing the index and term of the last
    entries in the logs.
*** Committing entries from previous terms
    * A leader knows that an entry from its current term is committed once that entry is stored on
       a majority of the serves.
    * If a leader crashes before committing an entry, future leaders will attempt to finish
      replicating the entry.
    * However, a leader cannot immediately conclude that an entry from a previous term is committed
      once it is stored on a majority of servers

    Below is an illustration:
    #+ATTR_LATEX: :width .7\textwidth
    #+NAME:
    #+CAPTION:
    [[../images/6.824/8.png]]

    To eliminate problems like the one in figure, Raft never commits log entries from previous terms
    by counting replicas. Once an entry from the current term has been committed, then all prior
    entries are committed indirectly because of the Log Machting Property
*** Safety argument
    We argue the Leader Completeness Property here (if a log entry is commited in a given term, then
    that entry will be present in the logs of the leaders for all higher-numbered terms)

    We assume that the Leader Completeness Property does not hold, then we prove a contradiction.

    Suppose the leader for term T (leader\textsubscript{T}) commits a log entry from its term, but
    that log entry is not stored by the leader of some future term. Consider the smallest term U>T
    whose leader (leader\textsubscript{U}) does not store the entry
    1. The committed entry must have been absent from leader\textsubscript{U}'s log at the time of
       its election (leaders never delete or overwrite entries)
    2. leader\textsubscript{T} replicated the entry on a majority of the cluster, and
       leader\textsubscript{U} received votes from a majority of the cluster. Thus at least one
       server ("the voter") both accepted the entry from leader\textsubscript{T} and votes for
       leader\textsubscript{U}.
       #+ATTR_LATEX: :width .6\textwidth
       #+NAME:
       #+CAPTION:
       [[../images/6.824/9.png]]
** Log compaction
    In snapshotting, the entire current system state is written to a *snapshot* on stable storage,
    then the entire log up to that point is discarded.

    #+ATTR_LATEX: :width .6\textwidth
    #+NAME:
    #+CAPTION:
    [[../images/6.824/10.png]]

    Figure 12 shows that basic idea of snapshotting in Raft. Each server takes snapshots
    independently, covering just the committed entries in its log. Most of the work consists of the
    state machine writing its current state to the snapshot. Raft also includes a small amount of
    metadata in the snapshot: the *last included index* is the index of the last entry in the log that
    the snapshot replaces (the last entry the state machine had applied), and the *last included term*
    is the term of this entry. These are preserved to support the AppendEntries consistency check
    for the first log entry following the snapshot, since that entry needs a previous log index and
    term.

    Although servers normally take snapshots independently, the leader must occasionally send
    snapshots to followers that lag behind. This happens when the leader has already discarded the
    next log entry that it needs to send to a follower.

    The leader uses a new RPC called InstallSnapshot to send snapshots to followers that are too far
    behind
    #+ATTR_LATEX: :width .6\textwidth
    #+NAME:
    #+CAPTION:
    [[../images/6.824/11.png]]

    This snapshotting approach departs from Raft's strong leader principle, since followers can take
    snapshots without the knowledge of the leader.

    While having a leader helps avoid conflicting decisions in reaching consensus, consensus has
    already been reached when snapshotting, so no decisions conflict. Data still only flows from
    leaders to followers

    One simple strategy is to take a snapshot when the log reaches a fixed size in byte
** Implementing linearizable semantics
    From[[ cite:&ongaro2014consensus]]

    Raft provides at-least-once semantics for clients; the replicated state machine may apply a
    command multiple times: For example, suppose a client submits a command to a leader
    and the leader appends the command to its log and commits the log entry, but then it crashes before
    responding to the client. Since the client receives no acknowledgment, it resubmits the command to
    the new leader, which in turn appends the command as a new entry in its log and also commits this
    new entry.

    To achieve linearizability in Raft, servers must filter out duplicate requests. The basic idea
    is that servers save the results of client operations and use them to skip executing the same
    request multiple times. To implement this, each client is given a unique identifier, and clients
    assign unique serial numbers to every command. Each serverâ€™s state machine maintains a session
    for each client. The session tracks the latest serial number processed for the client, along
    with the associated response. If a server receives a command whose serial number has already
    been executed, it responds immediately without re-executing the request.
* ZooKeeper
    ZooKeeper provides a per client guarantee of FIFO execution of requests and linearizability of
    all requests that change the ZooKeeper state.
** Introduction
    ZooKeeper implements an API that manipulates simple /wait-free/ data objects organized
    hierarchically as in file systems.

    We find that guaranteeing both /FIFO client ordering/ of all operations and /lineariable writes/
    enables an efficient implementation of the service and it is sufficient to implement
    coordination primitives

    To guarantee that update operations satisfy linearizability, we implement a leader-based atomic
    broadcast protocol, called Zab

    Caching data on the client side is an important technique to increase the performance of reads,
    e.g., current leader. ZooKeeper uses a watch mechanism to enable clients to cache data without
    managing the client cache directly

    Summary:
    * *Coordination kernel*: we propose a wait-free coordination service with relaxed consistency
      guarantees for use in distributed systems. In particular, we describe our design and
      implementation of a *coordination kernel*, which we have used in many critical applications to
      implement various coordination techniques
    * *Coordination recipes*: We show how ZooKeeper can be used to build higher level coordination
      primitives, even blocking and strongly consistent primitives, that are often used in
      distributed applications
** The ZooKeeper service
    Clients submit requests to ZooKeeper through a client API using a ZooKeeper client library. In
    addition to exposing the ZooKeeper service interface through the client API, the client library
    also manages the network connections between the client and ZooKeeper servers.

    In this paper, we use *client* to denote a user of the ZooKeeper service, *server* to denote a
    process providing the ZooKeeper service, and *znode* to denote an in-memory data node in the
    ZooKeeper data, which is organized in a hierarchical namespace referred to as the *data tree*. We
    also use the terms update and write to refer to any operation that modifies the state of the
    data tree. Clients establish a *session* when they connect to ZooKeeper and obtain a session
    handle through which they issue requests.
*** Service overview
    To refer to a given znode, we use the standard UNIX notation for file system paths. For example,
    we use ~/A/B/C~ to denote the path to znode ~C~. All znodes store data, and all znodes, except for
    ephemeral znodes, can have children

    There are two types of znodes that a client can create
    * *Regular*: Clients manipulate regular znodes by creating and deleting them explicitly
    * *Ephemeral*: Clients create such znodes, and they either delete them explicitly, or let the
      system remove them automatically when the session that creates them terminates

    Additionally, when creating a new znode, a client can set a *sequential* flag. Nodes created with
    the sequential flag set have the value of a monotonically increasing counter appended to its
    name. If \(n\) is the new znode and \(p\) is the parent znode
