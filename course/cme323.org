#+title: Distributed Algorithms and Optimizations
#+AUTHOR: Reza Zadeh

#+EXPORT_FILE_NAME: ../latex/cme323/cme323.tex
#+LATEX_HEADER: \graphicspath{{../../books/}}
#+LATEX_HEADER: \input{../preamble.tex}
#+LATEX_HEADER: \makeindex
#+LATEX_HEADER: \DeclareMathOperator{\SpeedUp}{\texttt{SpeedUp}}
#+LATEX_HEADER: \DeclareMathOperator{\AllPrefixSum}{\texttt{AllPrefixSum}}
#+LATEX_HEADER: \DeclareMathOperator{\PrefixSum}{\texttt{PrefixSum}}
#+LATEX_HEADER: %\DeclareMathOperator{\Pr}{\text{Pr}}

* Overview, Models of Computation, Brent's Theorem
** Parallel RAM model
    In a Parallel RAM (PRAM) Model, we always have multiple processors. But how these processors
    interact with the memory module(s) may have different variants, explained in the caveat below:

    #+BEGIN_center
    when two processors want to access the same location in memory at the same time (whether its
    read or write), we need to come up with a resolution.
    #+END_center

    What type of resolution we come up with dictates what kind of parallel model we use. The
    different ways a PRAM model can be set up is the subset of the following combinations:
    \begin{equation*}
    \{\text{Exclusive,Concurrent}\}\times\{\text{Read, Write}\}
    \end{equation*}

    |                  | Exclusive Read   | Concurrent Read |
    | Exclusive Write  |                  |                 |
    | Concurrent Write | Never considered |                 |

    The *most popular model* is the concurrent read and exclusive write mode.

    *Resolving Concurrent Writes*. When dealing with concurrent writes, there needs to be a way to
     resolve when multiple processors attempt to write to the same memory cell at the same time.
     Here are the ways to deal with concurrent write:
     * _Undefined/Garbage_. Machine could die or results could be garbage
     * _Arbitrary_ There is no predetermined rule on which processor gets write-priority. For example,
       if \(p_1,\dots,p_j\) all try to write to same location, we randomly select arbitrarily *exactly one*
       of them to give reference to
     * _Priority_ There is a predetermined rule on which processors gets to write
     * _Combination_ We write a combination of the values being written, e.g., max or logical ~or~ of
       bit-values written.
** Generic PRAM Models
    Below, we depict two examples of PRAM models. On the left, we have a *multi-core* model, where
    multiple processors can access a single shared memory module. This is the model of computation
    used in our phones today. On the right, we have a machine with multiple processors connected to
    multiple memory modules via a bus. This generalizes to a model in which each processor can
    access memory from any of the memory modules, or a model in which each processor has its own
    memory module, which cannot be directly accessed by other processors but instead may be accessed
    indirectly by communicating with the corresponding processor.
    #+ATTR_LATEX: :width .8\textwidth :float nil
    #+NAME: Examples of PRAM
    #+CAPTION:
    [[../images/cme323/1.png]]

    In practice, either *arbitrary* or *garbage* is used.
** Defenestration of bounds on runtime
     In a PRAM, we have to wait for the slowest processor to finish all of its computation before we
     can declare the entire computation to be done. This is known as the *depth* of an algorithm. We
     define
     \begin{align*}
    T_1&=\text{amount of (wall-clock) time algorithm takes on one processor}\\
    T_p&=\text{amount of (wall-clock) time algorithm takes on \(p\) processors}
     \end{align*}
** Practical implications of work and depth
    #+ATTR_LATEX: :options [Work]
    #+BEGIN_definition
    The *work* of an algorithm is defined to be the amount of time required complete all computations
    times the number of processors used.
    #+END_definition

    *Fundamental lower bound \(T_p\)*:
    \begin{equation*}
    \frac{T_1}{p}\le T_p
    \end{equation*}
** Representing algorithms as DAG's
    *Constructing a DAG from an algorithm*: Specifically, each fundamental unit of computation is
    represented by a node. We draw a directed arc from node \(u\) to node \(v\) if
    computation \(u\) is required as an *input* to computation \(v\).

    *Operations in different layers of a DAG can /not/ be computed in parallel*. W.L.O.G., we will
    assume our DAG is a tree, so the levels of the tree are well-defined. Let the root of the tree
    have depth 0. Suppose \(m_i\) denotes the number of operations performed in level \(i\) of the
    DAG. Each of the \(m_i\) operations

    *How an algorithm is executed with an unlimited number of processors* At each level \(i\), if
    there are \(m_i\) operations we may use \(m_i\) processors to compute all results in constant
    time. So with an infinitude of processors, the compute time is given by the depth of the tree.
    We then define *depth* to be
     \begin{equation*}
    T_\infty=\text{depth of computation DAG}
     \end{equation*}
** Brent's theorem
    #+ATTR_LATEX: :options []
    #+BEGIN_theorem
    With \(T_1,T_p,T_\infty\) defined as above, if we assume optimal scheduling, then
    \begin{equation*}
    \frac{T_1}{p}\le T_p\le \frac{T_1}{p}+T_\infty
    \end{equation*}
    #+END_theorem

    #+BEGIN_proof
    On level \(i\) of our DAG, there are \(m_i\) operations. Hence \(T_1=\sum_{i=1}^nm_i\) where \(T_\infty=n\).
    For each level \(i\) of the DAG, the time taken by \(p\) processors is given by
    \begin{equation*}
    T_p^i=\ceil{\frac{m_i}{p}}\le\frac{m_i}{p}+1
    \end{equation*}
    #+END_proof
** Parallel summation
    \(T_1=n\). We may partition the integers in each level, therefore
    \begin{equation*}
    T_p\le\frac{n}{p}+\log_2n
    \end{equation*}
    \(\log_2n\) is just like \(\log_mn\) with some constant \(m\).
* Scalable algorithms, Scheduling, and a glance at All Prefix Sum
** Types of scaling
    Another fundamental quantity to understand is the idea of how much speed-up we can hope to
    achieve given more processors. There are three different types of scalability:(1) Strong
    Scaling, (2) Weak Scaling, and (3) Embarrassingly Parallel

    Let \(T_{1,n}\) denote the run-time on one processor given an input of size \(n\). Suppose we
    have \(p\) processors. We define the speed up of a parallel algorithm as
    \begin{equation*}
    \texttt{SpeedUp}(p,n)=\frac{T_{1,n}}{T_{p,n}}
    \end{equation*}

    #+ATTR_LATEX: :options [Strongly Scalable]
    #+BEGIN_definition
    If \(\texttt{SpeedUp}(p,n)=\Theta(p)\), we say that the algorithm is *strongly scalable*
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_proposition
    The parallel sum of \(n\) numbers on \(p\) processors is strongly scalable
    #+END_proposition

    #+BEGIN_proof
    \(T_1=n\), \(T_\infty=\log_2n\), so \(\texttt{SpeedUp}(p,n)=\frac{n}{\frac{n}{p}+\log_2n}=\Theta(p)\).
    Specifically, both \(n\) and \(p\) are assumed to be going to infinity, but \(p\) grows much
    slower than \(n\), hence
    \begin{equation*}
    \frac{n}{\frac{n}{p}+\log_2n}\ge\frac{n}{\frac{n}{p}+\frac{n}{p}}\ge\frac{p}{2}
    \quad\text{ and }\quad
    \frac{n}{\frac{n}{p}+\log_2n}\le\frac{n}{\frac{n}{p}}=p
    \end{equation*}
    #+END_proof

    Note that we have used Brent's theorem to derive the scaling bounds. But Brent's theorem assumes
    optimal scheduling, which is NP-hard. Fortunately, the existence of a polynomial time constant
    approximation algorithm for optimal scheduling implies that these bounds still hold.

    #+ATTR_LATEX: :options [Weakly Scalable]
    #+BEGIN_definition
    If \(\SpeedUp(p,np)=\Omega(1)\), then our algorithm is *weakly scalable*
    #+END_definition

    This metric characterizes the case where, for each processor we add, we add more data as well.

    #+ATTR_LATEX: :options [Embarrassingly Parallel]
    #+BEGIN_definition
    When the DAG representing an algorithm has 0-depth, the algorithms is said to be *embarrasingly parallel*.
    #+END_definition
** Scheduling
    Given a DAG of computations, at any level in the DAG there are a certain number of computations
    which can be required to execute (at the same time). The number of computations is not necessarily
    equal to the number of processors you have available to you, so you need to decide how to assign
    computations to processorâ€”this is what is referred to as scheduling.
*** Problem definition
    #+ATTR_LATEX: :options []
    #+BEGIN_notation
    We assume that the processors are identical. More formally, we are given \(p\) processors and an
    unordered set of \(n\) jobs with processing times \(J_1,\dots,J_n\in\R\). Say that the final schedule
    for processor \(i\) is defined by a set of indices of jobs assigned to processor \(i\). We call
    this set \(S_i\). The load for processor \(i\) is therefore \(L_i=\sum_{k\in S_i}J_k\). The goal is to
    minimize the *makespan* defined as \(L_{\max}=\max_{i\in\{1,\dots,p\}}L_i\)
    #+END_notation
*** The simple (greedy) algorithm
    Take the jobs one by one and assign each job to the processor that has the least load at that time.
*** Optimality of the greedy approach
    In either of the cases, where jobs have dependencies or must be scheduled online, the problem is
    NP hard. So we use approximation algorithms. We claim that the simple algorithm has an
    *approximation ratio* of 2. For this analysis, we define the optimal makespan to be OPT and try to
    compare the output of the greedy algorithm to this. We also define \(L_{\max}\) as above to be
    the makespan

    #+BEGIN_claim
    Greedy algorithm has an approximation ratio of 2
    #+END_claim

    #+BEGIN_proof
    Obviously, \(OPT\ge\frac{1}{p}\sum_{i=1}^nJ_i\), and \(OPT\ge\max_iJ_i\)

    Now consider running the greedy algorithm and identifying the processor responsible for the
    makespan of the greedy algorithm. Let \(J_t\) be the load of the last job placed on this
    processor. Before the last job was placed on this processor, the load of this processor was
    thus \(L_{\max}-J_t\). Therefore, all other processors /at this time/ must have load at
    least \(L_{\max}-J_t\), i.e., \(L_{\max}-J_t\le L_i'\) for all \(i\). Hence summing the inequality
    over all \(i\)
    \begin{equation*}
    p(L_{\max}-J_t)\le\sum_{i=1}^pL_i'\le\sum_{i=1}^pL_i=\sum_{i=1}^nJ_i
    \end{equation*}

    Therefore
    \begin{equation*}
    L_{\max}\le\frac{1}{p}\sum_{i=1}^nJ_i+J_t\le OPT+OPT=2OPT
    \end{equation*}
    #+END_proof

    *What if we could see in the future?* We note that if we first sort the jobs in descending order
    and assign larger jobs first, we can naively get a 3/2 approximation. If we use the same
    algorithm with a tighter analysis, we get a 4/3 approximation.

    *What's realistic?*
* All Prefix Sum
    Given a list of integers, we want to find the sum of all prefixes of the list, i.e., the
    running sum. We are given an input array \(A\) of size \(n\) elements long. Out output is of
    size \(n+1\) elements long, and its first entry is *always* zero. As an example,
    suppose \(A=[3,5,3,1,6]\), then \(R=\AllPrefixSum(A)=[0,3,8,11,12,18]\)
** Algorithm Design
    \begin{algorithm}
    \caption{Prefix Sum}
    \begin{algorithmic}[1]
    %\State \textbf{Input}: All prefix sum for an array \(A\)
    \If{size of \(A\) is 1}
        \State \textbf{return} only element of \(A\)
    \EndIf
    \State Let \(A'\) be the sum of adjecent pairs
    \State Comput \(R'=\AllPrefixSum(A')\)
    \State Fill in missing entries of \(R'\) using another \(\frac{n}{2}\) processors
    \end{algorithmic}
    \end{algorithm}

    The general idea is that we first take the sums of adjacent pairs of \(A\). So the size
    of \(A'\) is exactly half the size of \(A\).

    To compute the running sum for elements whose index is of odd parity in \(A\), i.e., set
    \begin{equation*}
    r_i=r_{i-1}=a_i
    \end{equation*}
    for \(i=1,3,5,\dots\) where we by convention let \(r_0=0\)
**  Algorithm Analysis
    *Pairing entries*: in line 5, where we let \(A'\) be the sum of adjecent pairs, we must
    perform \(n/2\) summations, hence work is \(O(n)\).

    *Recursive call*: Line 6 is our recursive call, which is fed an input of half the size of \(A\).

    *Filling in missing entries*: In line 7, filling in missing entries, we can assign each of
     the \(n/2\) missing entries of \(R\) to a processor and compute its corresponding value in
     constant time. Hence line 7 has work \(n/2\), and depth \(O(1)\).

     *Total work and depth*: Let \(T_1=W(n)\), and \(T_\infty=D(n)\),
     \begin{align*}
    W(n)&=W(n/2)+O(n)\Rightarrow W(n)=O(n)\\
    D(n)&=D(n/2)+O(1)\Rightarrow D(n)=O(\log(n))
     \end{align*}
** Mergesort
    Suppose we parallelize the algorithm via the obvious divide-and-conquer approach, the work done
    is then
    \begin{equation*}
    W(n)=2(W/n)+O(n)=O(n\log n)
    \end{equation*}
    The depth is
    \begin{equation*}
    D(n)=D(n/2)+O(n)=O(n)
    \end{equation*}
    By Brent's theorem, we have that
    \begin{equation*}
    T_p\le O(n\log n)/p+O(n)
    \end{equation*}
    The bottleneck lies in ~merge~.

    How do we merge \(L\) and \(R\) in parallel?

    *Use binary search to find the rank of an element*: Let's call the output of our algorithm \(M\).
     For an element \(x\in R\), define \(\rank_M(x)\) to be the index of element \(x\) in
     output \(M\). For an such element \(x\in R\), we know how many elements (say \(a\)) in \(R\) come
     before \(x\) since we have sorted \(R\). But we don't know immediately.

     If we know how many elements (say \(b\)) in \(L\) are less than \(x\), then we know we should
     place \(x\) in the \((a+b)^{th}\) position in the merged array \(M\). It remains to find \(b\).
     We can find \(b\) by performing a binary search over \(L\). We perform the symmetric procedure
     for each \(l\in L\), so for a call to ~merge~ on an input of size \(n\), we perform \(n\) binary
     searches, each of which takes \(O(\log n)\) time.
** Parallel merge
    \begin{algorithm}
    \caption{Parallel Merge}
    \begin{algorithmic}[1]
    \State \textbf{Input}: Two sorted arrays \(A,B\) each of length \(n\)
    \State \textbf{Output}: Merged array \(C\), consisting of elements of \(A\) and \(B\) in sorted order
    \For{each \(a\in A\)}
        \State Do a binary search to find where \(a\) would be added into \(B\)
        \State The final rank of \(a\) given by \(\rank_M(a)=\rank_A(a)+\rank_B(a)\)
    \EndFor
    \end{algorithmic}
    \end{algorithm}

    To find the rank of an element \(x\in A\) in another sorted \(B\) requires \(O(\log n)\) work
    using a sequential processor. Hence in total, this parallel merge routine
    requires \(O(n\log n)\) work and \(O(\log n)\) depth.

    Hence when we use ~parallelMerge~ in our ~mergeSort~ algorithm,
    \begin{alignat*}{2}
    W(n)&=2W(n/2)+O(n\log n)&&\Rightarrow W(n)=O(n\log^2n)\\
    D(n)&=D(n/2)+\log n&&\Rightarrow D(n)=O(\log^2n)
    \end{alignat*}
    By Brent's Theorem, we get
    \begin{equation*}
    T_p\le O(n\log^2n)/p+O(\log^2n)
    \end{equation*}
    so for large \(p\) we significantly outperform the naive implementation.
** Motivating Cole's mergesort
    Can we do better than binary sort?

    Let \(L_m\) denote the median index of array \(L\). We then find the corresponding index
    in \(R\) using binary search with logarithmic work. We then observe that all of the elements
    in \(L\) at or below \(L_m\) and all of the elements in \(R\)
    at \(\rank_R(\texttt{value}(L_m))\) are at most the value of \(L\)'s median element. Hence if we
    were to recursively merge-sort the first \(L_m\) elements in \(L\) along with the
    first \(\rank_R(\texttt{value}(L_m))\) elements in \(R\), and correspondingly for the upper
    parts of \(L\) and \(R\), we may simply append the results together to maintain sorted order.
    This leads us to Richard Cole[[cite:&doi:10.1137/0217049]]. He works out all the intricate details
    in this approach nicely to achieve
    \begin{align*}
    W(n)&=O(n\log n)\\
    D(n)&=O(\log n)
    \end{align*}
* Divide and Conquer Recipe, Parallel Selection
** General Divide and Conquer Technique
    \begin{align*}
    T_1&=W(n)=aW(\frac{n}{b})+w\\
    T_\infty&=D(n)=D(\frac{n}{b})+t
    \end{align*}
** Parallel Quick Selection
    Suppose we have a list of unsorted integers, which we know nothing about. We wish to find
    the \(k^{\text{th}}\) largest element of the integers.

    Assume that our input array \(A\) has unique elements.

    *Idea*: From the input list \(A\) pick a value at random called a pivot \(p\). For each item
    in \(A\), put them into one of two sublists \(L\) and \(R\) s.t.:
    \begin{gather*}
    x\in L\Leftrightarrow x<p\\
    y\in R\Leftrightarrow y>p
    \end{gather*}

    Note that the rank of \(p\) is /exactly/ \(\abs{L}\). To find the \(k^{\text{th}}\) largest
    element in \(A\), call it \(z\), note the following:
    #+BEGIN_center
    If \(\abs{L}<k\), then \(z\notin L\), we discard the values in \(L\).

    If \(\abs{L}>k\), then \(z\notin R\). We discard the values in \(R\).
    #+END_center

    #+ATTR_LATEX: :width .7\textwidth :float nil
    #+NAME:
    #+CAPTION:
    [[../images/cme323/2.png]]

    So we say that a *phase* in our algorithm ends as soon as we pick a pivot in the middle half of
    our array. Recognize that in phase \(k\), the array size is at
    most \(n\left( \frac{3}{4} \right)^k\). The maximum number of /phases/ before we can hit a base
    case is given by \(\ceil{\log_{4/3}n}\)
** Analysis - Expected Work
    Let \(X_k\) denote the number of times ~Select~ called with array of input size between
    \begin{equation*}
    n\left( \frac{3}{4} \right)^{k+1}\le\abs{A}<n\left( \frac{3}{4} \right)^k
    \end{equation*}
    Total work is given by the sum of the work done for each \(X_k\) multiplied by the number of
    calls of that size. Realize that total work done during phase \(k\) given by
    \begin{equation*}
    X_k\cdot cn\left( \frac{3}{4} \right)^k
    \end{equation*}
    for some constant \(c\in\R^+\). Total number of phases is \(\ceil{\log_{4/3}n}\). Let \(W\) be a
    random variable describing the total work done. Then
    \begin{equation*}
    W\le\sum_{k=0}^{\ceil{\log_{4/3}n}}\left( X_k\cdot cn\left( \frac{3}{4} \right)^k \right)=
    cn\sum_{k=0}^{\ceil{\log_{4/3}n}}\left( X_k\cdot\left( \frac{3}{4} \right)^k \right)
    \end{equation*}
    We are interested in the expected amount of total work, therefore
    \begin{equation*}
    \E[W]\le cn\sum_{k=0}^{\ceil{\log_{4/3}n}}\E[X_k]\left( \frac{3}{4} \right)^k
    \end{equation*}
    We now analyze \(\E[X_k]\). Recall that
    \begin{equation*}
    \E[X_k]=\sum_{i=0}^\infty i\cdot\Pr(X_k=i)
    \end{equation*}

    Note that
    \begin{equation*}
    \Pr(X_k=i)=\left( \frac{1}{2} \right)^{i-1}\cdot\frac{1}{2}=\frac{1}{2^i}
    \end{equation*}
    Hence
    \begin{equation*}
    \E[X_k]=\sum_{i=0}^\infty\frac{i}{2^i}=2
    \end{equation*}
    Ultimately,
    \begin{equation*}
    \E[W]\le cn\sum_{k=0}^{\ceil{\log_{4/3}n}}\E[X_k]\left( \frac{3}{4} \right)^k\le
    2cn\sum_{k=0}^\infty\left( \frac{3}{4} \right)^k=8cn=O(n)
    \end{equation*}
    So in expectation,
    \begin{equation*}
    T_1=O(n)
    \end{equation*}

    *Applying Markov's Inequality*: We can compute a bound on the probability that our total work
     exceeds a multiple of our expected total work. For example, if we wanted to do analysis that
     our total work will exceed 5 times our expected work:
     \begin{equation*}
    P(\text{Total Work}\ge 5\times E[\text{Total Work}])\le\frac{E[\text{Total Work}]}{5\times E[\text{Total Work}]}=\frac{1}{5}
     \end{equation*}
** Parallelizing our Select Algorithm
    *Constructing L and R with Small Depth*.

    \begin{algorithm}
    \caption{Constructing \(L\)(or \(R\))}
    \begin{algorithmic}[1]
    \State Allocate an empty array of size \(n\), with all value initially 0.
    \State Construct indicator list \(B_L[0,\dots,n-1]\) where \(b_i=1\) if \(a_i<p\)
    \State Compute \textbf{PrefixSum} on \(B_l\)\Comment{\(O(\log n)\) depth}
    \State Create the array \(L\) of size \textbf{PrefixSum}\((B_L[n-1])\)
    \For{i=1,2,\dots,n}
        \If{B[i]=1}
            \State \(L[\textbf{PrefixSum}(B_L[i])]\leftarrow a[i]\)\Comment{Can be done in parallel}
        \EndIf
    \EndFor
    \end{algorithmic}
    \end{algorithm}
    \begin{equation*}
    D(n)=D\left( \frac{n}{4/3} \right)+O(\log n)\Rightarrow D(n)=O(\log^2n)
    \end{equation*}
* Memory Management and (Seemingly) Trivial Operations
    We assume that we can allocate memory in constant time, as long as we don't ask for the meomory
    to have special values in it.  That is, we can request a large chunk of memory (filled with
    garbage bit sequences) in constant time. However, requesting an array of zeros already
    requires \(\Theta(n)\) work since we must ensure the integrity of each entry. In the context of
    sequential algorithms, this is not a concern since reading in an input of \(n\) bits, or
    outputting \(n\) bits already requires \(\Theta(n)\) work, so zeroing out an array of size n does not
    dominate the operation count. However, in some parallel algorithms, no processor reads in the
    entire input, so naively zeroing out a large array can easily dominate the operation time of the
    algorithm.
* QuickSort
** Analysis on Memory Management
    \begin{algorithm}
    \caption{QuickSort}
    \begin{algorithmic}[1]
    \Require An array \(A\)
    \Ensure Sorted A
    \State \(p\leftarrow\) element of \(A\) chosen uniformly at random
    \State \(L\leftarrow[a\mid a\in A,a<p]\)\Comment{Implicitely: \(B_L\leftarrow\bbone\{a_i<p\}^n_{i=1}\), \texttt{prefixSum(\(B_L\))}}
    \State \(R\leftarrow[a\mid a\in A,a>p]\)\Comment{which requires \(\Theta(n)\) work and \(O(\log n)\) depth}
    \State \textbf{Return} [QuickSort(\(L\)), QuickSort(\(R\))]
    \end{algorithmic}
    \end{algorithm}

    We denote the size of our input array \(A\) by \(n\). To be precise, we can perform step 1
    in \(\Theta(\log n)\) work and \(O(1)\) depth. That is, to generate a number uniformly from the
    set \(\{1,\dots,n\}\) we can assign \(\log n\) processors to independently flip a bit "on" with
    probability \(1/2\).

    *Allocating storage for \(L\) and \(R\)*: Start by making a call to the OS to allocate an array
    of \(n\) elements; this requires \(O(1)\) work and depth, since we do not require the elements
    to be initialized. We compare each element in the array with the pivot, \(p\), and write a 1 to
    the corresponding element if the element belongs in \(L\) and a 0 otherwise. This
    requires \(\Theta(n)\) work but can be done in parallel, i.e., \(O(1)\) depth. We are left with an
    array of 1's and 0's indicating whether an element belongs in \(L\) or not, call
    it \(\bbone_L\),
    \begin{equation*}
    \bbone_L=\bbone\{a\in A:a<p\}
    \end{equation*}
    We then apply ~PrefixSum~ on the indicator array \(\bbone_L\), which requires \(O(n)\) work
    and \(O(\log n)\) depth. Then we may examine the value of the last element in the output array
    from ~PrefixSum~ to learn the size of \(L\). Looking up the last element in array \(\bbone_L\)
    requires \(O(1)\) work and depth. We can further allocate a new array for \(L\) in constant time
    and depth. Since we know \(\abs{L}\) and we know \(n\), we also know \(\abs{R}=n-\abs{L}\);
    computing \(\abs{R}\) and allocating corresponding storage requires \(O(1)\) work and depth.

    Thus allocating space for \(L\) and \(R\) requires \(O(n)\) work and \(O(\log n)\) depth.

    *Filling \(L\) and \(R\)*: Now we use \(n\) processors, assigning each to exactly one element in
    our input array \(A\), and in parallel we perform the following steps. Each
    processor \(1,2,\dots,n\) is assigned to its corresponding entry in \(A\).

    Suppose we fix attention to the \(k\)th processor, which is responsible for assigning
    the \(k\)th entry in \(A\) to its appropriate location in either \(L\) and \(R\). We first
    examine \(\bbone_L[k]\) to determine whether the element belongs in \(L\) or \(R\). In addition,
    examine the corresponding entry in ~PrefixSum~ output, denote this value
    by \(i=\PrefixSum(\bbone_L[k])\). If the \(k\)th entry of \(A\) belongs in \(L\), then it may be
    written to the position \(i\) in \(L\) immediately. If the \(k\)th entry instead belongs
    in \(R\), then realize that index \(i\) tells us that exactly \(i\) entries "before"
    element \(k\) belong in \(L\). Hence exactly \(k-i\) elements belong in array \(R\) before
    element.

    The process of filling \(L\) and \(R\) requires \(O(n)\) work and \(O(1)\) depth

    Therefore we need \(O(n)\) work and \(O(\log n)\) depth.
** Total Expected Work
    The level of our computational DAG is \(n\) and there are \(\log_{4/3}n\) levels, therefore
    \begin{equation*}
    \E[T_1]=O(n\log n)
    \end{equation*}

    We define the random indicator variable \(X_{ij}\) to be one if the algorithm /does compare/
    the \(i\)th /smallest/ and the \(j\)th /smallest/ elements of input array \(A\) during the course of
    its sorting routine, and zero otherwise. Let \(X\) denote the /total/ number of comparisons made
    by our algorithm. Then we have that
    \begin{equation*}
    X=\sum_{i=1}^{n-1}\sum_{j=i+1}^nX_{ij}
    \end{equation*}
    The only time in our ~QuickSort~ algorithm whereby we make a comparison of two elements is when we
    construct \(L\) and \(R\). In doing so, we compare each element in the array to a fixed pivot
    of \(p\), after which all elements in \(L\) less than \(p\) and all elements in \(R\) greater
    than \(p\). Realize that pivot \(p\) is never compared to elements in \(L\) and \(R\) for the
    remainder of the algorithm. Hence each of the \(\binom{n}{2}\) pairings are considered /at most/
    once by our ~QuickSort~ algorithm.

    Expectation operator \(\E\) is monotone, hence
    \begin{equation*}
    \E[X]\le\E\left[ \sum_{i=1}^{n-1}\sum_{j=n+1}^nX_{ij} \right]=\sum_{i=1}^{n-1}\sum_{j=i+1}^n\E[X_{ij}]
    \end{equation*}
    where \(\E[X_{ij}]=0\cdot\Pr(X_{ij}=0)+1\cdot\Pr(X_{ij}=1)=\Pr(X_{ij})\)

    Consider any particular \(X_{ij}\) for \(i<j\) (i.e., one of interest). Denote the \(i\)th
    smallest element of input array \(A\) by \(\rank_A^{-1}(i)\) for \(i=1,2,\dots,n\). For any one call
    to ~QuickSort~, there are exactly /three/ cases to consider
    1. \(p\in\{\rank^{-1}_A(i),\rank_A^{-1}(j)\}\). In this case, \(X_{ij}=1\)
    2. \(\rank_A^{-1}(i)<p<\rank_A^{-1}(j)\). \(X_{ij}=0\)
    3. \(p<\rank_A^{-1}(i)\) or \(p>\rank_A^{-1}(j)\): \(X_{ij}\) may be "flipped on" in another
       recursive call to ~QuickSort~


    If we ignore the case 3 temporarily, that is, the condition is 1 or 2, therefore
    \begin{equation*}
    \E[X]=\frac{2}{j-i+1}
    \end{equation*}
    and
    \begin{equation*}
    \E[X]=\sum_{i=1}^{n-1}\sum_{j=i+1}^n\frac{2}{j-i+1}=\sum_{i=1}^{n-1}2\left( \frac{1}{2}+\frac{1}{3}
    +\dots+\frac{1}{n-i+1} \right)
    \end{equation*}
    We know that \(\sum_{i=1}^n\frac{1}{i}<\int_1^n\frac{1}{x}dx\) and \(\int_1^n\frac{1}{x}dx=\ln n\), therefore
    \begin{equation*}
    \E[X]<2n\ln n=O(n\log n)
    \end{equation*}
** Total Expected Depth
    Similar to ~QuickSelect~, we consider the number of /recursive calls/ made to our algorithm when our
    input array is of size
    \begin{equation*}
    \left( \frac{3}{4} \right)^{k+1}n<\abs{A}<\left( \frac{3}{4} \right)^kn
    \end{equation*}
    and denote this by \(X_k\). Then \(\E[X_k]=2\). Hence we need \(2c\log_{4/3}n\) recursive calls
    and \(\E[D(n)]=O(\log^2n)\)
* Matrix Multiplication: Strassen's algorithm
** Idea - Block Matrix Multiplication
    Assume \(A,B\in\R^{n\times n}\) and \(C=AB\), where \(n\) is a power of 2.

    Write
    \begin{equation*}
    A=
    \begin{pmatrix}
    A_{11}&A_{12}\\A_{21}&A_{22}
    \end{pmatrix},\quad
    B=
    \begin{pmatrix}
    B_{11}&B_{12}\\B_{21}&B_{22}
    \end{pmatrix},\quad
    C=
    \begin{pmatrix}
    C_{11}&C_{12}\\C_{21}&C_{22}
    \end{pmatrix}
    \end{equation*}
    where \(A_{ij}\) are of size \(n/2\times n/2\). Then
    \begin{align*}
    &C_{11}=A_{11}B_{11}+A_{12}B_{21}\\
    &C_{12}=A_{11}B_{12}+A_{12}B_{22}\\
    &C_{21}=A_{21}B_{11}+A_{22}B_{21}\\
    &C_{22}=A_{22}B_{12}+A_{22}B_{22}
    \end{align*}
** Parallelizing the Algorithm
    \begin{equation*}
    W(n)=8W(n/2)+O(n^2)
    \end{equation*}
    Then, \(W(n)=O(n^{\log_28})=O(n^3)\)
** Strassen's Algorithm
    Let
    \begin{align*}
    &M_1=(A_{11}+A_{22})(B_{11}+B_{22})\\
    &M_2=(A_{21}+A_{22})B_{11}\\
    &M_3=A_{11}(B_{12}-B_{22})\\
    &M_4=A_{22}(B_{21}-B_{11})\\
    &M_5=(A_{11}+A_{12})B_{22}\\
    &M_6=(A_{21}-A_{11})(B_{11}+B_{12})\\
    &M_7=(A_{12}-A_{22})(B_{21}+B_{22})
    \end{align*}
    Then
    \begin{align*}
    &C_{11}=M_1+M_4-M_5+M_7\\
    &C_{12}=M_3+M_5\\
    &C_{21}=M_2+M_4\\
    &C_{22}=M_1-M_2+M_3+M_6
    \end{align*}
    and
    \begin{align*}
    &W(n)=7W(n/2)+O(n^2)\Rightarrow W(n)=O(n^{\log_27})\\
    &D(n)=D(n/2)+O(1)\Rightarrow D(n)=O(\log n)
    \end{align*}
** Drawbacks of Divide and Conquer
    * We haven't considered communication bottlenecks; in real life communication is expensive
    * Disk/RAM differences are a bottleneck for recursive algorithms
    * PRAM assumes perfect scheduling
* Minimum Spanning Tree Algorithms
    We'll denote our /connected/ and /undirected/ graph by \(G=(V,E,w)\). The size of the vertex
    size \(\abs{V}=n\), the size of the edge set \(\abs{E}=m\), and we assume that the
    weights \(w:E\to\R\) are distinct for convenience. [fn:1]

    A *tree* is an undirected graph \(G\) which satisfies any two of the three following statements,
    such a graph also then satisfies the other property as well
    1. \(G\) connected
    2. \(G\) has no cycles
    3. \(G\) has exactly \(n-1\) edges.

    This *minimum spanning tree* (MST) of \(G\) is \(T^*=(V,E_T,w)\)
    s.t. \(\sum_{e\in E_{T^*}}w(e)\le\sum_{e\in E^T}w(e)\) for any other spanning tree \(T\).
** Sequential approaches and Kruskal's algorithm
    \begin{algorithm}
    \caption{Kruskal's MST algorithm}
    \begin{algorithmic}[1]
    \Require \(G=(V,E,w)\) an undirected and connected graph
    \Ensure \(T^*\), the MST of \(G\)
    \State Sort \(E\) in increasing order by edge weight \(T\)
    \State \(T\leftarrow\emptyset\)
    \While{\(T\) not yet a spanning tree}
        \State \(e\leftarrow\) next edge in the queue
        \If{\(T\cup\{e\}\) contains no cycles}
            \State \(T\leftarrow T\cup\{e\}\)
        \EndIf
    \EndWhile
    \State \textbf{Return} \(T\)
    \end{algorithmic}
    \end{algorithm}
    Kruskal's algorithm is based on the *cut property* (or *red rule*)
    #+ATTR_LATEX: :options [Cur-Property, Red-Rule]
    #+BEGIN_theorem
    Let \(G(V,E,w)\) be an undirected and connected graph. Then the edge with minimum weight leaving
    any cut \(S\subset V\) is in the MST of \(G\)
    #+END_theorem

    #+BEGIN_proof
    We are given a graph \(G\) and a cut \(S\subset V\). We say that an edge is in a cut \(S\) if exactly
    one incident node is in \(S\) and the other incident node of the edge in \(V\setminus S\).
    Let \(e^*=(u,v)\) be the minimum weight edge in a cut \(S\). Assume that the edge with minimum
    weight leaving cut \(S\) is /not/ in the MST \(T\), i.e., \(e^*\notin T\) where \(T\) is the MST
    of \(G\) and
    \begin{equation*}
    e^*=\argmin_{e\in S}w(e)
    \end{equation*}
    Suppose \(e^*=(u,v)\). Since \(T\) is a spanning tree of \(G\), we know that \(u\) and \(v\)
    are connected in the edge set of \(T\). We construct a \(u\)-\(v\) path using depth first
    search. Let \((x,y)=e'\) denote the edge in \(T\) which crosses cut \(S\) in the \(u\)-\(v\)
    path. Replace \((x,y)\) with \((u,v)\). Call the result \(T'\).

    We claim that \(T'=(T\setminus(x,y))\cup(u,v)\) is still a tree. We first claim \(T'\) retains connectivity
    among all nodes. To see this, realize first that since \((x,y)\) and \((u,v)\) each have exactly
    one incident node in \(S\) and one incident node in \(V\setminus S\), then it is /not/ possible that
    either \((x,y)\) or \((u,v)\), when removed from \(T\), could result in \(S\) becoming
    disconnected or \(V\setminus S\) becoming disconnected. Hence, without either of these edges, we can get
    from all nodes in \(S\) to all other nodes in \(S\), and same goes for the set \(V\setminus S\). It
    remains to show that we can still traverse from \(S\) to \(V\setminus S\). Realize that all paths
    previously going from \(S\) to \(V\setminus S\) using \((x,y)\) can now simply utilize \((u,v)\)
    instead.

    Further, realize that we have not created any cycles. Specifically, when we add edge \((u,v)\)
    to the tree \(T\), we include a cycle. But realize that the edge \((x,y)\) is part of the cycle,
    and we immediately remove it.

    Since \(e^*\) is the minimum weight edge in \(S\), and since it is not contained in \(T\), we
    know that \(w(e^*)<w(e')\), but this implies that \(w(T')<w(T)\), a contradiction.
    #+END_proof
** Complexity Analysis for Kruskal's
    *Sorting Edges*: \(\E[W(n)]=O(m\log m)=O(m\log n)\) since \(m\ge\binom{n}{2}\).

    *Checking for Cycles*: We show a way to check cycles in \(O(\log n)\) work that is a bit more
    accessible. We must ensure that if edge \(e=(u,v)\) can be added, then the component of \(u\) is
    /not/ the same component as \(v\). To do this, we keep several data structures around: an array
    and a binary search tree. We assume that with each component of our graph, we associate it with
    a binary search tree storing values of nodes in our graph \(G\) which are in a particular
    component \(k\).

* References
<<bibliographystyle link>>
bibliographystyle:alpha

<<bibliography link>>
bibliography:/Users/wu/notes/references.bib

* Footnotes

[fn:1] If the edge weights are not distinct, then we may simply simply perturb each edge weight by a
small random factor \(\epsilon>0\)  where \(\epsilon<1/n^3\).
