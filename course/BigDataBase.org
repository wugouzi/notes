#+title: Big DataBase
#+EXPORT_FILE_NAME: ../latex/bigdatabase/bigdatabase.tex
#+STARTUP: latexpreview
#+LATEX_HEADER: \graphicspath{{../../books/}}
#+LATEX_HEADER: \input{/Users/wu/notes/preamble.tex}
#+LATEX_HEADER: \DeclareMathOperator{\commit}{\text{commit}}
#+LATEX_HEADER: \DeclareMathOperator{\DT}{\text{DT}}
#+LATEX_HEADER: \DeclareMathOperator{\RF}{\text{RF}}
#+LATEX_HEADER: \DeclareMathOperator{\CP}{\text{CP}}
#+LATEX_HEADER: \DeclareMathOperator{\Gen}{\text{Gen}}
#+LATEX_HEADER: \DeclareMathOperator{\CSR}{\text{CSR}}
#+LATEX_HEADER: \makeindex
* Query Optimization
:PROPERTIES:
:EXPORT_AUTHOR: Thomas Neumann
:END:
** Introduction
    Compile time system:
    1. parsing: parsing, AST production
    2. semantic analysis: schema lookup, variable binding, type inference
    3. normalization, factorization, constant folding
    4. rewrite 1: view resolution, unnesting, deriving predicates
    5. plan generation: constructing the execution plan
    6. rewrite 2: refining the plan, pushing group
    7. code generation: producing the imperative plan

    Different optimization goals:
    * minimize response time
    * minimize resource consumption
    * minimize time to first tuple
    * maximize throughput

    Notation:
    * \(\cala(e)\): attributes of the tuples produces by \(e\)
    * \(\calf(e)\) free variable of the expression \(e\)
    * binary operators \(e_1\theta e_2\) usually require \(\cala(e_1)=\cala(e_2)\)
    * \(\rho_{a\to b(e)}\), rename
    * \(\Pi_A(e)\), projection
    * \(\sigma_p(e)\), selection, \(\{x\mid x\in e\wedge p(x)\}\)
    * \(e_1\bowtie_pe_2\), join, \(\{x\circ y\mid x\in e_1\wedge y\in e_2\wedge p(x\circ y)\}\)


    Different join implementations have different characteristics:
    * \(e_1\bowtie^{NL}e_2\) Nested Loop Join:
    * \(e_1\bowtie^{BNL}e_2\) Blockwise Nested Loop Join: Read chunks of \(e_1\) into memory and
      read \(e_2\) once for each chunk. Further improvement: Use hashing for equi-joins
    * \(e_1\bowtie^{SM}e_2\) Sort Merge Join: Equi-joins only
    * \(e_1\bowtie^{HH}e_2\) Hybrid-Hash Join: Partitions \(e_1\) and \(e_2\) into partitions that
      can be joined in memory. Equi-joins only

** Query Optimization
    steps
    1. translate the query into its canonical algebraic expression
    2. logical query optimization
    3. physical query optimization
*** Algebra Revisited
    Tuple is a (unordered) mapping from attribute names to values of a domain

    Schema is a set of attributes with domain, written \(\cala(t)\)

    concatenation of tuple:
    * \(t_1\circ t_2\), note \(t_1\circ t_2=t_2\circ t_1\)
    * \(\cala(t_1)\cap\cala(t_2)=\emptyset\)
    * \(\cala(t_1\circ t_2)=\cala(t_1)\cup\cala(t_2)\)

    tuple projection:
    * \(t.a\), \(t|_A\)
    * \(a\in\cala(t), A\subseteq\cala(t)\)
    * \(\cala(t|_A)=A\)
    * \(t.a\) produces a value, \(t|_A\) produces a tuple

    Relation is a set of tuples with the same schema. Schema of the contained tuples,
    written \(\cala(R)\)

    Real data is usually a multi set (bag). The optimizar must consider three different semantics:
    * logical algebra operates on bags
    * physical algebra operates on streams
    * explicit duplicate elimination \(\Rightarrow\) sets

    Set operations are part of the algebra:
    * union, intersection, difference
    * but have schema constraints
    * \(\cala(L)=\cala(R)\)
    * \(\cala(L\cup R)=\cala(L)=\cala(R)\), \(\cala(L\cup R)=\cala(L)=\cala(R)\), \(\cala(L\setminus R)=\cala(L)=\cala(R)\)

    \(\calf(e)\) are the free variables of \(e\)

    Selection:
    * \(\sigma_p(R)\)
    * \(\calf(p)\subseteq\cala(R)\)
    * \(\cala(\sigma_p(R))=\cala(R)\)

    Projection:
    * \(\Pi_A(R)\)
    * eliminates duplicates for set semantic, keeps them for bag semantic
    * \(A\subseteq\cala(R)\)
    * \(\cala(\Pi_A(R))=A\)

    Rename:
    * \(\rho_{a\to b}(R)\)
    * \(a\in \cala(R),b\notin\cala(R)\)
    * \(\cala(\rho_{a\to b}(R))=\cala(R)\setminus\{a\}\cup\{b\}\)

      \begin{align*}
      \sigma_{p_1\wedge p_2}&\quad\equiv\quad\sigma_{p_1}(\sigma_{p_2}(e))\tag{1}\\
      \sigma_{p_1}(\sigma_{p_2}(e))&\quad\equiv\quad\sigma_{p_2}(\sigma_{p_1}(e))\tag{2}\\
      \Pi_{A_1}(\Pi_{A_2}(e))&\quad\equiv\quad\Pi_{A_1}(e)\tag{3}\\
      &\quad\equiv\quad\text{if }A_1\subseteq A_2\\
      \sigma_p(\Pi_A(e))&\quad\equiv\quad\Pi_A(\sigma_p(e))\tag{4}\\
      &\quad\equiv\quad\text{if }\calf(p)\subseteq A\\
      \sigma_p(e_1\cup e_2)&\quad\equiv\quad\sigma_p(e_1)\cup\sigma_p(e_2)\tag{5}\\
      \sigma_p(e_1\cap e_2)&\quad\equiv\quad\sigma_p(e_1)\cap\sigma_p(e_2)\tag{6}\\
      \sigma_p(e_1\setminus e_2)&\quad\equiv\quad\sigma_p(e_1)\setminus\sigma_p(e_2)\tag{7}\\
      \Pi_A(e_1\cup e_2)&\quad\equiv\quad\Pi_A(e_1)\cup\Pi_A(e_2)\tag{8}\\
      e_1\times e_2&\quad\equiv\quad e_2\times e_1\tag{9}\\
      e_1\bowtie_pe_2&\quad\equiv\quad e_2\bowtie_pe_1\tag{10}\\
      (e_1\times e_2)\times e_3&\quad\equiv\quad e_1\times(e_2\times e_3)\tag{11}\\
      (e_1\bowtie_{p_1}e_2)\bowtie_{p_2}e_3&\quad\equiv\quad e_1\bowtie_{p_1}(e_2\bowtie_{p_2}e_3)\tag{12}\\
      \sigma_p(e_1\times e_2)&\quad\equiv\quad e_1\bowtie_pe_2\tag{13}\\
      \sigma_p(e_1\times e_2)&\quad\equiv\quad\sigma_p(e_1)\times e_2\tag{14}\\
      &\quad\equiv\quad\text{if }\calf(e)\subseteq\cala(e_1)\\
      \sigma_{p_1}(e_1\bowtie_{p_2}e_2)&\quad\equiv\quad\sigma_{p_1}(e_1)\bowtie_{p_2}e_2\tag{15}\\
      &\quad\equiv\quad\text{if }\calf(p_1)\subseteq\cala(e_1)\\
      \Pi_A(e_1\times e_2)&\quad\equiv\quad\Pi_{A_1}(e_1)\times\Pi_{A_2}(e_2)\tag{16}\\
      &\quad\equiv\quad\text{if }A=A_1\cup A_2, A_1\subseteq\cala(e_1),A_2\subseteq\cala(e_2)
      \end{align*}

*** Canonical Query Translation
    Restrictions:
    * only *select distinct*
    * no *group by, order by, union, intersect, except*
    * only attributes in *select* clause
    * no nested queries
    * not discussed here: ~NULL~ values

*** Logical Query Optimization
    * foundation: algebraic equivalence

    Which plans are better?
    * plans can only be compared if there is a cost function
    * cost functions need details that are not available when only considering logical algebra
    * consequence: logical query optimization remains a heuristic

    Phases
    1. break up conjunctive selection predicates, \((1)\to\)
    2. push selections down, \((2)\to,(14)\to\)
    3. introduce joins, \((13)\to\)
    4. determine join order \((9),(10),(11),(12)\)
    5. introduce and push down projections \((3)\leftarrow,(4)\leftarrow,(16)\to\)
       * eliminate redundant attributes

    This kind of phases has limitation: different join order would allow further push down. The
    phases are interdependent
*** Physical Query Optimization
    * add more execution information to the plan
    * allow for cost calculations
    * select index structures/access paths
      * scan+selection could be done by an index lookup
      * multiple indices to choose from
      * table scan might be the best, even if an index is available
      * depends on selectivity, rule of thumb: 10%
      * detailed statistics and costs required
      * related problem: materialized view
      * even more complex, as more than one operator could be substitued
    * choose operator implementations
      * replace a logical operator (e.g. \(\bowtie\)) with a physical one (e.g. \(\bowtie^{HH}\))
      * semantic restrictions: e.g., most join operators require equi-conditions
      * \(\bowtie^{BNL}\) is better than \(\bowtie^{NL}\)
      * \(\bowtie^{SM}\) and \(\bowtie^{HH}\) are usually better than both
      * \(\bowtie^{HH}\) is often the best if not reusing sorts
      * decision must be cost-based
      * even \(\bowtie^{NL}\) can be optimal
      * not only joins, has to be done for all operators
    * add property enforcer
      * certain physical operators need certain properties
      * example: sort for \(\bowtie^{SM}\)
      * example: in a distributed database, operators need the data locally to operate
      * many operator requirements can be modeled as properties
    * choose when to materialize
      * temp operator stores input on disk
      * essential for multiple consumers (factorization, DAGs)
      * also relevant for \(\bowtie^{NL}\)
** Join Ordering
*** Basics
    Concentrate on join ordering, that is:
    * conjunctive queries
    * simple predicates
    * predicates have the form \(a_1=a_2\) where \(a_1\) is an attribute and \(a_2\) is either an
      attribute or a constant
    * even ignore constants in some algorithms

    We join relations \(R_1,\dots,R_n\) where \(R_i\) can be
    * a base relation
    * a base relation including selections
    * a more complex building block or access path

    Queries of this type can be characterized by their query graph:
    * the query graph is an undirected graph with \(R_1,\dots,R_n\) as nodes
    * a predicate of the form \(a_1=a_2\) where \(a_1\in R_i\) and \(a_2\in R_j\) forms an edge
      between \(R_i\) and \(R_j\) labeled with the predicate
    * a predicate of the form \(a_1=a_2\) where \(a_1\in R_i\) and \(a_2\) is a constant forms a self-edge
      on \(R_i\) labeled with the predicate

    #+ATTR_LATEX: :width .7\textwidth :float nil
    #+NAME:
    #+CAPTION:
    [[../images/db/2.png]]

    #+ATTR_LATEX: :width .8\textwidth :float nil
    #+NAME:
    #+CAPTION: Shapes of Query Graphs
    [[../images/db/3.png]]



    A join tree is a binary tree with
    * join operators as inner nodes
    * relations as leaf nodes

    Commonly used classes of join trees:
    * left-deep tree
    * right-deep tree
    * zigzag tree: at least one input of every join is a relation \(R\)
    * bushy tree:
    The first three are summariezed as *linear trees*

    *Join selectivity*
    * input
      * cardinalities \(\abs{R_i}\)
      * selectivities \(f_{i,j}\): if \(p_{i,j}\) is the join predicate between \(R_i\) and \(R_j\), define
        \begin{equation*}
        f_{i,j}=\frac{R_i\bowtie_{p_{i,j}}R_j}{R_i\times R_j}
        \end{equation*}
    * Calculate: \(\abs{R_i\bowtie_{p_{i,j}}R_j}=f_{i,j}\abs{R_i}\abs{R_j}\)
    * Rational: The selectivity can be computed/estimated easily (ideally)

    Given a join tree \(T\), the result cardinality \(\abs{T}\) can be computed recursively as
    \begin{equation*}
    \abs{T}=
    \begin{cases}
    \abs{R_i}&\text{if $T$ is a leaf }R_i\\
    (\displaystyle\prod_{R_i\in T_1,R_j\in T_2}f_{i,j})\abs{T_1}\abs{T_2}&\text{if }T=T_1\bowtie T_2
    \end{cases}
    \end{equation*}
    assuming independence of the predicates

    Given a join tree \(T\), the cost function \(C_{out}\) is defined as
    \begin{equation*}
    C_{out}(T)=
    \begin{cases}
    0&\text{if $T$ is a leaf }R_i\\
    \abs{T}+C_{out}(T_1)+C_{out}(T_2)\text{if }T=T_1\bowtie T_2
    \end{cases}
    \end{equation*}

    Consider nested loop join (nlj), hash join (hj), and sort merge join (smj),
    [[cite:&10.5555/645913.671481]] proposes
    \begin{align*}
    C_{nlj}(e_1\bowtie_pe_2)&\quad=\quad\abs{e_1}\abs{e_2}\\
    C_{hj}(e_1\bowtie_pe_2)&\quad=\quad h\abs{e_1}\\
    C_{smj}(e_1\bowtie_pe_2)&\quad=\quad\abs{e_1}\log(\abs{e_1})+\abs{e_2}\log(\abs{e_2})
    \end{align*}
    where \(e_i\) are join trees and \(h\) is the average length of the collision chain in the hash table. We
    will assume \(h=1.2\).

    For sequence of join operators \(s=s_1\bowtie\dots\bowtie s_n\)
    \begin{align*}
    C_{nlj}(s)&\quad=\quad\sum_{i=2}^n\abs{s_1\bowtie\dots\bowtie s_{i-1}}\abs{s_i}\\
    C_{hj}(s)&\quad=\quad\sum_{i=2}^nh\abs{s_1\bowtie\dots\bowtie s_{i-1}}\\
    C_{smj}(s)&\quad=\quad\sum_{i=2}^n\abs{s_1\bowtie\dots\bowtie s_{i-1}}\log(\abs{s_1\bowtie\dots\bowtie s_{i-1}})+\sum_{i=2}^n\abs{s_i}\log(\abs{s_i})
    \end{align*}
    #+ATTR_LATEX: :options []
    #+BEGIN_remark
    Note that the aboves cost functions are designed for left-deep trees.
    #+END_remark

    Cost function \(C_{impl}\) is *symmetric* if \(C_{impl}(e_{1}\bowtie^{impl}e_2)=C_{impl}(e_2\bowtie^{impl}e_1)\)

    ASI: adjacent sequence interchange

    Out basic cost functions can be classified as:
    |                | ASI         | \(\neg\)ASI    |
    |----------------+-------------+-------------|
    | symmetric      | \(C_{out}\) | \(C_{smj}\) |
    | \(\neg\)symmetric | \(C_{hj}\)  |             |

*** Search Space
    We distringuish four different dimensions:
    1. query graph class: chain, cycle, star, and clique
    2. join tree structures: left-deep, zig-zag, or bushy
    3. join construction: with or without cross product
    4. cost functions: with or without ASI property
    In total, 48 different join ordering problems

    The number of binary trees with \(n\) leave nodes is given by \(\calc(n-1)\), where \(\calc(n)\) is defined as
    \begin{equation*}
    \calc(n)=
    \begin{cases}
    1&n=0\\
    \sum_{k=0}^n-1\calc(k)\calc(n-k-1)&n>0
    \end{cases}
    \end{equation*}
    It can be written in a closed form as
    \begin{equation*}
    \calc(n)=\frac{1}{n+1}\binom{2n}{n}
    \end{equation*}
    The Catalan numbers grow in the order of \(\Theta(4^n/n^{1.5})\)

    Number of join trees with cross products:
    * left deep/right deep: \(n!\)
    * zig-zag: there are \(n-1\) join operators, and for every left-deep tree, we can derive zig-zag trees by
      exchanging the left and right inputs. Hence, from any left-deep tree for \(n\) relations, we can
      derive \(2^{n-2}\) zig-zag trees. Therefore there exists a total of \(2^{n-2}n!\) zig-zag trees.
    * bushy tree: \(n!\calc(n-1)=\frac{(2n-2)!}{(n-1)!}\)

    *Chain queries, left-deep join trees, no Cartesian product*: let's denote the number of left-deep join trees for a chain
    query \(R_1-\dots-R_n\) as \(f(n)\). \(f(0)=0\), \(f(1)=1\); for \(n>1\), consider adding \(R_n\) to all join
    trees for \(R_1-\dots-R_{n-1}\). Let's denote the position of \(R_{n-1}\) from the bottom with \(k\in[1,n-1]\).
    Then there are \(n-k\) join trees for adding \(R_n\) after \(R_{n-1}\) and one additional tree if \(k=1\)
    as \(R_n\) can be placed before \(R_{n-1}\). What's more, for \(R_{n-1}\) to
    be \(k\), \(R_{n-k}-\dots-R_{n-2}\) must be below it, which is \(f(k-1)\) trees for \(n>1\). Therefore
    \begin{equation*}
    f(n)=1+\sum_{k=1}^{n-1}f(k-1)*(n-k)=2^{n-1}
    \end{equation*}

    *Chain queries, zig-zag join trees, no Cartesian product*: \(2^{n-2}*2^{n-1}=2^{2n-3}\)

    *Chain queries, bushy join trees, no Cartesian product*: Every subtree of the join tree must contain a
    subtrain in order to prevent cross products.
    \begin{equation*}
    f(n)=
    \begin{cases}
    1&n<2\\
    \sum_{k=1}^{n-1}2f(k)f(n-k)&n\ge 2
    \end{cases}=2^{n-1}\calc(n-1)
    \end{equation*}

    *Star queries, no Cartesian product*: \(2*(n-1)!\) possible left-deep join trees
    and \(2*(n-1)!*2^{n-2}=2^{n-1}*(n-1)!\) zig-zag trees

*** Greedy Heuristics
    \begin{algorithm}
    \caption{GreedyJoinOrdering-1($\{R=R_1,\dots,R_n\}$,\(w:R\to\R\))}
    \KwIn{a set of relations to be joined and a weight function}
    \KwOut{a join order \(S\)}
    \(S=\epsilon\)\;
    \(R=\{R_1,\dots,R_n\}\)\;
    \While{\(!\texttt{empty}(R)\)}{
        Let \(k\) be s.t. \(weight(R_k)=\min_{R_i\in R}(weight(R_i))\)\;
        \(R\setminus=R_k\)\;
        \(S\circ=R_k\)\;
    }
    \end{algorithm}

    \begin{algorithm}
    \caption{GreedyJoinOrdering-2($\{R=R_1,\dots,R_n\}$,\(w:R^*\times R\to\R\))}
    \KwIn{a set of relations to be joined and a weight function}
    \KwOut{a join order \(S\)}
    \(S=\epsilon\)\;
    \(R=\{R_1,\dots,R_n\}\)\;
    \While{\(!\texttt{empty}(R)\)}{
        Let \(k\) be s.t. \(weight(S,R_k)=\min_{R_i\in R}(weight(S,R_i))\)\;
        \(R\setminus=R_k\)\;
        \(S\circ=R_k\)\;
    }
    \end{algorithm}

    \begin{algorithm}
    \caption{GreedyJoinOrdering-3($\{R=R_1,\dots,R_n\}$,\(w:R^*\times R\to\R\))}
    \KwIn{a set of relations to be joined and a weight function}
    \KwOut{a join order \(S\)}
    \(S=\epsilon\)\;
    \(R=\{R_1,\dots,R_n\}\)\;
    \For{\(i=1;i\le n;++i\)}{
        \(S=R_i\)\;
        \(R=R\setminus R_i\)\;
        \While{\(!\texttt{empty}(R)\)}{
            Let \(k\) be s.t. \(weight(S,R_k)=\min_{R_i\in R}(weight(S,R_i))\)\;
            \(R\setminus=R_k\)\;
            \(S\circ=R_k\)\;
        }
        \(Solutions+=S\)
    }
    \Return{cheapest in solutions}
    \end{algorithm}

    The above algorithms only generate linear join trees, but Greedy Operator Ordering (GOO) generates bushy
    join trees.

    \begin{algorithm}
    \caption{GOO(\(\{R_1,\dots,R_n\}\))}
    \KwIn{a set of relations to be joined}
    \KwOut{join tree}
    \(Trees:=\{R_1,\dots,R_n\}\)\;
    \While{\(\abs{Trees}!=1\)}{
        find \(T_1,T_j\in Trees\) s.t. \(i\neq j\),  \(\abs{T_i\bowtie T_j}\) is minimal\;
        \quad among all pairs of trees in \(Trees\)\;
        \(Trees\setminus=\{T_i,T_j\}\)\;
        \(Trees+=T_i\bowtie T_j\)\;
    }
    \end{algorithm}
*** IKKBZ
    The most general case for which a polynomial solution is known is charactized by the following features:
    * the query graph must be acyclic
    * no cross products are considered
    * the search space is restricted to left-deep trees
    * the cost function must have the ASI property

    The IKKBZ-algorithm considers only join operators that have a cost function of the form
    \begin{equation*}
    cost(R_i\bowtie R_j)=\abs{R_i}*h_j(\abs{R_i})
    \end{equation*}
    where each \(R_j\) have its own cost function \(h_j\). We denote the set of \(h_j\) by \(H\). Let us
    denote by \(n_i\) the cardinality of the relation \(R_i\).

    The algorithm works as follows. For every relation \(R_k\) it computes the optimal join order under the
    assumption that \(R_k\) is the first relation in the join sequence. The resulting subproblems then
    resemble a job-scheduling problem.

    Given a query graph \(G=(V,E)\) and a starting relation \(R_k\), we construct the directed *precedence
    graph* \(G^p_k=(V_k^p,E_k^p)\) rooted in \(R_k\) as follows:
    1. choose \(R_k\) as the root node of \(G_k^p\), \(V_k^p=\{R_k\}\)
    2. while \(\abs{V_k^p}<\abs{V}\), choose \(R_i\in V\setminus V_k^p\) s.t. \(\exists R_j\in V_k^p:(R_j,R_i)\in E\). Add \(R_i\)
       to \(V_k^p\) and \(R_j\to R_i\) to \(E_k^p\)

    The precedence graph describes the ordering of joins implied by the query graph.

    A sequence \(S=v_1,\dots,v_k\) of nodes conforms to a precedence graph \(G=(V,E)\) if
    1. \(\forall i\in[2,k]\exists j\in[1,i):(v_j,v_i)\in E\)
    2. \(\not\exists i\in[1,k],j\in(i,k]:(v_j,v_i)\in E\)

    For non-empty sequence \(S_1\) and \(S_2\) and a precedence graph \(G=(V,E)\), we write \(S_1\to S_2\) if \(S_1\)
    must occur before \(S_2\), i.e.:
    1. \(S_1\) and \(S_2\) conform to \(G\)
    2. \(S_1\cap S_2=\emptyset\)
    3. \(\exists v_i,v_j\in V:v_i\in S_1\wedge v_j\in S_2\wedge(v_i,v_j)\in E\)
    4. \(\not\exists v_i,v_j\in V:v_i\in S_1\wedge v_j\in V\setminus S_1\setminus S_2\wedge(v_i,v_j)\in E\)
    Further we write
    \begin{align*}
    R_{1,2,\dots,k}&\quad=\quad R_1\bowtie R_2\bowtie\dots\bowtie R_k\\
    n_{1,2,\dots,k}&\quad=\quad \abs{R_{1,2,\dots,k}}
    \end{align*}

    For a given precedence graph, let \(R_i\) be a relation and \(\calr_i\) be the set of relations from which there
    exists a path to \(R_i\)
    * in any conforming join tree which includes \(R_i\), all relations from \(\calr_i\) must be joined first
    * all other relations \(R_j\) that might be joined before \(R_i\) will have no connection to \(R_i\),
      thus \(f_{i,j}=1\)

    Hence we can define selectivity of the join with \(R_i\) as
    \begin{equation*}
    s_i=
    \begin{cases}
    1&\abs{\calr_i}=0\\
    \prod_{R_j\in\calr_i}f_{i,j}&\abs{\calr_i}>0
    \end{cases}
    \end{equation*}
    If the query graph is a chain, the following conditions holds
    \begin{equation*}
    n_{1,2,\dots,k+1}=n_{1,2,\dots,k}*s_{k+1}*n_{k+1}
    \end{equation*}
    We define \(s_1=1\). Then we have
    \begin{equation*}
    n_{1,2}=s_2*(n_1*n_2)=(s_1*s_2)*(n_1*n_2)
    \end{equation*}
    and, in general,
    \begin{equation*}
    n_{1,2,\dots,k}=\prod_{i=1}^k(s_i*n_i)
    \end{equation*}

    The costs for a totally ordered precedence graph \(G\) can be computed as follows:
    \begin{align*}
    Cost_H(G)&=\sum_{i=2}^n[n_{1,2,\dots,i-1}h_i(n_i)]\\
    &=\sum_{i=2}^n[(\prod_{j=1}^is_jn_j)h_i(n_i)]
    \end{align*}
    If we choose \(h_i(n_i)=s_in_i\), then \(C_H\equiv C_{out}\). If \(s_in_i\) is less than one, we call the join
    *decreasing* and *increasing* otherwise.

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Define the cost function \(C_H\) as follows
    \begin{align*}
    C_H(\epsilon)&=0\\
    C_H(R_j)&=0\hspace{1cm}\text{if $R_j$ is the root}\\
    C_H(R_j)&=h_j(n_j)\hspace{1cm}\text{else}\\
    C_H(S_1S_2)&=C_H(S_1)+T(S_1)*C_H(S_2)
    \end{align*}
    where
    \begin{align*}
    T(\epsilon)&=1\\
    T(S)&=\prod_{R_i\in S}(s_i*n_i)
    \end{align*}
    #+END_definition

    By induction, \(C_H(G)=Cost_{H}(G)\)

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Let \(A\) and \(B\) be two sequences and \(V\) and \(U\) two non-empty sequences. We say that a cost
    function \(C\) has the *adjacent sequence interchange property* (ASI property) iff there exists a
    function \(T\) and a rank function defined for sequence \(S\) as
    \begin{equation*}
    rank(S)=\frac{T(S)-1}{C(S)}
    \end{equation*}
    s.t. for non-empty sequences \(S=AUVB\) the following holds
    \begin{equation*}
    C(AUVB)\le C(AVUB)\Leftrightarrow rank(U)\le rank(V)
    \end{equation*}
    if \(AUVB\) and \(AVUB\) satisfy the precedence constraints imposed by a given precedence graph
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_lemma
    label:3.2.3
    \(C_H\) has the ASI property
    #+END_lemma

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Let \(M=\{A_1,\dots,A_n\}\) be a set of node sequences in a given precedence graph. Then \(M\) is called a *module*
    if for all sequences \(B\) that do not overlap with the sequences in \(M\) one of the following conditions
    holds:
    * \(B\to A_i\), \(\forall 1\le i\le n\)
    * \(A_i\to B\), \(\forall 1\le i\le n\)
    * \(B\not\rightarrow A_i\) and \(A_i\not\rightarrow B\), \(\forall 1\le i\le n\)
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_lemma
    label:3.2.5
    Let \(C\) be any cost function with the ASI property and \(\{A,B\}\) a module. If \(A\to B\) and
    additionally \(rank(B)\le rank(A)\), then we can find an optimal sequence among those where \(B\) directly
    follows \(A\)
    #+END_lemma

    #+BEGIN_proof
    Every optimal permutation must have the form \((U,A,V,B,W)\) since \(A\to B\). Assume \(V\neq\epsilon\).
    If \(rank(A)\le rank(V)\), then \(rank(B)\le rank(V)\) and we can exchange \(V\) and \(B\). Therefore \(V\) is empty.
    #+END_proof

    If the precedence graph demands \(A\to B\) but \(rank(B)\le rank(A)\), we speak of *contradictory
    sequences* \(A\) and \(B\). Since the lemma shows that no non-empty subsequence can occur between \(A\)
    and \(B\), we will combine \(A\) and \(B\) into a new single node replacing \(A\) and \(B\). This node
    represents a *compound relation* comprising all relations in \(A\) and \(B\). Its cardinality is computed by
    multiplying the cardinalities of all relations in \(A\) and \(B\), and its selectivity \(s\) is the
    product of all the selectivities \(s_i\) of the relations \(R_i\) contained in \(A\) and \(B\). The
    continued process of this step until no more contradictory sequences exits is called *normalization*. The
    opposite step, replacing a compound node by the sequence of relations it was derived from, is called
    *denormalization*.

    \begin{algorithm}
    \caption{\texttt{IKKBZ}(\(G\))}
    \KwIn{an acyclic query graph \(G\) for relations \(R_1,\dots,R_n\)}
    \KwOut{the best left-deep tree}
    \(R=\emptyset\)\;
    \For{\(i=1;i\le n;++i\)}{
        Let \(G_i\) be the precedence graph derived from \(G\) and rooted at \(R_i\)\;
        \(T=\texttt{IKKBZ-Sub}(G_i)\)\;
        \(R=R\cup\{T\}\)\;
    }
    \Return{best of \(R\)}
    \end{algorithm}

    \begin{algorithm}
    \caption{\texttt{IKKBZ-Sub}(\(G\))}
    \KwIn{a precedence graph \(G_i\) for relations \(R_1,\dots,R_n\) rooted at some \(R_i\)}
    \KwOut{the optimal left-deep tree under \(G_i\)}
    \While{\(G_i\) is not a chain}{
        let \(r\) be the root of a subtree in \(G_i\) whose subtrees are chains\;
        \texttt{IKKBZ-Normalize}(\(r\))\;
        merge the chains under \(r\) according to the rank function in ascending order\;
    }
    \texttt{IKKBZ-Denormalize}(\(G_i\))\;
    \Return{\(G_i\)}
    \end{algorithm}

    \begin{algorithm}
    \caption{\texttt{IKKBZ-Normalize}(\(r\))}
    \KwIn{the root \(r\) of a subtree \(T\) of a precedence graph \(G=(V,E)\)}
    \KwOut{a normalized subchain}
    \While{\(\exists r',c\in V,r\to^*r',(r',c)\in E:rank(r')>rank(c)\)}{
        replace \(r'\) by a compound relation \(r''\) that represents \(r'c\)\;
    }
    \end{algorithm}


*** The Maximum-Value-Precedence Algorithm
    Observations:
    * greedy heuristic can produce poor results
    * IKKBZ only support acyclic queries and ASI cost functions
    * MVP algorithm is a polynomial time heuristic with good results
*** Dynamic Programming
*** Simplifying the Query Graph
*** Adaptive Optimization
*** Generating Permutations
*** Transformative Approaches
*** Randomized Approaches
*** Metaheuristics
*** Iterative Dynamic Programming
*** Order Preserving Joins
*** Complexity of Join Processing
** Accessing the Data

** Physical Properties

** Query Rewriting

** Self Tuning
* Transaction System
** Computational Models
*** Page Model
    #+ATTR_LATEX: :options [Page Model Transaction]
    #+BEGIN_definition
    A *transaction* \(t\) is a partial order of steps of the form \(r(x)\) or \(w(x)\)
    where \(x\in D\) and reads and writes as well as multiple writes applied to the same object are
    ordered. We write \(t=(op,<)\) for transaction \(t\) with step set \(op\) and partial order \(<\)
    #+END_definition
*** Object Model
    #+ATTR_LATEX: :options [Object Model Transaction]
    #+BEGIN_definition
    A *transaction* \(t\) is a (finite) tree of labeled nodes with
    * the transaction identifier as the label of the root node,
    * the names and parameters of invoked operations as labels of inner nodes, and
    * page-model read/write operations as labels of leafs nodes, along with a partial order < on the
      leaf nodes s.t. for all leaf-node operations \(p\) and \(q\) with \(p\) of the form \(w(x)\)
      and \(q\) of the form \(r(x)\) or \(w(x)\) or vice versa, we have \(p<q\vee q<p\).
    #+END_definition

    #+ATTR_LATEX: :width .8\textwidth :float nil
    #+NAME:
    #+CAPTION:
    [[../images/bigdatabase/1.png]]
** Notions of Correctness for the Page Model
*** Canonical Synchronization Problems

    Lost Update Problem:
    #+ATTR_LATEX: :width .8\textwidth :float nil
    #+NAME:
    #+CAPTION:
    [[../images/bigdatabase/2.png]]

    Inconsistent Read Problem
    #+ATTR_LATEX: :width .8\textwidth :float nil
    #+NAME:
    #+CAPTION:
    [[../images/bigdatabase/3.png]]

    Dirty Read Problem
    #+ATTR_LATEX: :width .8\textwidth :float nil
    #+NAME:
    #+CAPTION:
    [[../images/bigdatabase/4.png]]
*** Syntax of Histories and Schedules
    #+ATTR_LATEX: :options [Schedules and histories]
    #+BEGIN_definition
    Let \(T=\{t_1,\dots,t_n\}\) be a set of transactions, where each \(t_i\in T\) has the form
    \(t_i=(op_i,<_i)\)
    1. A *history* for \(T\) is a pair \(s=(op(s),<_s)\) s.t.
       1. \(op(s)\subseteq\bigcup_{i=1}^nop_i\cup\bigcup_{i=1}^n\{a_i,c_i\}\)
       2. for all \(1\le i\le n\), \(c_i\in op(s)\Leftrightarrow a_i\notin op(s)\)
       3. \(\bigcup_{i=1}^n<_i\subseteq<_s\)
       4. for all \(1\le i\le n\) and all \(p\in op_i\), \(p<_sc_i\vee p<_sa_i\)
       5. for all \(p,q\in op(s)\) s.t. at least one of them is a write and both access the same
          data item: \(p<_sq\vee q<_sp\)
    2. A *schedule* is a prefix of a history
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    A history \(s\) is *serial* if for any two transactions \(t_i\) and \(t_j\) in \(s\),
    where \(i\neq j\), all operations from \(t_i\) are ordered in \(s\) before all operations
    from \(t_j\) or vice versa
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    * \(trans(s):=\{t_i\mid s\text{ contains step of }t_i\}\)
    * \(commit(s):=\{t_i\in trans(s)\mid c_i\in s\}\)
    * \(abort(s):=\{t_i\in trans(s)\mid a_i\in s\}\)
    * \(active(s):=trans(s)-(commit(s)\cup abort(s))\)
    #+END_definition


    #+ATTR_LATEX: :width .8\textwidth :float nil
    #+NAME:
    #+CAPTION:
    [[../images/bigdatabase/6.png]]

    #+ATTR_LATEX: :width .8\textwidth :float nil
    #+NAME:
    #+CAPTION:
    [[../images/bigdatabase/5.png]]
*** Herbrand Semantics of Schedules
    #+ATTR_LATEX: :options [Herbrand Semantics of Steps]
    #+BEGIN_definition
    For schedule \(s\) the *Herbrand semantics* \(H_s\) of steps \(r_i(x),w_i(x)\in op(s)\) is :
    1. \(H_s[r_i(x)]:=H_s[w_j(x)]\) where \(w_j(x)\) is the last write on \(x\) in \(s\)
       before \(r_i(x)\)
    2. \(H_s[w_i(x)]:=f_{ix}(H_x[r_i(y_1)],\dots,H_s[r_i(y_m)])\) where
       the \(r_i(y_j)\), \(1\le j\le m\), are all read operations of \(t_i\) that occur in \(s\)
       before \(w_i(x)\) and \(f_{ix}\) is an uninterpreted \(m\)-ary function symbol.
    #+END_definition

    #+ATTR_LATEX: :options [Herbrand Universe]
    #+BEGIN_definition
    For data items \(D=\{x,y,z,\dots\}\) and transactions \(t_i\), \(1\le i\le n\), the *Herbrand
    universe HU* is the smallest set of symbols s.t.
    1. \(f_{0x}()\in HU\) for each \(x\in D\) where \(f_{0x}\) is a constant, and
    2. if \(w_i(x)\in op_i\) for some \(t_i\), there are \(m\) read
       operations \(r_i(y_1),\dots,r_i(y_m)\) that precede \(w_i(x)\) in \(t_i\),
       and \(v_1,\dots,v_m\in HU\), then \(f_{ix}(v_1,\dots,v_m)\in HU\)
    #+END_definition

    #+ATTR_LATEX: :options [Schedule Semantics]
    #+BEGIN_definition
    The *Herbrand semantics of a schedule* \(s\) is the mapping \(H[s]:D\to HU\) defined
    by \(H[s](x):=H_s[w_i(x)]\) where \(w_i(x)\) is the last operation from \(s\) writing \(x\), for
    each \(x\in D\)
    #+END_definition

    #+ATTR_LATEX: :width .6\textwidth :float nil
    #+NAME:
    #+CAPTION:
    [[../images/bigdatabase/7.png]]
*** Final-State Serializability
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Schedules \(s\) and \(s'\) are called *final state equivalent*, denoted \(s\approx_fs'\)
    if \(op(s)=op(s')\) and \(H[s]=H[s']\)
    #+END_definition

    #+ATTR_LATEX: :width .7\textwidth :float nil
    #+NAME:
    #+CAPTION:
    [[../images/bigdatabase/13.png]]

    #+ATTR_LATEX: :options [Reads-from Relation]
    #+BEGIN_definition
    Given a schedule \(s\), extended with an initial and a final transaction, \(t_0\)
    and \(t_\infty\)
    1. \(r_j(x)\) *reads \(x\) in \(s\) from \(w_i(x)\)* if \(w_i(x)\) is the last write on \(x\)
       s.t. \(w_i(x)<_sr_j(x)\)
    2. The *reads-from relation* of \(x\) is
       \begin{equation*}
       RF(s):=\{(t_i,x,t_j)\mid \text{an }r_j(x)\text{ reads \(x\) from a }w_i(x)\}
       \end{equation*}
    3. Step \(p\) is *directly useful* for step \(q\), denoted \(p\to q\), if \(q\) reads from \(p\),
       or \(p\) is a read step and \(q\) is a subsequent write step of the same
       transaction. \(\to^*\), the *useful relation*, denotes the reflexive and transitive closure of \(\to\).
    4. Step \(p\) is *alive* in \(s\) if it is useful for some step from \(t_\infty\), i.e.,
       \begin{equation*}
       (\exists q\in t_\infty)p\xrightarrow{*}q
       \end{equation*}
        and *dead* otherwise
    5. The *live-reads-from relation* of \(s\) is
       \begin{equation*}
       LRF(s):=\{(t_i,x,t_j)\mid \text{an alive \(r_j(x)\) reads \(x\) from \(w_i(x)\)}\}
       \end{equation*}
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_theorem
    For schedules \(s\) and \(s'\) the following statements hold:
    1. \(s\approx_fs'\) iff \(op(s)=op(s')\) and \(LRF(s)=LRF(s')\)
    2. For \(s\) let the step graph \(D(s)=(V,E)\) be a directed graph with vertices \(V:=op(s)\)
       and edges \(E:=\{(p,q)\mid p\to q\}\), and the reduced step graph \(D_1(s)\) be derived
       from \(D(s)\) by removing all vertices that correspond to dead steps. Then \(LRF(s)=LRF(s')\)
       iff \(D_1(s)=D_1(s')\)
    #+END_theorem

    #+BEGIN_proof
    For a given schedule \(s\), we can construct a "step graph" \(D(s)=(V,E)\) as follows
    \begin{align*}
    V&:=op(s)\\
    E&:=\{(p,q)\mid p,q\in V,p\to q\}
    \end{align*}
    From a step graph \(D(s)\), a reduced step graph \(D_l(s)\) can be derived by dropping all vertices (and
    thier incident edges) that represent dead steps. Then the following can be proven:
    1. \(LRF(s)=LRF(s')\Leftrightarrow D_l(s)=D_l(s')\)

       If \(D_l(s)\neq D_l(s')\), if there is \(r(x)\in D_l(s)\setminus D_l(s')\), then clearly \(LRF(s)\neq LRF(s')\); if
       there is \(w_i(x)\in D_l(s)\setminus D_l(s')\), then \((t_i,x,t_\infty)\in LRF(s)\setminus LRF(s')\).

       If \(LRF(s)\neq LRF(s')\), suppose \((t_i,x,t_j)\in LRF(s)\setminus LRF(s')\), then clearly \(D_l(s)\neq D_l(s')\)
    2. \(s\approx_fs'\) iff \(op(s)=op(s')\) and \(D_l(s)=D_l(s')\)
    #+END_proof

    #+ATTR_LATEX: :options []
    #+BEGIN_corollary
    Final-state equivalence of two schedules \(s\) and \(s'\) can be decided in time that is
    polynomial in the length of the two schedules.
    #+END_corollary
*** View Serializability
    As we have seen, FSR emphasizes steps that are alive in a schedule. However, since the semantics
    of a schedule and of the transactions occurring in a schedule are unknown, it is reasonable to
    require that in two equivalent schedules, each transaction reads the same values, independent of
    its liveliness.

    *Lost update anomaly*: \(L=r_1(x)r_2(x)w_1(x)w_2(x)c_1c_2\). History is not
    FSR,
     \(LRF(L)=\{(t_0,x,t_2),(t_2,x,t_\infty)\}\),
     \(LRF(t_1t_2)=\{(t_0,x,t_1),(t_1,x,t_2),(t_2,x,t_\infty)\}\) and
     \(LRF(t_2t_1)=\{(t_0,x,t_2),(t_2,x,t_1),(t_1,x,t_\infty)\}\)


     *Inconsistent read anomaly*: \(I=r_2(x)w_2(x)r_1(x)r_1(y)r_2(y)w_2(y)c_1c_2\), history is FSR
     \(LFR(I)=LFR(t_1t_2)=LFR(t_2t_1)=\{(t_0,x,t_2),(t_0,y,t_2),(t_2,x,t_\infty),(t_2,y,t_\infty)\}\)


     #+ATTR_LATEX: :options [View Equivalence]
     #+BEGIN_definition
     Schedules \(s\) and \(s'\) are *view equivalent*, denoted \(s\approx_vs'\), if the following
     hold:
     1. \(op(s)=op(s')\)
     2. \(H[s]=H[s']\)
     3. \(H_s[p]=H_{s'}[p]\) for all (read or write) steps
     #+END_definition

     #+ATTR_LATEX: :options []
     #+BEGIN_theorem
     For schedules \(s\) and \(s'\) the following statements hold.
     1. \(s\approx_v s'\) iff \(op(s)=op(s')\) and \(RF(s)=RF(s')\)
     2. \(s\approx_vs'\) iff \(D(s)=D(s')\)
     #+END_theorem

     #+BEGIN_proof
     1. \(\Rightarrow\): Consider a read step \(r_i(x)\) from \(s\).
        Then \(H_s[r_i(x)]=H_{s'}[r_i(x)]\) implies that if \(r_i(x)\) reads from some
        step \(w_j(x)\) in \(s\), the same holds in \(s'\), and vice versa.

        \(\Leftarrow\): If \(RF(s)=RF(s')\), this in particular applies to \(t_\infty\);
        hence \(H[s]=H[s']\). Similarly, for all other reads \(r_i(x)\) in \(s\), we
        have \(H_s[r_i(x)]=H_{s'}[r_i(x)]\).

        Suppose for some \(w_i(x)\), \(H_s[w_i(x)]\neq H_{s'}[w_i(x)]\). Thus the set of values read
        by \(t_i\) prior to step \(w_i\) is different in \(s\) and \(s'\), a contradiction to our
        assumption that \(RF(s)=RF(s')\).
     #+END_proof

     #+ATTR_LATEX: :options []
     #+BEGIN_corollary
     View equivalence of two schedules \(s\) and \(s'\) can be decided in time that is polynomial in
     the length of the two schedules
     #+END_corollary

     #+ATTR_LATEX: :options []
     #+BEGIN_definition
     A schedule \(s\) is *view serializable* if there exists a serial schedule \(s'\)
     s.t. \(s\approx_vs'\). VSR denotes the class of all view-serializable histories
     #+END_definition

     #+ATTR_LATEX: :options []
     #+BEGIN_theorem
     \(VSR\subset FSR\)
     #+END_theorem

     #+ATTR_LATEX: :options []
     #+BEGIN_theorem
     Let \(s\) be a history without dead steps. Then \(s\in VSR\) iff \(s\in FSR\)
     #+END_theorem

     #+ATTR_LATEX: :options []
     #+BEGIN_theorem
     The problem of deciding for a given schedule \(s\) whether \(s\in VSR\) holds is NP-complete
     #+END_theorem

     #+ATTR_LATEX: :options [Monotone Classes of Histories]
     #+BEGIN_definition
     Let \(s\) be a schedule and \(T\subseteq trans(s)\). \(\pi_T(s)\) denotes the projection
     of \(s\) onto \(T\). A class of histories is called *monotone* if the following holds:
     #+BEGIN_center
     If \(s\) is in \(E\), then \(\Pi_T(s)\) is in \(E\) for each \(T\subseteq trans(s)\)
     #+END_center
     #+END_definition

     VSR is not monotone
*** Conflict Serializability
    #+ATTR_LATEX: :options [Conflicts and Conflict Relations]
    #+BEGIN_definition
    Let \(s\) be a schedule, \(t,t'\in trans(s)\), \(t\neq t'\)
    1. Two data operations \(p\in t\) and \(q\in t'\) are in *conflict* in \(s\) if
       they access the same data item and at least one of them is a write
    2. \(conf(s):=\{(p,q)\mid p,q\text{ are in conflict and }p<_sq\}\) is the *conflict relation* of \(s\)
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Schedules \(s\) and \(s'\) are *conflict equivalent*, denoted \(s\approx_cs'\),
    if \(op(s)=op(s')\) and \(conf(s)=conf(s')\)
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Schedule \(s\) is *conflict serializable* if there is a serial schedule \(s'\)
    s.t. \(s\approx_cs'\). CSR denotes the class of all conflict serializable schedules.
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_theorem
    \(CSR\subset VSR\)
    #+END_theorem

    [[index:conflict graph]]
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Let \(s\) be a schedule. The *conflict graph* \(G(s)=(V,E)\)  is a directed graph with
    vertices \(V:=commit(s)\) and
    edges \(E:=\{(t,t')\mid t\neq t'\text\wedge\exists p\in t,q\in t':(p,q)\in conf(s)\}\)
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_theorem
    Let \(s\) be a schedule. Then \(s\in CSR\) iff \(G(s)\) is acyclic.
    #+END_theorem

    #+BEGIN_proof
    \(\Rightarrow\): There is a serial history \(s'\) s.t. \(op(s)=op(s')\)
    and \(conf(s)=conf(s')\). Consider \(t,t'\in V\), \(t\neq t'\) with \((t,t')\in E\). Then we
    have
    \begin{equation*}
    (\exists p\in t)(\exists q\in t')p<_sq\wedge(p,q)\in conf(s)
    \end{equation*}
    Then \(p<_{s'}q\). Also all of \(t\) occur before all of \(t'\) in \(s'\).

    Suppose \(G(s)\) were cyclic. Then we have a cycle \(t_1\to t_2\to\dots\to t_k\to t_1\). The
    same cycle also exists in \(G(s')\), a contradiction

    \(\Leftarrow\):
    #+END_proof

    #+ATTR_LATEX: :options []
    #+BEGIN_corollary
    Testing if a schedule is in CSR can be done in time polynomial to the schedule's number of transactions
    #+END_corollary

    Commutativity rules:
    1. \(C_1:r_i(x)r_j(y)\sim r_j(y)r_i(x)\) if \(i\neq j\)
    2. \(C_2:r_1(x)w_j(y)\sim w_j(y)r_i(x)\) if \(i\neq j\) and \(x\neq y\)
    3. \(C_3:w_i(x)w_j(y)\sim w_j(y)w_i(x)\) if \(i\neq j\) and \(x\neq y\)
    Ordering rule:
    4. [@4] \(C_4\): \(o_i(x)\), \(p_j(y)\) unordered \(\Rightarrow\) \(o_i(x)p_j(y)\)
       if \(x\neq y\) or both \(o\) and \(p\) are reads


    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Schedules \(s\) and \(s'\) s.t. \(op(s)=op(s')\) are *commutativity based equivalent*,
    denoted \(s\sim^*s'\), if \(s\) can be transformed into \(s'\) by applying rules C1, C2, C3, C4 finitely.
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_theorem
    Let \(s\) and \(s'\)  be schedules s.t. \(op(s)=op(s')\). Then \(s\approx_cs'\) iff \(s\sim^*s'\)
    #+END_theorem

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Schedule \(s\) is *commutativity-based reducible* if there is a serial schedule \(s'\) s.t. \(s\sim^*s'\)
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_corollary
    Schedule \(s\) is commutativity-based reducible iff \(s\in CSR\)
    #+END_corollary

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Schedule \(s\) is *order preserving conflict serializable* if it is conflict equivalent to a
    serial schedule \(s'\) and for all \(t,t'\in trans(s)\), if \(t\) completely precedes \(t'\)
    in \(s\), then the same holds in \(s'\). OSCR denotes the class of all schedules with this property.
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_theorem
    \(OCSR\subset CSR\)
    #+END_theorem

    \(s=w_1(x)r_2(x)c_2w_c(y)c_3w_1(y)c_1\in CSR\setminus OCSR\)


    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Schedules \(s\) is *commit order preserving conflict serializable* if for
    all \(t_i,t_j\in trans(s)\), if there are \(p\in t_i\), \(q\in t_j\) with \((p,q)\in conf(s)\),
    then \(c_i<_sc_j\).

    COCSR denotes the class of all schedules with this property
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_theorem
    \(COCSR\subset CSR\)
    #+END_theorem

    #+ATTR_LATEX: :options []
    #+BEGIN_theorem
    Schedule \(s\) is in COCSR iff there is a serial schedule \(s'\) s.t. \(s\approx_cs'\) and for
    all \(t_i,t_j\in trans(s)\): \(t_i<_{s'}t_j\Leftarrow c_i<_{s}c_j\)
    #+END_theorem
*** Commit Serializability
*** An Alternative Criterion: Interleaving Specifications
** Concurrency Control Algorithms
*** General Scheduler Design
    #+ATTR_LATEX: :width .8\textwidth :float nil
    #+NAME:
    #+CAPTION:
    [[../images/bigdatabase/8.png]]

    #+ATTR_LATEX: :options [CSR Safety]
    #+BEGIN_definition
    For a scheduler \(S\), \(Gen(S)\) denotes the set of all schedules that \(S\) can generate. A
    scheduler is called *CSR safe* if \(Gen(S)\subseteq CSR\)
    #+END_definition

    #+ATTR_LATEX: :width .8\textwidth :float nil
    #+NAME:
    #+CAPTION:
    [[../images/bigdatabase/9.png]]
*** Locking Schedulers
**** Introduction
    General locking rules:
    1. Each data operation \(o_i(x)\) must be preceded by \(ol_i(x)\) and followed by \(ou_i(x)\)
    2. For each \(x\) and \(t_i\) there is at most one \(ol_i(x)\) and at most one \(ou_i(x)\)
    3. No \(ol_i(x)\)  or \(ou_i(x)\) is redundant
    4. If \(x\) is locked by both \(t_i\) and \(t_j\), then these locks are compatible

    Let \(\DT(s)\) denote the projection of \(s\) onto the steps of type \(r,w,a,c\). \(\CP(s)\)
    denotes the committed projection of \(s\).


**** Two-Phase Locking
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    A locking protocol is *two-phase* if for every output schedule \(s\) and every
    transaction \(t_i\in trans(s)\) no \(ql_i\) step follows the first \(ou_i\) step (\(q,0\in\{r,w\}\))
    #+END_definition
    #+ATTR_LATEX: :width .8\textwidth :float nil
    #+NAME:
    #+CAPTION:
    [[../images/bigdatabase/10.png]]


    #+ATTR_LATEX: :options []
    #+BEGIN_lemma
    label:4.1
    Let \(s\) be the output of a 2PL scheduler. Then for each transaction \(t_i\in commit(DT(s))\), the
    following holds:
    1. if \(o_i(x)\), \(o\in\{r,w\}\), occurs in \(\CP(\DT(s))\), then so do \(ol_i(x)\) and \(ou_i(x)\)
       with the sequencing \(ol_i(x)<o_i(x)<ou_i(x)\).
    2. If \(t_j\in\commit(\DT(s))\), \(i\neq j\), is another transaction s.t. some steps \(p_i(x)\)
       and \(q_j(x)\) from \(\CP(\DT(s))\) are in conflict, then either \(pu_i(x)<ql_j\)
       or \(qu_j(x)<pl_i(x)\) holds.
    3. If \(p_i(x)\) and \(q_j(y)\) are in \(\CP(\DT(s))\), then \(pl_i(x)<qu_i(y)\), i.e., every lock
       operation occurs before every unlock operation of the same transaction.
    #+END_lemma

    #+ATTR_LATEX: :options []
    #+BEGIN_lemma
    label:4.2
    Let \(s\) be the output of a 2PL scheduler, and let \(G:=G(\CP(\DT(s)))\) be the conflict graph
    of \(\CP(\DT(s))\), then the following holds:
    1. If \((t_i,t_j)\) is an edge in \(G\), then \(pu_i(x)<ql_j(x)\) for some data item \(x\) and two
       operations \(p_i(x)\), \(q_j(x)\) in conflict.
    2. If \((t_1,\dots,t_n)\) is a path in \(G\), \(n\ge 1\), then \(pu_1(x)<ql_n(y)\) for two data
       items \(x\) and \(y\) as well as operations \(p_1(x)\) and \(q_n(y)\).
    3. \(G\) is acyclic.
    #+END_lemma

    Since the conflict graph of an output produced by a 2PL scheduler is acyclic, we have

    #+ATTR_LATEX: :options []
    #+BEGIN_theorem
    \(Gen(2PL)\subset CSR\)
    #+END_theorem

    #+ATTR_LATEX: :options [Strict inclusion]
    #+BEGIN_examplle
    Let \(s=w_1(x)r_2(x)c_2r_3(y)c_3w_1(y)c_1\). \(s\in\CSR\) as \(s\approx_ct_3t_1t_2\). And \(s\) cannot be produced
    by a 2PL scheduler
    #+END_examplle

    #+ATTR_LATEX: :options []
    #+BEGIN_theorem
    \(Gen(2PL)\subset OCSR\)
    #+END_theorem


**** Deadlock Handling
    Deadlock detection:
    1. maintain dynamic *waits-for graph* (WFG) with active transactions as nodes and an edge
       from \(t_i\) to \(t_j\) if \(t_j\) waits for a lock held by \(t_i\)
    2. Test WFG for cycles

    Deadlock resolution: Choose a transaction on a WFG cycles as a *deadlock victim* and abort this
    transaction, and repeat until no more cycles.

    Possible victim selection strategies:
    1. Last blocked
    2. Random
    3. Youngest
    4. Minimum locks
    5. Minimum work
    6. Most cycles
    7. Most edges

    Deadlock Prevention: Restrict lock waits to ensure acyclic WFG at all times. Reasonable deadlock
    prevention strategies when \(t_i\) is blocked by \(t_j\):
    1. *wait-die*: if \(t_i\) started before \(t_j\) then wait else abort \(t_i\).
    2. *wound-wait*: if \(t_i\) started before \(t_j\) then abort \(t_i\) else wait
    3. *Immediate restart*: abort \(t_i\)
    4. *Running priority*: if \(t_j\) is itself blocked then abort \(t_j\) else wait
    5. *Timeout*: abort waiting transaction when a timer expires.
    Abort entails later restart
**** Variants of 2PL
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Under *static* or *conservative 2PL* (C2PL) each transaction acquires all its locks before the first
    data operation.
    #+END_definition

    #+ATTR_LATEX: :width .7\textwidth :float nil
    #+NAME:
    #+CAPTION: Conservative 2PL
    [[../images/bigdatabase/11.png]]

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Under *strict 2PL* (S2PL) each transaction holds all its write locks until the transaction terminates.
    #+END_definition

    #+ATTR_LATEX: :width .7\textwidth :float nil
    #+NAME:
    #+CAPTION: Strict 2PL
    [[../images/bigdatabase/12.png]]

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Under *strong 2PL* (SS2PL) each transaction holds all its locks until the transaction terminates
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_theorem
    \(\Gen(SS2PL)\subset\Gen(S2PL)\subset\Gen(2PL)\)
    #+END_theorem

    #+ATTR_LATEX: :options []
    #+BEGIN_theorem
    \(\Gen(SS2PL)\subset COCSR\)
    #+END_theorem


**** Ordered Sharing of Locks (O2PL)
**** Altruistic Locking (AL)
**** Non-Two-Phase Locking (WTL, RWTL)
    Motivation: concurrent executions of transactions with access patterns that comply with
    organizing data items into a virtual tree
    \begin{align*}
    &t_1=w_1(a)w_1(b)w_1(d)w_1(e)w_1(i)w_1(k)\\
    &t_2=w_2(a)w_2(b)w_2(c)w_2(d)w_2(h)
    \end{align*}
    \begin{center} \begin{forest}
    [a
        [b
            [c
                [f] [g]]
            [d [h]]
            [e
                [i
                    [j] [k]]]]]
    \end{forest}\end{center}

    #+ATTR_LATEX: :options [Write-only Tree Locking (WTL)]
    #+BEGIN_definition
    Lock requests and releases must obey LR1 - LR4 and the following additional rules
    1. WTL1: A lock on a node \(x\) other than the tree root can be acquired only if the transaction
       already holds a lock on the parent of \(x\)
    2. WTL2: After a \(wu_i(x)\) no further \(wl_i(x)\) is allowed
    #+END_definition
**** Geometry of Locking
*** Non-Locking Schedulers
**** Timestamp Ordering
*** Hybrid Protocols
** Multiversion Concurrency Control
*** Multiversion Schedules
    #+ATTR_LATEX: :options []
    #+BEGIN_examplle
    \(s=r_1(x)w_1(x)r_2(x)w_2(y)r_1(y)w_1(z)c_1c_2\notin\CSR\)
    #+END_examplle
    but schedule would be tolerable if \(r_1(y)\) could read the old version \(y_0\) of \(y\) to be
    consistent with \(r_1(x)\)

    Approach:
    * each \(w\) step creates a new version
    * each \(r\) step can choose which version it wants/needs to read
    * versions are transparent to application and transient


    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Let \(s\) be a history with initial transaction \(t_0\) and final transaction \(t_\infty\). A
    *version function* for \(s\) is a function \(h\) which associates with each read step of \(s\) a
    previous write step on the same data item, and the identity for writes.
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    A *multiversion (mv) history* for transactions \(T=\{t_1,\dots,t_n\}\) is a pair \(m=(\op(m),<_m)\)
    where \(<_m\) is an order on \(\op(m)\) and
    1. \(\op(m)=\bigcup_{i=1,\dots,n}h(\op(t_i))\) for some version function \(h\)
    2. for all \(t\in T\) and all \(p,q\in\op(t_i)\): \(p<_tq\Rightarrow h(p)<_mh(q)\)
    3. if \(h(r_j(x))=w_j(x_i)\), \(i\neq j\), then \(c_i\) is in \(m\) and \(c_i<_mc_j\)
    A *multiversion (mv) schedule* is a prefix of a multiversion history
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    A multiversion schedule is a *monoversion schedule* if its version function maps each read to the
    last preceding write on the same data item.
    #+END_definition
*** Multiversion Serializability
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    For mv schedule \(m\) the reads-from relation of \(m\) is \(\RF(m)=\{(t_i,x,t_j)\mid r_j(x_i)\in\op(m)\}\)
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    mv histories \(m\) and \(m'\) with \(trans(m)=trans(m')\) are *view equivalent*, \(m\equiv_vm'\), if \(\RF(m)=\RF(m')\)
    #+END_definition




*** Limiting the Number of Versions
*** Multiversion Concurrency Control Protocols
* OLAP
** Columar store
*** The Design and Implementation of Modern Column-Oriented Database Systems
    Focus on:
    * Virtual IDs:
    * Block-oriented and vectorized processing
    * Late materialization: a select operator scans a single column at a time with a tight for-loop, resulting
      in cache and CPU friendly patterns
    * Column-specific compression
    * Direct operation on compressed data
    * Efficient join implementation
    * Redundant representation of individual columns in different sort orders
    * Database cracking and adaptive indexing
    * Efficient loading architectures

**** History, trends, and performance tradeoffs
    Figure 2.1 comes from [[cite:&10.5555/645927.672367]], summary of which by [[https://www.cs.cmu.edu/~kgao/course/15823/ailamaki01.html][Kun Gao]]:
    Partition Attributes Across (PAX) is a new layout technique that groups the same attribute of different
    tuples on the same page together to improve cache performance and reduce the cost of joining attributes.
    This provides better performance than existing NSM and DSM storage layout methods.

**** Column-store Architecture
***** C-Store
        The primary representation of data on disk is a set of column files. Each column-file contains data
        from one column, compressed using a column-specific compression method, and sorted according to some
        atrribute in the table that the column belongs to. This collection of files is known as the "read
        optimized store" (ROS). Additionally, newly loaded data is stored in a write-optimized store (WOS),
        where data is uncompressed and not vertically partitioned. Periodically, data is moved from the WOS
        into the ROS via a background "tuple mover" process, which sorts, compresses, and writes re-organized
        data to disk in a columnar form.

        Each column in C-Store may be stored several times in several different sort orders. Groups of columns
        sorted on the same attribute are referred to as “projections”. Typically there is at least one
        projection containing all columns that can be used to answer any query.

        C-Store support efficient indexing into sorted projections through the use of *sparse indexes*. A sparse
        index is a small tree-based index that stores the first value contained on each physical page of a
        column. A typical page in C-Store would be a few megabytes in size. Given a value in a sorted
        projection, a lookup in this tree returns the first page that contains that value. The page can then
        be scanned to find the actual value.
***** MonetDB and VectorWise
        MonetDB stores data one column-at-a-time both in memory and on disk and exploits bulk processing and
        late materialization. MonetDB differs from traditional RDBMS architecture in many aspects, such as
        its:
        * Execution engine, which uses a column at-a-time-algebra
