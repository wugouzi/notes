#+TITLE: Introduction to Parallel Algorithms

#+AUTHOR: Guy E. Blelloch, Laxman Dhulipala, Yihan Sun

#+EXPORT_FILE_NAME: ../latex/IntroductionToParallelAlgorithms/IntroductionToParallelAlgorithms.tex
#+LATEX_HEADER: \input{/Users/wu/notes/preamble.tex}
#+LATEX_HEADER: \graphicspath{{../../books/}}
#+LATEX_HEADER: \algrenewcommand\algorithmicforall{\textbf{parfor}}

From https://www.cs.cmu.edu/~guyb/paralg/paralg/parallel.pdf

* Models
        We use the *Random Access Machine* (RAM) model which consists of a single processor with some constant
        number of registers, an instruction counter and an arbitrarily large memory. The RAM model assumes
        that all instructions take unit time.

        The RAM is by no stretch meant to model the runtime on a real machine with cycle-by-cycle level
        accuracy. It does not model, for example, that modern-day machines have cache hierarchies and
        therefore not all memory accesses are equally expensive.

        In *work-span* models, algorithms still assume a shared random access memory, but allow dynamically
        creating tasks. Costs are measured in terms of the total number of operations, the *work* and the
        longest chain of dependence.

        We call a parallel algorithm *work-efficient* if its work is work asymptotically the same as its
        best-known sequential counterpart.

        The span for a parallel algorithm is the running time when you have an infinite number of processors.

        The *Multi-Process Random-Access Machine* (MP-RAM) consists of a set of processes that share an
        unbounded memory. The MP-RAM extends the RAM with a fork instruction that takes a positive integer
        \(k\) and forks \(k\) new child processes. Each child process receives a unique integer in the range
        \([1, . . . , k]\) in its first register and otherwise has the identical state as the parent (forking
        process), which has that register set to 0. All children start by running the next instruction, and
        the parent suspends until all the children terminate (execute an end instruction). The first
        instruction of the parent after all children terminate is called the *join* instruction. A *computation*
        starts with a single root process and finishes when that root process ends. This model supports
        *nested parallelism* -the ability to fork processes in a nested fashion. If the root process never does
        a fork, it is a standard sequential program.

        A computation in the MP-RAM defines a partial order on the instructions. In particular
        1. every instruction depends on its previous instruction in the same thread (if any),
        2. every first instruction in a process depends on the fork instruction of the parent that generated
           it, and
        3. every join instruction depends on the end instruction of all child processes of the corresponding
           fork generated.

        The work of a computation is the total number of instructions, and the span is the longest sequences
        of dependent instructions.

        Two instructions are said to be *concurrent* if they are unordered, and ordered otherwise. Two
        instructions *conflict* if one writes to a memory location that the other reads or writes the same
        location. We say two instructions race if they are concurrent and conflict.

        A \(\textsc{testAndSet}(x)\) (TS) instruction takes a reference to a memory location \(x\), checks if
        the value of \(x\) is ~false~ and if so atomically sets it to ~true~ and returns ~true~; if already ~true~ it
        returns ~false~.

        A \(\textsc{compareAndSwap}(x, o, n)\) (CAS) instruction takes a reference to a memory location \(x\),
        checks if the value of \(x\) equals \(o\). If so, the instruction will change the value to \(n\) and
        return ~true~. If not, the instruction does nothing and simply returns ~false~.

        A \(\textsc{fetchAndAdd}(x, y)\) (FA) instruction takes a reference to a memory location \(x\), and a
        value \(y\), and it adds \(y\) to the value of \(x\), returning the old value. Different from a TS or
        a CAS, an FA instruction always successfully adds \(y\) to the value stored in \(x\).

        A \(\textsc{priorityWrite}(x, y)\) (PW) instruction takes a reference to a memory location \(x\), and
        checks if the value \(y\) is less than the current value in \(x\). If so, it changes the value stored
        in \(x\) to \(y\), and return ~true~. If not, it does nothing and return ~false~.
* Preliminaries
        #+ATTR_LATEX: :options [w.h.p.]
        #+BEGIN_definition
        \(g(n)\in O(f(n))\) with *high probability* (w.h.p.) if \(g(n)\in O(cf(n))\) with probability at least
        \(1-(\frac{1}{n})^c\), for some constant \(c_0\) and all \(c\ge c_0\).
        #+END_definition

        #+ATTR_LATEX: :options []
        #+BEGIN_theorem
        Consider a set of indicator random variables \(X_1,\dots,X_n\) for which \(p(X_i=1)\le\barp_i\) conditioned on
        all possible events \(X_j=\{0,1\}\), \(i\neq j\). Let \(X=\sum_{i=1}^nX_i\) and \(\barE[X]=\sum_{i=1}^n\barp_i\),
        then
        \begin{equation*}
        \Pr[X\ge k]\le\left( \frac{e\barE[X]}{k} \right)^k
        \end{equation*}
        #+END_theorem

        #+BEGIN_proof
        Let's first consider the special case that the \(\barp_i\) are all equal and have value \(p\). If
        \(X\ge k\), then we have that at least \(k\) of the random variables are 1. The probability of any
        particular \(k\) variables all being 1, and the others being anything, is upper bounded by \(p^k\).
        \begin{equation*}
        \Pr[X\ge k]\le p^k\binom{n}{k}<p^k\left(\frac{ne}{k}\right)^k
        =\left(\frac{pne}{k}\right)^k=
        \left(\frac{e\barE[X]}{k}\right)^k
        \end{equation*}
        Here we used a standard upper bound on the binomial coefficients: \(\binom{n}{m}<\left(\frac{ne}{m}\right)^m\).
        #+END_proof
* Some Building Blocks
** Scan
        A *scan* or *prefix-sum* function takes a sequence \(A\), an associative function \(f\), and a left
        identity element \(\bot\) and computes the values
        \begin{equation*}
        r_i=
        \begin{cases}
        \bot&i=0\\
        f(r_{i-1},A_i)&0<i\le\abs{A}
        \end{cases}
        \end{equation*}
        Each \(r_i\) is the "sum" of the prefix \(A[0,i]\) of \(A\) w.r.t. the function \(f\).

        \begin{algorithmic}
        \Function{scanUp}{\(A,L,f\)}
        \If{\(\abs{A}=1\)}
                \Return \(\abs{A[0]}\)
        \Else
                \State \(n\gets\abs{A}\);
                \State \(m\gets n/2\);
                \State \(l\gets\textsc{scanUp}(A[0:m],L[0:m-1],f)\quad||\)
                \State \(r\gets\textsc{scanUp}([A[m:n],L[m:n-1]],f)\);
                \State \(L[m-1]\gets l\);
                \State \Return \(f(l,r)\)
        \EndIf
        \EndFunction
        \end{algorithmic}

        \begin{algorithmic}
        \Function{scanDown}{\(R,L,f,s\)}
                \If{\(\abs{R}=1\)}
                        \State \(R[0]=s\);
                        \State \Return
                \Else
                        \State \(n\gets\abs{A}\);
                        \State \(m\gets\abs{R}/2\);
                        \State \(\textsc{scanDown}(R[0:m],L[0:m-1],s)\quad||\)
                        \State \(\textsc{scanDown}(R[m:n],L[m:n-1],f(s,L[m-1]))\);
                        \State \Return
                \EndIf
        \EndFunction
        \end{algorithmic}

        \begin{algorithmic}
        \Function{scan}{\(A,f,I\)}
        \State \(L\gets\textsc{array}[\abs{A}-1]\);
        \State \(R\gets\textsc{array}[\abs{A}]\);
        \State \(\texttt{total}\gets\textsc{scanUp}(A,L,f)\);
        \State \(\textsc{scanDown}(R,L,f,I)\);
        \State\Return \(\la R,\texttt{total}\ra\)
        \EndFunction
        \end{algorithmic}

        For \textsc{scanUp} it should be clear that the values written into \(L\) are indeed the sums of the
        left subtrees. For \textsc{scanDown} consider a node \(v\) in the tree and the value \(s\) passed to
        it. The algorithm maintains that the value \(s\) is the sum of all values to the left of the subtree
        rooted at \(v\).

        #+ATTR_LATEX: :width .8\textwidth :float nil
        #+NAME:
        #+CAPTION:
        [[../images/Parallel/1.png]]
        Work of \textsc{scanUp} and \textsc{scanDown} is
        \begin{equation*}
        W(n)=2W(n/2)+O(1)=O(n)
        \end{equation*}
        and the span is
        \begin{equation*}
        D(n)=D(n/2)+O(1)=\log(n)
        \end{equation*}
** Filter and Flatten
        \begin{algorithmic}
        \Function{filter}{\(A,p\)}
                \State \(n\gets\abs{A}\);
                \State \(F\gets\textsc{array}[n]\);
                \ForAll{\(i\in[0:n]\)}
                \State \(F[i]\gets p(A[i])\)
                \EndFor
                \State \(\la X,m\ra\gets\textsc{plusScan}(F)\);
                \State \(R\gets\textsc{array}[m]\);
                \ForAll{\(i\in[0:n]\)}
                \If{\(F[i]\)}
                        \State\(R[X[i]]\gets A[i]\);
                \EndIf
                \EndFor
                \State\Return \(R\)
        \EndFunction
        \end{algorithmic}

        Work \(O(n)\), span \(O(\log n)\)

        \begin{algorithmic}
        \Function{flatten}{\(A\)}
        \State \(\texttt{sizes}\gets\textsc{array}(\abs{A})\);
        \ForAll{\(i\in[0:\abs{A}]\)}
                \State \(\texttt{sizes}[i]\gets\abs{A[i]}\);
        \EndFor
        \State \(\la X,m\ra\gets\textsc{plusScan}(\texttt{sizes})\);
        \State \(R\gets\textsc{array}(m)\)
        \ForAll{\(i\in[0:\abs{A}]\)}
                \State \(o\gets X[i]\);
                \ForAll{\(j\in[0:\abs{A[i]}]\)}
                        \State\(R[o+j]\gets A[i][j]\)
                \EndFor
        \EndFor
        \State \Return\(R\)
        \EndFunction
        \end{algorithmic}
** Search
        The sorted search problem is given a sorted sequence \(A\) and a key \(v\), to find the position of the
        greatest element in \(A\) that is less than \(v\).

        \begin{algorithmic}
        \LComment{finds which of \(k\) blocks contains \(v\), returns \texttt{block} and \texttt{offset}}
        \Function{findBlock}{\(A,v,k\)}
        \State \(s\gets\abs{A}/k\)
        \State \(r\gets k\)
        \ForAll{\(i\in[0:k]\)}
                \If{\(A[i\times s]<v\wedge A[(i+1)\times s]>v\)}
                        \State \(r\gets i\)
                \EndIf
        \EndFor
        \State \Return \((A[r\times s,(r+1)\times s],i\times s)\)
        \EndFunction
        \end{algorithmic}

        \begin{algorithmic}
        \Function{search}{\(A,v,k\)}
        \State \((B,o)=\textsc{findBlock}{A,v,\min(\abs{A},k)}\);
        \If{\(\abs{A}\le k\)}
                \State \Return \(o\)
        \Else
                \State \Return \(o+\textsc{search}(B,v,k)\)
        \EndIf
        \EndFunction
        \end{algorithmic}

        By picking \(k=n_0^\alpha\), where \(n_0\) is the original input size, and for \(0<\alpha\le 1\), the algorithm
        will make \(O(1/\alpha)\) calls until it is of size 1 or less. The work on each recursive call is
        \(O(k)=O(n_0^\alpha)\) so the total work is \(W(n)=O((1/\alpha)n^\alpha)\). On the MP-RAM the span on each level is
        \(O(1)\) so the overall span is \(O(1/\alpha)\).

        Another related problem is given two sorted sequences \(A\) and \(B\), and an integer \(k\), to find
        the \(k\)th smallest elements. More specifically, \(\textsc{kth}(A,B,k)\) returns locations
        \((l_a,l_b)\) in \(A\) and \(B\) s.t. \(l_a+l_b=k\)
        \begin{algorithmic}
        \Function{kthHelp}{\(A,a_0,B,b_0,k\)}
        \If{\(\abs{A}+\abs{B}=0\)}
                \State \Return \((a_0,b_0)\)
        \ElsIf{\(\abs{A}=0\)}
                \State \Return \((a_0,b_0+k)\)
        \ElsIf{\(\abs{B}=0\)}
                \State \Return \((a_0+k,b_0)\)
        \Else
                \State \(m_a\gets\abs{A}/2\)
                \State \(m_b\gets\abs{B}/2\)
                \CaseOf{\((A[m_a]<B,k>m_a+m_b)\)}
                        \State \((T,T)\Rightarrow\) \Return \(\textsc{kthHelp}(A[m_a+1:\abs{A}],a_0+m_a+1,B,b_0,k-m_a-1)\)
                        \State \((T,F)\Rightarrow\) \Return \(\textsc{kthHelp}(A,a_0,B[0:m_b],b_0,k)\)
                        \State \((F,T)\Rightarrow\) \Return \(\textsc{kthHelp}(A,a_0,B[m_b+1:\abs{B}],b_0+m_b+1,k-m_b-1)\)
                        \State \((F,F)\Rightarrow\) \Return \(\textsc{kthHelp}(A[0:m_a],a_0,B,b_0,k)\)
                \EndCaseOf
        \EndIf
        \EndFunction
        \end{algorithmic}

        \begin{algorithmic}
        \Function{kth}{\(A,B,k\)}
        \State \Return \(\textsc{kthHelp}(A,0,B,0,k)\)
        \EndFunction
        \end{algorithmic}
        Work and span are \(O(\log\abs{A}+\log\abs{B})\)

        Again the idea is to do a \(k\)-way search. By picking \(k\) evenly spaced positions in one array it
        is possible to find them in the other array using the sorted search problem. This can be used to find
        the subblock of \(A\) and \(B\) that contains the location \((l_a,l_b)\). By doing this again from the
        other array, both subblocks can be reduced in size by a factor of \(k\). This is repeated for
        \(\log_k\abs{A}+\log_k\abs{B}\) levels. By picking \(k=n_0^\alpha\) this will result in an algorithm taking
        \(O(n^{2\alpha})\) work and \(O(1/\alpha^2)\) span. #+LATEX: \wu{
        Note here is fucking weird. I think they are \(O((1/\alpha)n^{\alpha})\) and \(O(1/\alpha)\) respectively
   #+LATEX: }
** Merge
        \begin{algorithmic}
        \Function{merge}{\(A,B,R\)}
        \CaseOf{\((\abs{A},\abs{B})\)}
                \State \((0,\_)\Rightarrow\texttt{copy }$B$\texttt{ to }R\); \Return
                \State \((\_,0)\Rightarrow\texttt{copy }$B$\texttt{ to }R\); \Return
                \State \texttt{otherwise} \(\Rightarrow\)
                \State \hspace{0,4cm} \(m\gets\abs{R}/2\);
                \State \hspace{0,4cm} \((m_a,m_b)=\textsc{kth}(A,B,m)\);
                \State \hspace{0,4cm} \textsc{merge}\((A[0:m_a],B[0:m_b],R[0:m])\quad||\)
                \State \hspace{0,4cm} \(\textsc{merge}(A[m_a:\abs{A}],B[m_b:\abs{B}],R[m:\abs{R}])\)
                \State \hspace{0,4cm} \Return
        \EndCaseOf
        \EndFunction
        \end{algorithmic}
        Let \(m=\abs{A}+\abs{B}\), then
        \begin{align*}
        W(m)&=2W(m/2)+O(\log m)=O(m)\\
        D(m)&=D(m/2)+O(\log m)=O(\log^2m)
        \end{align*}

        By using the parallel version of \textsc{kth} with some \alpha, the recurrences are
        \begin{align*}
        W(m)&=2W(m/2)+O(n^{1/2})=O(m)\\
        D(m)&=D(m/2)+O(1)=O(\log m)
        \end{align*}

        The span of parallel merge can be improved by using a multi-way divide-and-conquer instead of
        two-way,, as below.
        \begin{algorithmic}
        \Function{mergeFway}{\(A,B,R,f\)}
        \LComment{Same base cases}
        \Otherwise
        \State \(l\gets(\abs{R}-1)/f(\abs{R})+1\);
        \ForAll{\(i\in[0:f(\abs{R})]\)}
                \State \(s\gets\min(i\times l,\abs{R})\);
                \State \(e\gets\min((i+1)\times l,\abs{R})\);
                \State \((s_a,s_b)\gets\textsc{kth}(A,B,s)\);
                \State \((e_a,e_b)\gets\textsc{kth}(A,B,e)\);
                \State \(\textsc{mergeFway}(A[s_a:e_a],B[s_b:e_b],R[s:e],f)\)
        \EndFor
        \State \Return
        \EndOtherwise
        \EndFunction
        \end{algorithmic}

        If we use \(f(n)=\sqrt{n}\), and using dual binary search for \(\textsc{kth}\), then
        \begin{align*}
        W(m)&=\sqrt{m}W(\sqrt{m})+O(\sqrt{m}\log m)=O(m)\\
        D(m)&=D(\sqrt{m})+O(\log m)=O(\log m)
        \end{align*}
** K-th Smallest
        \Function{}{}

        \EndFunction
