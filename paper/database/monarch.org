#+title: Monarch: Google's Planet-Scale In-Memory Time Series Database

#+AUTHOR:
#+LATEX_HEADER: \input{/Users/wu/notes/preamble.tex}
#+EXPORT_FILE_NAME: ../../latex/papers/database/monarch.tex
#+LATEX_HEADER: \graphicspath{{../../../paper/database/}}
#+OPTIONS: toc:nil
#+STARTUP: shrink

        HackerNews has some additional discussions
        [[https://news.ycombinator.com/item?id=31379383][duscussions]]
* Introduction
        Borgmon's limitations:
        * Borgmon’s architecture encourages a decentralized operational model where each team sets up and
          manages their own Borgmon instances. However, this led to non-trivial operational overhead for many
          teams who do not have the necessary expertise or staffing to run Borgmon reliably. Additionally,
          users frequently need to examine and correlate monitoring data across application and infrastructure
          boundaries to troubleshoot issues; this is difficult or impossible to achieve in a world of many isolated Borgmon instances;
        * Borgmon’s lack of schematization for measurement dimensions and metric values has resulted in
          semantic ambiguities of queries, limiting the expressiveness of the query language during data analysis;
        * Borgmon does not have good support for a distribution (i.e., histogram) value type, which is a
          powerful data structure that enables sophisticated statistical analysis
        * Borgmon requires users to manually shard the large number of monitored entities of global services
          across multiple Borgmon instances and set up a query evaluation tree.

        Monarch provides multi-tenant monitoring as a single unified service for all teams, minimizing their
        operational toil.

* System Overview
        #+ATTR_LATEX: :width .8\textwidth :float nil
        #+NAME: f1
        #+CAPTION:
        [[../../images/papers/163.png]]

        1. Monarch readily trades consistency for high availability and partition tolerance
           * To promptly deliver alerts, Monarch must serve the most recent data in a timely fashion; for
             that, Monarch drops delayed writes and returns partial data for queries if necessary.
           * In the face of network partitions, Monarch continues to support its users’ monitoring and
             alerting needs, with mechanisms to indicate the underlying data may be incomplete or inconsistent.
        2. Monarch must be low dependency on the alerting critical path: Monarch stores monitoring data in
           memory despite the high cost.

        The primary organizing principle of Monarch, as shown in Figure [[ref:f1]], is local monitoring in
        regional /zones/ combined with global management and querying. Local monitoring allows Monarch to keep
        data near where it is collected, reducing transmission costs, latency, and reliability issues, and
        allowing monitoring within a zone independently of components outside that zone. Global management and
        querying supports the monitoring of global systems by presenting a unified view of the whole system.

        Each Monarch zone is autonomous, and consists of a collection of clusters, i.e., independent failure
        domains, that are in a strongly network-connected region. Components in a zone are replicated across
        the clusters for reliability. Monarch stores data in memory and avoids hard dependencies so that
        each zone can work continuously during transient outages of other zones, global components, and
        underlying storage systems. Monarch’s global components are geographically replicated and interact
        with zonal components using the closest replica to exploit locality.

        The components responsible for holding state are:
        * *Leaves* store monitoring data in an in-memory time series store.
        * *Recovery logs* store the same monitoring data as the leaves, but on disk. This data ultimately gets
          rewritten into a long-term time series repository (not discussed due to space constraints).
        * A global *configuration server* and its zonal mirrors hold configuration data in Spanner databases.

        The data ingestion components are:
        * *Ingestion routers* that route data to leaf routers in the appropriate Monarch zone, using information
          in time series keys to determine the routing.
        * *Leaf routers* that accept data to be stored in a zone and route it to leaves for storage.
        * *Range assigners* that manage the assignment of data to leaves, to balance the load among leaves in a
          zone.

        The components involved in query execution are:
        * *Mixers* that partition queries into sub-queries that get routed to and executed by leaves, and merge
          sub-query results. Queries may be issued at the root level (by root mixers) or at the zone level (by
          zone mixers). Root-level queries involve both root and zone mixers.
        * *Index servers* that index data for each zone and leaf, and guide distributed query execution.
        * *Evaluators* that periodically issue standing queries to mixers and write the results back to leaves.
* Data Model
        #+ATTR_LATEX: :width .8\textwidth :float nil
        #+NAME: f2
        #+CAPTION:
        [[../../images/papers/164.png]]

        Conceptually, Monarch stores monitoring data as time series in schematized tables. Each table consists
        of multiple *key columns* that form the time series key, and a *value column* for a history of points of
        the time series. See Figure [[ref:f2]] for an example. Key columns, also referred to as *fields*, have two sources: *targets* and *metrics*, defined as follows.
** Targets
        Monarch uses *targets* to associate each time series with its source entity (or monitored entity), which
        is, for example, the process or the VM that generates the time series. Each target represents a
        monitored entity, and conforms to a *target schema* that defines an ordered set of target field names
        and associated field types. Figure [[ref:f2]] shows a popular target schema named ~ComputeTask~; each
        ~ComputeTask~ target identifies a running task in a Borg cluster with four fields: ~user~, ~job~, ~cluster~,
        and ~task_num~

        For locality, Monarch stores data close to where the data is generated. Each target schema has one
        field annotated as *location*; the value of this location field determines the specific Monarch zone to
        which a time series is routed and stored. For example, the location field of ~ComputeTask~ is ~cluster~;
        each Borg cluster is mapped to one (usually the closest) Monarch zone.

        Within each zone, Monarch stores time series of the same target together in the same leaf because they
        originate from the same entity and are more likely to be queried together in a join. Monarch also
        groups targets into disjoint target ranges in the form of \([S_{start},S_{end})\) where \(S_{start}\)
        and \(S_{end}\) are the start and end target strings. A *target string* represents a target by
        concatenating the target schema name and field values in order . For example, in Figure [[ref:f2]], the
        target string ~ComputeTask::sql-dba::db.server::aa::0876~ represents the Borg task of a database server.
        Target ranges are used for lexicographic sharding and load balancing among leaves; this allows more
        efficient aggregation across adjacent targets in queries
** Metrics
        Similar to a target, a metric conforms to a metric schema, which defines the time series value type
        and a set of metric fields. Metrics are named like files. Figure [[ref:f2]] shows an example metric called
        ~/rpc/server/latency~ that measures the latency of RPCs to a server; it has two metric fields that
        distinguish RPCs by service and ~command~.

        #+ATTR_LATEX: :width .8\textwidth :float nil
        #+NAME: f3
        #+CAPTION:
        [[../../images/papers/165.png]]

        The value type can be ~boolean~, ~int64~, ~double~, ~string~, ~distribution~, or ~tuple~ of other types. All of
        them are standard types except distribution, which is a compact type that represents a large number of
        double values. A distribution includes a *histogram* that partitions a set of double values into subsets
        called *buckets* and summarizes values in each bucket using overall statistics such as mean, count, and
        standard deviation. Bucket boundaries are configurable for trade-off between data granularity (i.e.,
        accuracy) and storage costs: users may specify finer buckets for more popular value ranges. Figure
        [[ref:f3]] shows an example distribution-typed time series of ~/rpc/server/latency~ which measures servers’
        latency in handling RPCs; and it has a fixed bucket size of 10ms. Distribution-typed points of a time
        series can have different bucket boundaries; inter- polation is used in queries that span points with
        different bucket boundaries. Distributions are an effective feature for summarizing a large number of
        samples. Mean latency is not enough for system monitoring—we also need other statistics such as 99th
        and 99.9th percentiles. To get these efficiently, histogram support—aka distribution—is indispensable.

        #+ATTR_LATEX: :width .8\textwidth :float nil
        #+NAME: f4
        #+CAPTION:
        [[../../images/papers/166.png]]

        *Exemplars*. Each bucket in a distribution may contain an exemplar of values in that bucket. An exemplar
        for RPC metrics, such as ~/rpc/server/latency~, may be a Dapper RPC trace, which is very useful in
        debugging high RPC latency. Additionally, an exemplar contains information of its originating target
        and metric field values. The information is kept during distribution aggregation, therefore a user can
        easily identify problematic tasks via outlier exemplars. Figure [[ref:f4]] shows a heat map of a
        distribution-typed time series including the exemplar of a slow RPC that may explain the tail latency
        spike in the middle of the graph.

        *Metric types*. A metric may be a gauge or a cumulative. For each point of a gauge time series, its
        value is an instantaneous measurement, e.g., queue length, at the time indicated by the point
        timestamp. For each point of a cumulative time series, its value is the accumulation of the measured
        aspect from a start time to the time indicated by its timestamp.
* Scalable Collection
** Data Collection Overview
        The two levels of routers perform two levels of divide-and-conquer: /ingestion routers/ regionalize time
        series data into zones according to location fields, and /leaf routers/ distribute data across leaves
        according to the range assigner. Recall that each time series is associated with a target and one of
        the target fields is a location field.

        Writing time series data into Monarch follows four steps:
        1. A client sends data to one of the nearby ingestion routers, which are distributed across all our
           clusters. Clients usually use our instrumentation library, which automatically writes data at the
           frequency necessary to fulfill retention policies
        2. The ingestion router finds the destination zone based on the value of the target’s location field,
           and forwards the data to a leaf router in the destination zone. The location-to-zone mapping is
           specified in configuration to ingestion routers and can be updated dynamically.
        3. The leaf router forwards the data to the leaves responsible for the target ranges containing the
           target. Within each zone, time series are sharded lexicographically by their target strings. Each
           leaf router maintains a continuously-updated range map that maps each target range to three leaf
           replicas. Note that leaf routers get updates to the range map from leaves instead of the range
           assigner. Also, target ranges jointly cover the entire string universe; all new targets will be
           picked up automatically without intervention from the assigner. So data collection continues to
           work if the assigner suffers a transient failure.
        4. Each leaf writes data into its in-memory store and recovery logs. The in-memory time series store
           is highly optimized: it
           1. encodes timestamps efficiently and shares timestamp sequences among time series from the same
              target;
           2. handles delta and run-length encoding of time series values of complex types including
              distribution and tuple;
           3. supports fast read, write, and snapshot;
           4. operates continuously while processing queries and moving target ranges; and
           5. minimizes memory fragmentation and allocation churn.

        To achieve a balance between CPU and memory, the in-memory store performs only light compression such
        as timestamp sharing and delta encoding. Timestamp sharing is quite effective: one timestamp sequence
        is shared by around ten time series on average.

        Note that leaves do not wait for acknowledgement when writing to the *recovery logs* per range. Leaves
        write logs to distributed file system instances (i.e., Colossus) in multiple distinct clusters and
        independently fail over by probing the health of a log. However, the system needs to continue
        functioning even when all Colossus instances are unavailable, hence the best-effort nature of the
        write to the log. Recovery logs are compacted, rewritten into a format amenable for fast reads (leaves
        write to logs in a write-optimized format), and merged into the long-term repository by
        continuously-running background processes whose details we omit from this paper. All log files are
        also asynchronously replicated across three clusters to increase availability.

        Data collection by leaves also triggers updates in the zone and root *index servers* which are used to
        constrain query fanout
** Intra-zone Load Balancing
        As a reminder, a table schema consists of a target schema and a metric schema. The lexicographic
        sharding of data in a zone uses only the key columns corresponding to the target schema. This greatly
        reduces ingestion fanout. This not only allows a zone to scale horizontally by adding more leaf nodes,
        but also restricts most queries to a small subset of leaf nodes. Additionally, commonly used
        intra-target joins on the query path can be pushed down to the leaf-level, which makes queries cheaper
        and faster

        In addition, we allow heterogeneous replication policies (1 to 3 replicas) for users to trade off
        between availability and storage cost.

        Range assigners balance load in ways similar to Slicer. Within each zone, the range assigner splits,
        merges, and moves ranges between leaves to cope with changes in the CPU load and memory usage imposed
        by the range on the leaf that stores it. While range assignment is changing, data collection works
        seamlessly by taking advantage of recovery logs. For example (range splits and merges are similar),
        the following events occur once the range assigner decided to move a range, say \(\bR\), to reduce the
        load on the source leaf:
        1. The range assigner selects a destination leaf with light load and assigns \(\bR\) to it. The
           destination leaf starts to collect data for \(\bR\) by informing leaf routers of its new assignment
           of \(\bR\), storing time series with keys within \(\bR\), and writing recovery logs.
        2. After waiting for one second for data logged by the source leaf to reach disks, the destination
           leaf starts to recover older data within \(\bR\), in reverse chronological order (since newer data
           is more critical), from the recovery logs.
        3. Once the destination leaf fully recovers data in \(\bR\), it notifies the range assigner to
           unassign \(\bR\) from the source leaf. The source leaf then stops collecting data for \(\bR\) and
           drops the data from its in-memory store.

        During this process, both the source and destination leaves are collecting, storing, and logging the
        same data simultaneously to provide continuous data availability for the range \(\bR\). Note that it
        is the job of leaves, instead of the range assigner, to keep leaf routers updated about range
        assignments for two reasons:
        1. leaves are the source of truth where data is stored; and
        2. it allows the system to degrade gracefully during a transient range assigner failure.
** Collection Aggregation
* Problems


* References
<<bibliographystyle link>>
bibliographystyle:alpha

\bibliography{/Users/wu/notes/notes/references.bib}
