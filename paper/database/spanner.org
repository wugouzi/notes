#+title: Spanner
#+AUTHOR:
#+LATEX_HEADER: \input{/Users/wu/notes/preamble.tex}
#+EXPORT_FILE_NAME: ../../latex/papers/database/spanner.tex
#+LATEX_HEADER: \graphicspath{{../../../paper/database/}}
#+LATEX_HEADER: \definecolor{mintedbg}{rgb}{0.99,0.99,0.99}
#+LATEX_HEADER: \usepackage[cachedir=\detokenize{~/miscellaneous/trash}]{minted}
#+LATEX_HEADER: \setminted{breaklines,
#+LATEX_HEADER:   mathescape,
#+LATEX_HEADER:   bgcolor=mintedbg,
#+LATEX_HEADER:   fontsize=\footnotesize,
#+LATEX_HEADER:   frame=single,
#+LATEX_HEADER:   linenos}
#+OPTIONS: toc:nil
#+STARTUP: shrink

* Introduction
        1. The replication configurations for data can be dynamically controlled at a fine grain by
           applications.
           * Applications can specify constraints to control which datacenters contain which data, how far
             data is from its users (to control read latency), how far replicas are from each other (to
             control write latency), and how many replicas are maintained (to con- trol durability, availability, and read performance).
           * Data can also be dynamically and transparently moved between datacenters by the system to balance resource usage across datacenters.
        2. Spanner provides externally consistent reads and writes, and globally-consistent reads across the
           database at a timestamp.
* Implementation
        A Spanner deployment is called a *universe*.

        Spanner is organized as a set of *zones*, where each zone is the rough analog of a deployment of
        Bigtable servers. Zones are the unit of administrative deployment.
        * The set of zones is also the set of locations across which data can be replicated.
        * Zones can be added to or removed from a running system as new datacenters are brought into service
          and old ones are turned off, respectively.
        * Zones are also the unit of physical isolation: there may be one or more zones in a datacenter, for
          example, if different applications’ data must be partitioned across different sets of servers in the
          same datacenter.


        #+ATTR_LATEX: :width .5\textwidth :float nil
        #+NAME:
        #+CAPTION:
        [[../../images/papers/58.png]]

        A zone has one *zonemaster* and between one hundred and several thousand *spanservers*. The former assigns
        data to spanservers; the latter serve data to clients. The per-zone *location proxies* are used by
        clients to locate the spanservers assigned to serve their data. The *universe master* and the *placement
        driver* are currently singletons.
        * The universe master is primarily a console that displays status information about all the zones for
          interactive debugging.
        * The placement driver handles automated movement of data across zones on the timescale of minutes.
          The placement driver periodically communicates with the spanservers to find data that needs to be
          moved, either to meet updated replication constraints or to balance load.
** Spanserver Software Stack
        #+ATTR_LATEX: :width .7\textwidth :float nil
        #+NAME:
        #+CAPTION:
        [[../../images/papers/59.png]]
        At the bottom, each spanserver is responsible for between 100 and 1000 instances of a data structure
        called *tablet*. A tablet implements a bag of the following mappings:
        \begin{equation*}
        (\textsf{key:string}, \textsf{timestamp:int64})\to\textsf{string}
        \end{equation*}
        A tablet's state is stored in set of B-tree-like files and a write-ahead log, all on a distributed
        filesystem called Colossus.

        To support replication, each spanserver implements a single Paxos state machine on top of each tablet.
        Each state machine stores its metadata and log in its corresponding tablet. Our Paxos implementation
        supports long-lived leaders with time-based leader leases, whose length defaults to 10 seconds. The
        current Spanner implementation logs every Paxos write twice: once in the tablet’s log, and once in the
        Paxos log. This choice was made out of expediency, and *we are likely to remedy this eventually*. Our
        implementation of Paxos is pipelined, so as to improve Spanner’s throughput in the presence of WAN
        latencies; but writes are applied by Paxos in order.

        The Paxos state machines are used to implement a consistently replicated bag of mappings. The
        key-value mapping state of each replica is stored in its corresponding tablet. Writes must initiate
        the Paxos protocol at the leader; reads access state directly from the underlying tablet at any
        replica that is sufficiently up-to-date. The set of replicas is collectively a *Paxos group*.

        At every replica that is a leader, each spanserver implements a *lock table* to implement concurrency
        control. The lock table contains the state for two-phase locking: it maps ranges of keys to lock
        states. (/Note that having a long-lived Paxos leader is critical to efficiently managing the lock
        table/.) In both Bigtable and Spanner, we designed for long-lived transactions (for example, for report
        generation, which might take on the order of minutes), which perform poorly under optimistic
        concurrency control in the presence of conflicts. Operations that require synchronization, such as
        transactional reads, acquire locks in the lock table; other operations bypass the lock table.

        At every replica that is a leader, each spanserver also implements a *transaction manager* to support
        distributed transactions. The transaction manager is used to implement a *participant leader*; the other
        replicas in the group will be referred to as *participant slaves*. If a transaction involves only one
        Paxos group (as is the case for most transactions), it can bypass the transaction manager, since the
        lock table and Paxos together provide transactionality. If a transaction involves more than one Paxos
        group, those groups' leaders coordinate to perform two-phase commit. One of the participant groups is
        chosen as the coordinator: the participant leader of that group will be referred to as the *coordinator
        leader*, and the slaves of that group as *coordinator slaves*. The state of each transaction manager is
        stored in the underlying Paxos group (and therefore is replicated).
** Directories and Placement
        On top of the bag of key-value mappings, the Spanner implementation supports a bucketing abstraction
        called a *directory*, which is a set of contiguous keys that share a common prefix. Supporting
        directories allows applications to control the locality of their data by choosing keys carefully.

        A directory is the unit of data placement. All data in a directory has the same replication
        configuration. When data is moved between Paxos groups, it is moved directory by directory, as shown
        in Figure [[ref:3]]. Spanner might move a directory to shed load from a Paxos group; to put directories
        that are frequently accessed together into the same group; or to move a directory into a group that is
        closer to its accessors. Directories can be moved while client operations are ongoing. One could
        expect that a 50MB directory can be moved in a few seconds.

        #+ATTR_LATEX: :width .5\textwidth :float nil
        #+NAME: 3
        #+CAPTION:
        [[../../images/papers/60.png]]

        Spanner tablet is a container that may encapsulate multiple partitions of the row space. We made this
        decision so that it would be possible to colocate multiple directories that are frequently accessed
        together.

        *Movedir* is the background task used to move directories between Paxos groups. Movedir is also used to
        add or remove replicas to Paxos groups because Spanner does not yet support in-Paxos configuration
        changes. Movedir is not implemented as a single transaction, so as to avoid blocking ongoing reads and
        writes on a bulky data move. Instead, movedir registers the fact that it is starting to move data and
        moves the data in the background. When it has moved all but a nominal amount of the data, it uses a
        transaction to atomically move that nominal amount and update the metadata for the two Paxos groups.

        A directory is also the smallest unit whose geographic-replication properties (or *placement*, for
        short) can be specified by an application. The design of our placement-specification language
        separates responsibilities for managing replication configurations. Administrators control two
        dimensions: the number and types of replicas, and the geographic placement of those replicas. They
        create a menu of named options in these two dimensions (e.g., North America, replicated 5 ways with 1
        witness). An application controls how data is replicated, by tagging each database and/or individual
        directories with a combination of those options. For example, an application might store each
        end-user’s data in its own directory, which would enable user A’s data to have three replicas in
        Europe, and user B’s data to have five replicas in North America.

        Spanner will shard a directory into multiple *fragments* if it grows too large. Fragments may be served
        from different Paxos groups (and therefore different servers). Movedir actually moves fragments, and
        not whole directories, between groups.
** Data Model
        Spanner exposes the following set of data features to applications: a data model based on schematized
        semi-relational tables, a query language, and general-purpose transactions.

        The application data model is layered on top of the directory-bucketed key-value mappings supported by
        the implementation. An application creates one or more *databases* in a universe. Each database can
        contain an unlimited number of schematized *tables*. Tables look like relational-database tables, with
        rows, columns, and versioned values.

        Spanner’s data model is not purely relational, in that rows must have names. More precisely, every
        table is required to have an ordered set of one or more primary-key columns. This requirement is where
        Spanner still looks like a key-value store: the primary keys form the name for a row, and each table
        defines a mapping from the primary-key columns to the non-primary-key columns.

        #+begin_src sql
CREATE TABLE Users {
  uid INT64 NOT NULL, email STRING
} PRIMARY KEY (uid), DIRECTORY;

CREATE TABLE Albums {
  uid INT64 NOT NULL, aid INT64 NOT NULL,
  name STRING
} PRIMARY KEY (uid, aid),
  INTERLEAVE IN PARENT Users ON DELETE CASCADE;
        #+end_src

        #+ATTR_LATEX: :width .5\textwidth :float nil
        #+NAME:
        #+CAPTION:
        [[../../images/papers/61.png]]

        Every Spanner database must be partitioned by clients into one or more hierarchies of tables. The
        table at the top of a hierarchy is a *directory table*. Each row in a directory table with key \(K\),
        together with all of the rows in descendant tables that start with \(K\) in lexicographic order, forms
        a directory.

        This interleaving of tables to form directories is significant because it allows clients to describe
        the locality relationships that exist between multiple tables, which is necessary for good
        performance in a sharded, distributed database. <<1>>
* TrueTime
        |------------------+------------------------------------------|
        | Method           | Returns                                  |
        |------------------+------------------------------------------|
        | \(TT.now()\)     | \(TTinterval:[earliest,latest]\)         |
        | \(TT.after(t)\)  | true if \(t\) has definitely passed      |
        | \(TT.before(t)\) | true if \(t\) has definitely not arrived |
        |------------------+------------------------------------------|

        Denote the absolute time of an event \(e\) by the function \(t_{abs}(e)\). TrueTime guarantees that
        for an invocation \(tt=TT.now()\), \(tt.earliest\le t_{abs}(e_{now})\le tt.latest\), where \(e_{now}\)
        is the invocation event.

        TrueTime is implemented by a set of *time master* machines per datacenter and a *timeslave daemon* per
        machine. The majority of masters have GPS receivers with dedicated antennas; these masters are
        separated physically to reduce the effects of antenna failures, radio interference, and spoofing. The
        remaining masters (which we refer to as *Armageddon masters*) are equipped with atomic clocks. An atomic
        clock is not that expensive: the cost of an Armageddon master is of the same order as that of a GPS
        master. All masters’ time references are regularly compared against each other. Each master also
        cross-checks the rate at which its reference advances time against its own local clock, and evicts
        itself if there is substantial divergence. Between synchronizations, Armageddon masters advertise a
        slowly increasing time uncertainty that is derived from conservatively applied worst-case clock drift.
        GPS masters advertise uncertainty that is typically close to zero.

        Every daemon polls a variety of masters to reduce vulnerability to errors from any one master. Some
        are GPS masters chosen from nearby datacenters; the rest are GPS masters from farther datacenters, as
        well as some Armageddon masters. Daemons apply a variant of Marzullo’s algorithm to detect and reject
        liars, and synchronize the local machine clocks to the non-liars. To protect against broken local
        clocks, machines that exhibit frequency excursions larger than the worst-case bound derived from
        component specifications and operating environment are evicted.

        Between synchronizations, a daemon advertises a slowly increasing time uncertainty. \(\epsilon\) is
        derived from conservatively applied worst-case local clock drift. \(\epsilon\) also depends on
        time-master uncertainty and communication delay to the time masters.

        In our production environment, \(\epsilon\) is typically a sawtooth function of time, varying from
        about 1 to 7 ms over each poll interval. \(\bar{\epsilon}\) is therefore 4 ms most of the time. The daemon’s
        poll interval is currently 30 seconds, and the current applied drift rate is set at 200
        microseconds/second, which together account for the sawtooth bounds from 0 to 6 ms. The remaining 1 ms
        comes from the communication delay to the time masters.
* Concurrency Control
** Timestamp Management
        |------------------------------------------+---------------------+------------------------------------|
        | Operation                                | Concurrency Control | Replica Required                   |
        |------------------------------------------+---------------------+------------------------------------|
        | Read-Write Transaction                   | pessimistic         | leader                             |
        | Read-Only Transaction                    | lock-free           | leader for timestamp, any for read |
        | Snapshot Read, client-provided timestamp | lock-free           | any                                |
        | Snapshot Read, client-provided bound     | lock-free           | any                                |
        |------------------------------------------+---------------------+------------------------------------|

        A read-only transaction must be predeclared as not having any writes.
*** Paxos Leader Leases
        Spanner’s Paxos implementation uses *timed leases* to make leadership long-lived (10 seconds by
        default). A potential leader sends requests for timed lease votes; upon receiving a quorum of lease
        votes the leader knows it has a lease. A replica extends its lease vote implicitly on a successful
        write, and the leader requests lease-vote extensions if they are near expiration. Define a leader’s
        *lease interval* as starting when it discovers it has a quorum of lease votes, and as ending when it no
        longer has a quorum of lease votes (because some have expired). Spanner depends on the following
        *disjointness invariant*:
        #+BEGIN_center
        For each Paxos group, each Paxos leader’s lease interval is disjoint from
        every other leader’s.
        #+END_center

        The Spanner implementation permits a Paxos leader to abdicate by releasing its slaves from their lease
        votes. To preserve the disjointness invariant, Spanner constrains when abdication is permissible.
        Define \(s_{max}\) to be the maximum timestamp used by a leader. Subsequent sections will describe
        when smax is advanced. Before abdicating, a leader must wait until \(TT.after(s_{max})\) is true.
*** Assining Timestamps to RW Transactions
        [[label:4.1.2]]
        Transactional reads and writes use two-phase locking. As a result, they can be assigned timestamps at
        any time when all locks have been acquired, but before any locks have been released. For a given
        transaction, Spanner assigns it the timestamp that Paxos assigns to the Paxos write that represents
        the transaction commit.

        Spanner depends on the following *monotonicity invariant*:
        #+BEGIN_center
        Within each Paxos group, Spanner assigns timestamps to Paxos writes in monotonically increasing order,
        even across leaders
        #+END_center
        This invariant is enforced across leaders by making use of the disjointness invariant: a leader must
        only assign timestamps within the interval of its leader lease. Note that whenever a timestamp \(s\) is
        assigned, \(s_{max}\) is advanced to \(s\) to preserve disjointness.

        Spanner also enforces the following *external consistency invariant*:
        #+BEGIN_center
        If the start of a transaction \(T_2\) occurs after the commit of a transaction \(T_1\) , then the
        commit timestamp of \(T_2\) must be greater than the commit timestamp of \(T_1\).
        #+END_center
        Define the start and commit events for a transaction \(T_1\) by \(e_i^{start}\) and \(e_i^{commit}\);
        and the commit timestamp of a transaction \(T_i\) by \(s_i\). The invariant becomes
        \begin{equation*}
        t_{abs}(e^{commit}_1)<t_{abs}(e_2^{start})\Rightarrow s_1<s_2
        \end{equation*}

        The protocol for executing transactions and assigning timestamps obeys /two/ rules, which together
        guarantee this invariant, as shown below. Define the arrival event of the commit request at the
        coordinator leader for a write \(T_i\) to be \(e_i^{server}\).

        * *Start*: The coordinator leader for a write \(T_i\) assigns a commit timestamp \(s_i\) no less than the
          value of \(TT.now().latest\), computed after \(e_i^{server}\)
        * *Commit Wait*: The coordinator leader ensures that clients cannot see any data committed by \(T_i\)
          until \(TT.after(s_i)\) is true. Commit wait ensures that \(s_i\) is less than the absolute commit
          time of \(T_i\), or \(s_i<t_{abs}(e_i^{commit})\). Now we get
          \begin{align*}
          s_1&<t_{abs}(e_1^{commit})\tag{commit wait}\\
          t_{abs}(e_1^{commit})&<t_{abs}(e_2^{start})\tag{assumption}\\
          t_{abs}(e_2^{start})&\le t_{abs}(e_2^{server})\tag{causality}\\
          t_{abs}(e_2^{server})&\le s_2\tag{start}\\
          s_1&<s_2
          \end{align*}

*** Serving Reads at a Timestamp
        [[label:4.1.3]]
        The monotonicity invariant allows Spanner to correctly determine whether a replica's state is
        sufficiently up-to-date to satisfy a read.

        Every replica tracks a value called *safe time* \(t_{safe}\) which is the maximum timestamp at which a
        replica is up-to-date. A replica can satisfy a read at a timestamp \(t\) if \(t\le t_{safe}\).

        Define \(t_{safe}=\min(t_{safe}^{Paxos},t_{safe}^{TM})\), where each Paxos state machine has a safe
        time \(t_{safe}^{Paxos}\) and each transaction manager has a safe time \(t_{safe}^{TM}\).

        \(t_{safe}^{Paxos}\) is the timestamp of the highest-applied Paxos write. Because timestamps increase
        monotonically and writes are applied in order, writes will no longer occur at or below
        \(t_{safe}^{Paxos}\) with respect to Paxos.

        \(t_{safe}^{TM}\) is \(\infty\) at a replica if there are zero prepared (but not committed)
        transactions—that is, transactions in between the two phases of two-phase commit. (For a participant
        slave, \(t_{safe}^{TM}\) actually refers to the replica’s leader’s transaction manager, whose state
        the slave can infer through metadata passed on Paxos writes.) If there are any such transactions, then
        the state affected by those transactions is indeterminate: a participant replica does not know yet
        whether such transactions will commit. The commit protocol ensures that every participant knows a
        lower bound on a prepared transaction’s timestamp. Every participant leader (for a group \(g\)) for a
        transaction \(T_i\) assigns a prepare timestamp \(s^{prepare}_{i,g}\) to its prepare record. The
        coordinator leader ensures that the transaction’s commit timestamp \(s_i\ge s_{i,g}^{prepare}\) over
        all participant groups \(g\). Therefore, for every replica in a group \(g\), over all transactions
        \(T_i\) prepared at \(g\), \(t_{safe}^{TM}=\min_i(s_{i,g}^{prepare}-1)\) over all transactions
        prepared at \(g\).

*** Assigning Timestamps to RO Transactions
        A read-only transaction executes in two phases:
        1. Assign a timestamp \(s_{read}\)
        2. execute the transaction's reads as snapshot reads at \(s_{read}\)

        The simple assignment of \(s_{read}=TT.now().latest\), at any time after a transaction starts,
        preserves external consistency by an argument analogous to that presented for writes in Section
        [[ref:4.1.2]].

        However, such a timestamp may require the execution of the data reads at \(s_{read}\) to block if
        \(s_{safe}\) has not advanced sufficiently. (In addition, note that choosing a value of \(s_{read}\) may also
        advance \(s_{max}\) to preserve disjointness.) To reduce the chances of blocking, Spanner should
        assign the oldest timestamp that preserves external consistency.

** Details
*** Read-Write Transactions
        Like Bigtable, writes that occur in a transaction are buffered at the client until commit. As a
        result, reads in a transaction do not see the effects of the transaction’s writes.

        Reads within read-write transactions use wound-wait to avoid deadlocks.
        1. The client issues reads to the leader replica of the appropriate group, which acquires read locks
           and then reads the most recent data.
        2. While a client transaction remains open, it sends keepalive messages to prevent participant leaders
           from timing out its transaction.
        3. When a client has completed all reads and buffered all writes, it begins two-phase commit.
        4. The client chooses a coordinator group and sends a commit message to each participant’s leader with
           the identity of the coordinator and any buffered writes.

        Having the client drive two-phase commit avoids sending data twice across wide-area links.

        A non-coordinator-participant leader first acquires write locks. It then chooses a prepare timestamp
        that must be /larger than any timestamps it has assigned to previous transactions/ (to preserve
        monotonicity), and logs a prepare record through Paxos. Each participant then notifies the coordinator
        of its prepare timestamp.

        The coordinator leader also first acquires write locks, but skips the prepare phase. It chooses a
        timestamp for the entire transaction after hearing from all other participant leaders. The commit
        timestamp \(s\) must be greater or equal to all prepare timestamps (to satisfy the constraints
        discussed in Section ref:4.1.3), greater than \(TT.now().latest\) at the time the coordinator received
        its commit message, and greater than any timestamps the leader has assigned to previous transactions
        (again, to preserve monotonicity). The coordinator leader then logs a commit record through Paxos (or
        an abort if it timed out while waiting on the other participants).

        Before allowing any coordinator replica to apply the commit record, the coordinator leader waits until
        \(TT.after(s)\), so as to obey the commit-wait rule described in Section ref:4.1.2. Because the
        coordinator leader chose s based on \(TT.now().latest\), and now waits until that timestamp is
        guaranteed to be in the past, the expected wait is at least \(2\cdot\bar{\epsilon}\). This wait is
        typically overlapped with Paxos communication. After commit wait, the coordinator sends the commit
        timestamp to the client and all other participant leaders. Each participant leader logs the
        transaction’s outcome through Paxos. All participants apply at the same timestamp and then release
        locks.
*** Read-Only Transactions
        Assigning a timestamp requires a negotiation phase between all of the Paxos groups that are involved
        in the reads. As a result, Spanner requires a *scope* expression for every read-only transaction, which
        is an expression that summarizes the keys that will be read by the entire transaction. Spanner
        automatically infers the scope for standalone queries.

        If the scope’s values are served by a single Paxos group, then the client issues the read-only
        transaction to that group’s leader. (The current Spanner implementation only chooses a timestamp for a
        read-only transaction at a Paxos leader.) That leader assigns \(s_{read}\) and executes the read. For
        a single-site read, Spanner generally does better than \(TT.now().latest\). Define \(LastTS()\) to be
        the timestamp of the last committed write at a Paxos group. If there are no prepared transactions, the
        assignment \(s_{read}=LastTS()\) trivially satisfies external consistency: the transaction will see
        the result of the last write, and therefore be ordered after it.

        If the scope’s values are served by multiple Paxos groups, there are several options. The most
        complicated option is to do a round of communication with all of the groups’s leaders to negotiate
        \(s_{read}\) based on \(LastTS()\). Spanner currently implements a simpler choice. The client avoids a
        negotiation round, and just has its reads execute at \(s_{read} = TT.now().latest\) (which may wait
        for safe time to advance). All reads in the transaction can be sent to replicas that are sufficiently
        up-to-date.
*** Schema-Change Transactions
        TrueTime enables Spanner to support atomic schema changes. It would be infeasible to use a standard
        transaction, because the number of participants (the number of groups in a database) could be in the
        millions. Bigtable supports atomic schema changes in one datacenter, but its schema changes block all
        operations. A Spanner schema-change transaction is a generally non-blocking variant of a standard
        transaction.
        1. It is explicitly assigned a timestamp in the future, which is registered in the prepare phase. As a
           result, schema changes across thousands of servers can complete with minimal disruption to other
           concurrent activity
        2. Reads and writes, which implicitly depend on the schema, synchronize with any registered
           schema-change timestamp at time \(t\): they may proceed if their timestamps precede \(t\), but they
           must block behind the schema-change transaction if their timestamps are after \(t\).

           Without TrueTime, defining the schema change to happen at \(t\) would be meaningless.
*** Refinements
        \(t_{safe}^{TM}\) as defined above has a weakness, in that a single prepared transaction prevents
        \(t_{safe}\) from advancing. As a result, no reads can occur at later timestamps, even if the reads do
        not conflict with the transaction. Such false conflicts can be removed by augmenting \(t_{safe}^{TM}\)
        with a fine-grained mapping from key ranges to prepared-transaction timestamps. This information can
        be stored in the lock table, which already maps key ranges to lock metadata. When a read arrives, it
        only needs to be checked against the fine-grained safe time for key ranges with which the read
        conflicts.

        \(LastTS()\) as defined above has a similar weakness: if a transaction has just committed, a
        non-conflicting read-only transaction must still be assigned \(s_{read}\) so as to follow that
        transaction. As a result, the execution of the read could be delayed. This weakness can be remedied
        similarly by augmenting \(LastTS()\) with a fine-grained mapping from key ranges to commit timestamps
        in the lock table.

        When a read-only transaction arrives, its timestamp can be assigned by taking the maximum value of
        \(LastTS()\) for the key ranges with which the transaction conflicts, unless there is a conflicting
        prepared transaction (which can be determined from fine-grained safe time).

        \(t_{safe}^{Paxos}\) as defined above has a weakness in that it cannot advance in the absence of Paxos
        writes. That is, a snapshot read at \(t\) cannot execute at Paxos groups whose last write happened
        before \(t\). Spanner addresses this problem by taking advantage of the disjointness of leader-lease
        intervals. Each Paxos leader advances \(t_{safe}^{Paxos}\) by keeping a threshold above which future
        writes’ timestamps will occur: it maintains a mapping \(MinNextTS(n)\) from Paxos sequence number
        \(n\) to the minimum timestamp that may be assigned to Paxos sequence number \(n+1\). A replica can
        advance \(t_{safe}^{Paxos}\) to \(MinNextTS(n) − 1\) when it has applied through \(n\).

        A single leader can enforce its \(MinNextTS()\) promises easily. Because the timestamps promised by
        \(MinNextTS()\) lie within a leader’s lease, the disjointness invariant enforces \(MinNextTS()\)
        promises across leaders. If a leader wishes to advance \(MinNextTS()\) beyond the end of its leader
        lease, it must first extend its lease. Note that \(s_{max}\) is always advanced to the highest value
        in \(MinNextTS()\) to preserve disjointness.

        A leader by default advances \(MinNextTS()\) values every 8 seconds. Thus, in the absence of prepared
        transactions, healthy slaves in an idle Paxos group can serve reads at timestamps greater than 8
        seconds old in the worst case. A leader may also advance \(MinNextTS()\) values on demand from slaves.
* Evaluation
* Problems
        1. [[1]]: Why?
        2. What is \(\epsilon\)
        3. Why do we need long lease?
        4.

* References
<<bibliographystyle link>>
bibliographystyle:alpha

<<bibliography link>>
bibliography:/Users/wu/notes/references.bib
