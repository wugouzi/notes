#+title: Adaptive Statistics In Oracle 12c
#+AUTHOR:
#+LATEX_HEADER: \input{/Users/wu/notes/preamble.tex}
#+EXPORT_FILE_NAME: ../../latex/papers/database/adaptive_statistics_in_oracle_12c.tex
#+LATEX_HEADER: \graphicspath{{../../../paper/database/}}
#+LATEX_HEADER: \def \hmu {\hat{\mu}}
#+LATEX_HEADER: \def \hM {\what{\bM}}
#+LATEX_HEADER: \def \hsigma {\what{\sigma}}
#+OPTIONS: toc:nil
#+STARTUP: shrink
#+LATEX_HEADER: \definecolor{mintedbg}{rgb}{0.99,0.99,0.99}
#+LATEX_HEADER: \usepackage[cachedir=\detokenize{~/miscellaneous/trash}]{minted}
#+LATEX_HEADER: \setminted{breaklines,
#+LATEX_HEADER:   mathescape,
#+LATEX_HEADER:   bgcolor=mintedbg,
#+LATEX_HEADER:   fontsize=\footnotesize,
#+LATEX_HEADER:   frame=single,
#+LATEX_HEADER:   linenos}

* Introduction
        #+ATTR_LATEX: :width .8\textwidth :float nil
        #+NAME: f1
        #+CAPTION: Architecture of SQL Engine
        [[../../images/papers/167.png]]

        The Code Generator (CG) stores the optimizer decisions into a structure called a cursor. All cursors
        are stored in a shared memory area of the database server called the Cursor Cache (CC). The goal of
        caching cursors in the cursor cache is to avoid compiling the same SQL statement every time it is
        executed by using the cached cursor for subsequent executions of the same statement.

        If a matching cursor is found then it is used to execute the  otherwise the SQL compiler builds a new
        one. Several cursors may exist for the same SQL text, e.g. if the same SQL text is submitted by two
        users that have different authentication rules.

** Effect of Cardinality on Plan Generation

** Cardinality Estimation errors
        #+ATTR_LATEX: :width .8\textwidth :float nil
        #+NAME: f2
        #+CAPTION: Execution Plan for Q1
        [[../../images/papers/170.png]]


** Contributions
        automagic creation of auxiliary statistics structures based on workload analysis

        validation of optimizer statistics using actual table data
* Related Work
* Adaptive Statistics
** Architecture Overview
        This technique consists of computing the statistics (selectivity/cardinality, even first class
        statistics like number of distinct values) during optimization of the SQL statement. This process
        happens in the Cost Estimator module. The statistics are computed by executing a SQL statement against
        the table with relevant predicates. This technique can be used to estimate cardinality of operations
        that involve only single table as well as more complex operations that involve join, group by etc.
        These kinds of queries are referred to as statistics queries. Statistics queries are executed in most
        stages of plan generation. Some example statistics queries executed while optimizing the query Q1
        #+begin_src sql
-- Q1
SELECT
    prod_name,
    sum(amount_sold) amount_sold
FROM
    products p,
    customers c,
    sales s
WHERE
    p.prod_category = 'Electronics'
    AND p.prod_subcategory = 'Y Box Games'
    AND p.prod_id = s.prod_id
    AND c.cust_state_province = 'CA'
    AND c.cust_id = s.cust_id
GROUP BY
    prod_name;
        #+end_src
        are
        1. Query Q3 below estimates the cardinality when costing full table scan of products. It provides the
           cardinality after applying both predicates on this table.
           #+begin_src sql
-- Q3
SELECT
    sum(c1)
FROM (
    SELECT
        1 AS c1
    FROM
        products p
    WHERE (p.prod_subcategory = 'Y Box Games')
    AND (p.prod_category = 'Electronics'));
           #+end_src
        2. Query Q4 estimates the cardinality when costing the access using the index on ~PROD_SUBCATEOGORY~
           column.
           #+begin_src sql
-- Q4
SELECT
    c1
FROM (
    SELECT
        /*+ index(p products_prod_cat_ix) */
        count(*) AS c1
    FROM
        products p
    WHERE (p.prod_category = 'Electronics'));
           #+end_src
        3. Query Q5 estimates the cardinality of the join between sales and products.
           #+begin_src sql
-- Q5
SELECT
    /*+opt_estimate(@innerquery,table,p#2,rows=8)
    */
    sum(c1)
FROM (
    SELECT
        /*+ qb_name(innerQuery) */
        1 AS c1
    FROM
        sales SAMPLE BLOCK (47, 8) SEED (1) s#0,
        products p#2
    WHERE (p # 2.prod_subcategory = 'Y Box Games')
    AND (p # 2.prod_category = 'Electronics')
    AND (p # 2.prod_id = s # 0.prod_id)) innerQuery
           #+end_src

        #+ATTR_LATEX: :width .8\textwidth :float nil
        #+NAME: f3
        #+CAPTION: Plan for Q1 using statistics queries
        [[../../images/papers/168.png]]
        Figure ref:f3 shows the execution plan generated by the optimizer using statistics queries.

        Executing statistics queries as part of optimizing user SQL statements incur additional optimization
        time. Oracle employs several techniques to reduce this overhead. We describe two of these, adaptive
        sampling, and SQL plan directives (SPDs), below.

        #+ATTR_LATEX: :width .7\textwidth :float nil
        #+NAME: f4
        #+CAPTION: Adaptive Statistics Flow
        [[../../images/papers/169.png]]


        Adaptive Sampling: Use sample of the table in statistics queries to estimate the cardinality. Sampling
        is done by Statistics Query Engine as shown in Figure ref:f4. This module is responsible for computing
        the optimal sample size, executing the statistics queries within a specified time budget, using the
        full or partial results from statistics queries to derive the cardinality estimate, and storing the
        result of statistics queries in SPDs.

        SQL Plan Directives: SPDs are persistent objects that have run time information of SQL or SQL
        constructs. They are used for the following purposes.
        * For tracking the SQL constructs that caused misestimates: This happens in the Execution Engine when
          the cardinality estimate for a particular construct in an operation is significantly different from
          the actual rows produced by the operation. Cost Estimator requests estimates from the Statistics
          Query Engine only for the constructs for which misestimates are recorded as SPDs. This is to avoid
          executing statistics queries for each and every construct. To avoid the overhead of tracking in the
          Execution Engine, the directives are first recorded in Directive Cache in memory (SGA) before it is
          flushed to disk by background process (MMON).
        * Statistics collector (DBMS_STATS) also looks at the SPDs for constructs with a misestimate in the
          Dictionary and gathers statistics for them. For example, if the SQL construct has multiple equality
          predicates, statistics collector will collect statistics for the group of columns in the predicates.
          This allows the statistics collector to collect statistics only for group of columns that caused the
          misestimate.
        * For persistently storing the result of statistics queries to avoid repeated execution of the same
          statistics queries: Statistics Query Engine first checks if there is a SPD that has the result of
          the statistics query in Dictionary and uses it if the result is still valid. If the result is stale,
          it executes the statistics query to get the correct result and stores the new result in directive.
** Adaptive Sampling Methodology
        We rely on sampling to limit the overhead when reading data from tables to validate optimizer
        statistics. If an access structure (e.g. index) is efficient then we skip sampling. For the latter
        case, the index is forced using a hint as in example Q4. This section describes the algorithms used to
        compute an appropriate sample size, and extrapolating the statistics value to the full data .

        Formally, the adaptive sampling addresses the following problem: given a table \(T\) and a set of
        operators applied to \(T\), provide an estimate of the cardinality of the resulting dataset, based on a
        sample. The operators applied to \(T\) include table filters, joins, group by etc. The adaptive
        sampling algorithm consists of iterating through the following four steps, until the quality test at
        step 3 is successful:

        Given \(n\), number of blocks in the initial sample, and a query \(Q\):
        1. Sample: Randomly sample \(n\) blocks from \(T\). Apply the operators on the sample.
        2. Cardinality Estimate: Estimate the cardinality of query \(Q\) for the entire dataset, based on the
           resulting cardinality after applying the operators on this sample and samples from previous iterations (if any).
        3. Quality Test: Calculate a confidence interval around the cardinality estimate, and perform a
          quality test on the confidence interval.
        4. Next Sample Size Estimate: If the quality test succeeds, stop. If the test fails, calculate \(n_{next}\),
           the number of additional blocks required to be sampled, so that the resultant sample size meets the
           quality test (with a certain probability). Set \(n=n_{next}\).

        As mentioned earlier, we sample a random set of blocks from \(T\), as opposed to a random set of rows.
        This means that internal correlation within rows in a block have to be taken into account during the
        variance calculation, possibly resulting in larger required sample sizes. However, block sampling is
        far cheaper than row sampling, which makes this a reasonable trade-off.

        Sampling at the block level introduces another complication: it is expensive to remember for each row
        which block it originated from, making a straightforward estimate of the block-level variance
        impossible. To address this problem, we rely on two statistical properties:
        1. Central Limit Theorem
        2. The sum of square of \(K\) independent standard normal random variables follows a chi-squared
           distribution with \(K\) degrees of freedom

        The basic approach, then is, to
        * Take multiple block samples of sufficient size, so that each can be modeled as a sample from a
          Normal distribution, and
        * Model the variance across samples as a chi-squared distribution, to establish confidence intervals
          on the variance, and derive the bounds on across-block variance from the bounds on across-sample
          variance.
** Mathematical Details
*** Problem Formulation
        1. Cardinality Estimate and Confidence Interval: Arrive at an unbiased estimate \(\hM\) of the
           true cardinality \(\bM\) of query \(Q\). Establish a 95% lower bound \(\hM_L\) and a 95% upper
           bound \(\hM_U\) s.t. \(\hM_L\le\hM\le\hM_U\) with 95% possibility
        2. Quality Test: For a pre-determined \(\lambda\), check if \(\hM_U\le(1+\lambda)\hM\). For example, if \(\lambda=1\), we
           can be 95% confident that \(\hM_U\le2\hM\).
        3. Next Sample Size Estimation: Given the current cardinality estimate, and information about the
           samples taken till date, estimate \(n_k\), the size of the next sample to take, so that the
           condition \(\hM_U\le(1+\lambda)\hM\) is likely to be met
*** Solution Outline
        Let \(\mu\) be the ground truth mean number of rows matching the query per block (referred to as the per
        block query cardinality). Assuming that the number of blocks \(B\) constituting the table is known, it
        is sufficient to estimate \(\mu\), as \(M=\mu B\)

        After \(K\) rounds of adaptive sampling, let the total number of blocks sampled so far be \(N\), and
        let the number of rows matching \(Q\) in the \(i\)th block be \(x_i\). Then \(\mu\) is estimated as:
        \begin{equation}
        \label{eq1}
        \hmu=\frac{\sum_{i=1}x_i}{N}
        \end{equation}
        The confidence interval around \(\hmu\) can be calculated using the Central Limit Theorem, which
        states that, for a simple random sample \(x_1,\dots,x_N\) from a population with mean \(\mu\) and
        finite variance \(\sigma^2\), the sample mean (calculated by eqref:eq1) is an unbiased estimator of
        the population mean \(\mu\), and is normally distributed as
        \begin{equation*}
        \hmu=N(\mu,\frac{\sigma^2}{N})
        \end{equation*}
        Using properties of the Normal distribution, after \(N\) samples, the \(100(1-\alpha)\)% upper
        confidence bound on \(\mu\) is given by:
        \begin{equation*}
        \mu_{UB}=\hmu+z_\alpha\cdot\frac{\sigma}{\sqrt{N}}
        \end{equation*}
        Here \(z_\alpha\) is the \(100(1-\alpha)\%\) percentile standard score of the standard normal
        distribution. We use \(\alpha=0.025\), so that \(z_\alpha\).

        To establish this confidence interval, we need to estimate \(\sigma\), the query cardinality standard
*** Variance Estimation of Per Block Query Cardinality
        An alternate way to calculate \(\hmu\) is in terms of the number of matching rows observed per round
        of sampling. Let the number of rounds of sampling completed be \(K\), \(K\ge 2\). Let \(n_i\) be the
        number of blocks sampled in the \(i\)th round of adaptive sampling, and let \(s_i\) be the number of
        rows matching \(Q\) found in the sample taken in the \(i\)th round. Then
        \begin{equation*}
       \hmu=\frac{\sum_{i=1}^Ks_i} {\sum_{i=1}^Kn_i}
        \end{equation*}
        An unbiased estimate \(\hM\) of \(M\) is then given by \(\hM=\hmu B\), that is, \(E(\hM)=M\)

        While we do not have access to the across-block variance \(\sigma\), we can compare how the estimate
        of the same mean changes from round to round. We use these values to arrive at an estimate of
        \(\sigma\). Let \(x_i\) be the observed per block query cardinality for the \(i\)th sample, defined as
        \(x_i=\frac{s_i}{n_i}\). By  the Central Limit theorem, \(x_i\) can be modeled as being sampled from a
        normal random variable, \(X_i=N(\mu,\sigma_i^2)\), where \(\sigma_i=\frac{\sigma}{\sqrt{n_i}}\). In
        other words, \(\frac{X_i-\mu}{\sigma_i}\) follows a standard normal distribution. As the sum of square
        of \(K\) standard normal random variables follows a Chi-squared distribution with \(K\) degrees of
        freedom \(\chi_K\), the following holds after \(K\) rounds:
        \begin{gather*}
        \sum_{i=1}^K\left( \frac{X_i-\mu}{\sigma_i} \right)^2\sim\chi_K\\
        \Rightarrow\sum_{i=1}^Kn_i\left( \frac{X_i-\mu}{\sigma} \right)^2\sim\chi_K\\
        \Rightarrow\frac{1}{\sigma^2}\sum_{i=1}^Kn_i(X_i-\mu)^2\sim\chi_K
        \end{gather*}
        After \(K\) rounds, the \(\beta=97.5\%\) upper bound on \(\sigma^2\), written as \(\hsigma^2_{UB}\),
        can be calculated as:
        \begin{gather*}
        \frac{1}{\hsigma^2_{UB}}\sum_{i=1}^Kn_i(X_i-\mu)^2=\chi_{K,\beta}\\
        \hsigma^2_{UB}=\frac{\sum_{i=1}^Kn_i(X_i-\mu)^2}{\chi_{K,\beta}}
        \end{gather*}
        We know that with 95% probability, \(\sigma^2\) is less than \(\hsigma^2_{UB}\). Here\(\chi_{K,0.95}\)
        is the value \(v\) s.t. \(P(\chi_{K}\ge v)=0.05\).

        For \(K=2\), \(\chi_{K,0.95}=0.103\)

        The 92.5% upper bound on the per block cardinality is then given by the formula:
        \begin{equation*}
        \hmu_{UB}=\hmu+1.96\cdot\hsigma_{UB}
        \end{equation*}
        Similarly, the lower bound \(\hmu_{LB}=\hmu-1.96\cdot\hsigma_{UB}\).

        The reason we arrive at a 92.5% upper bound on the cardinality estimate, is due to the probabilistic
        approximation we do at two stages: while estimating the standard deviation, and while estimating the
        mean. Combining the two probabilistic estimates using the union bound. Combining the two probabilistic
        estimates using the union bound, we get
        \begin{equation*}
        P(\mu>\hmu_{UB}\vee\sigma>\hsigma_{UB})<P(\mu>\hmu_{UB})+P(\sigma>\hsigma_{UB})=0.075
        \end{equation*}

        Therefore, since the overall probability of error is less than 7.5%, the result has at least a 92.5%
        confidence. Similarly, it can be shown that
        \begin{equation*}
        P(\mu>\hmu_{UB}\vee\mu<\hmu_{LB}\vee\sigma>\hsigma_{UB})<0.1
        \end{equation*}
        In other words, setting \(α=0.025\), \(β=0.95\) gives us a 90% confidence interval on the cardinality
        estimate.

        While the above approach requires at least two rounds of sampling before arriving at a confidence
        interval, it has the following advantage: it can calculate an accurate confidence interval from a
        block sample, without requiring any block-level information. This is very useful, as storing block
        level information per row is expensive computationally and in terms of memory usage.
*** Next Sample Size Calculation
        Let the number of rounds of sampling completed be \(K-1\), with a total of \(N_{K-1}\) blocks sampled.
        At the end of the \(K\)th round of sampling, we would like the following condition to hold, so that no
        more rounds are required
        \begin{gather*}
        \hat{\mu}+z_{\alpha}\frac{\hat{\sigma}_{UB}}{\sqrt{N}}\le(1+\lambda)\hat{\mu}\\
        \hat{\sigma}_{UB}^2\le\frac{\lambda^2\hat{\mu}^2N}{z^2}\\
        \frac{\sum_{i=1}^Kn_i(X_i-\mu)^2}{\chi_{K,\beta}}\le\frac{\lambda^2\hat{\mu}^2N}{z^2}\\
        N\ge\frac{z^2}{\lambda^2\hat{\mu}^2\chi_{K,\beta}}\left( \sum_{i=1}^{K-1}n_i(x_i-\hat{\mu})^2+n_K(x_K-\hat{\mu})^2 \right)
        \end{gather*}
        Since \((x_K-\hat{\mu})^2\) is not known until after the \(K\)th sample, we use an estimate.
        By the Central Limit Theorem:
        \begin{equation*}
        E[(x_K-\hat{\mu})^2]=\frac{\sigma^2}{n_K}
        \end{equation*}
        Now we get
        \begin{gather*}
        N\ge\frac{z^2}{\lambda\hat{\mu}^2\chi_{K,\beta}}\left( \sum_{i=1}^{K-1}n_i(x_i-\hat{\mu})^2+\sigma^2 \right)\\
        N\ge\frac{z^2}{\lambda\hat{\mu}^2\chi_{K,\beta}}\left( \sum_{i=1}^{K-1}n_i(x_i-\hat{\mu})^2 \right)
        \left( 1+\frac{1}{\chi_{K-1,\beta}} \right)
        \end{gather*}
*** Special Case: No Matching Rows
        In the case where no matching rows are found in the two initial samples, we follow the following
        strategy: a sample of double the size in the previous iteration is taken, till at least one matching
        row is found, or till the total number of blocks sampled reaches a pre-determined threshold. If no
        matching rows are found till the threshold is reached, the query cardinality is estimated as zero. If
        matching rows are found in the j-th iteration, the next sample is calculated using  \(j = K-1\) and
        \(x_i=0\) for iteration \(i<j\).
*** Sampling for Complex Operators
        To get optimal plans for complex statistics queries, Oracle sends estimates generated for the parts of
        the statement earlier. This is done using ~opt_estimate~ hints. An example hint can be seen in Q5.

        Currently Oracle uses sampling only for the largest table in the complex statistics query and
        estimates the result using the formulas mentioned before.
*** Time Budget and enforcement for statistics queries
** SQL Plan Directives
        SPDs are persistent objects that have run time information of SQL or SQL constructs. These objects are
        stored in the Dictionary, which can be used to improve statistics gathering and query optimization on
        future executions. Currently Oracle has two types of directives – “Adaptive Sampling” and “Adaptive
        Sampling Result” directives. They are described next.
*** Adaptive Sampling Directives
        Adaptive sampling directives are created if execution-time cardinalities are found to deviate from
        optimizer estimates. They are used by the optimizer to determine if statistics queries (using
        sampling) should be used on portions of a query. Also, these types of directives are used by the
        statistics gathering module to determine if additional statistics should be created (e.g. extended
        statistics). The directives are stored based on the constructs of a query rather than a specific
        query, so that similar queries can benefit from the improved estimates.

        Creation of directives is completely automated. The execution plan can be thought of as a tree with
        nodes that evaluates different SQL constructs of the query. During compilation of the query (more
        precisely in Code Generator), the constructs evaluated in these nodes are recorded in a compact form
        in the system global memory area (SGA), and can be looked up later using a signature. The signature
        enables sharing of a construct between queries.

        For example consider Figure [[ref:f2]], node 6 of the query plan for Q1 scans the ~Products~ table with
        predicates on columns ~PROD_CATEGORY~, ~PROD_SUBCATEGORY~. The signature in this case will be built using
        ~PRODUCTS~, ~PROD_CATEGORY~, ~PROD_SUBCATEGORY~. That is, the signature does not use the values used in the
        predicates. So if another query has predicates on the same set of columns but with different values,
        the construct in the SGA can be shared.

        At the end of execution of every query, the Execution Engine goes over all nodes in the execution plan
        starting from the leaf nodes and marks those SQL constructs corresponding to node in SGA, whose
        cardinality estimate is significantly different from the actual value. The nodes whose children have
        misestimates are not marked, as the misestimate can be caused by a misestimate in the children. For
        example, in Q1, the optimizer has misestimated the cardinality for products table in node 6. The
        construct in this node (~PRODUCTS~ table with ~PROD_CATEGORY~ and ~PROD_SUBCATEGORY~) is marked while that
        of the parent nodes 5, 4 etc are not. The SQL constructs that are marked (because they caused a
        misestimate) are used for creating the directive. The creation is done periodically by a separate
        background process, called ~MMON~. The directives are stored persistently in Dictionary along with the
        objects that constitute constructs. They are called directive objects. In our example, ~PRODUCTS~,
        ~PROD_CATEGORY~, ~PROD_SUBCATEGORY~ are the directive objects created for the misestimate in node 6 of Q1.
        The directive can be used for other queries where these directive objects are present.

        Cost Estimator estimates the cardinality for SQL constructs using the available pre-computed
        statistics in Dictionary in the normal way. Once this is done, it will look for any directive that
        exists for the construct. It will request Statistics Query Engine to execute statistics adaptive
        sampling query and get the more accurate estimate if a directive exists for the construct.

        One straight forward way to check if a directive exists for a construct is to build the signature of
        the construct and see if there exists a directive with the same signature. To maximize the usage of
        directives and reduce the number of directives created, instead of doing an exact match on the
        signature, we check if there is a directive that has a subset of objects of the current construct
        being estimated. If we find such a directive, we execute the statistics adaptive sampling query. For
        example, the directives created for products table during execution of Q1 can be used by another query
        with an additional predicate on products table.

        As mentioned earlier, we do not create directives for a node if there is misestimate for its children.
        Instead a directive for the children is created. If the misestimate in the parent node still manifests
        without any misestimates in child nodes after using the directives for children, a directive for the
        parent node is created. In this case the misestimate in parent is not caused by children. The overall
        process is shown in Figure [[ref:f4]].
*** Adaptive Sampling Result Directives
        Adaptive sampling directives reduces the number of statistics queries executed in the system by
        executing statistics queries only if there is a directive created for the construct it is estimating
        cardinality for. For the statistics queries executed, it still adds an overhead to compilation. The
        same statistics queries may get executed for several top level SQL statements. We use directive
        infrastructure to avoid the overhead of this repeated execution.

        The result of the statistics query is stored in a directive of type Adaptive Sampling Result. This
        type directive has the following directive objects:
        * The tables along with its current number of rows referenced in the statistics query.
        * The SQL identifier (sqlid). It is a hash value created based on the SQL text.
        * A signature of the environment (bind variables etc) in which the statistics query is executed.

        This type of directive is created immediately after executing a statistics query in Statistics Query
        Engine. The usage of the result stored in these type of directives is as follows:
        * The statistics query engine first checks if a directive is created for the statistics query before
          executing the statement. The lookup is done based on the sqlid of the statistics query.
        * If there is a directive, we check if the result stored in the directive is stale. The result can be
          stale if some DML has happened for any of the tables involved in the statistics query. If the
          current number of rows (maintained in SGA) for any of the tables is significantly different from
          what is stored in the directive, we consider the directive as stale.
        * If a directive is stale, we mark it as such and execute the statistics query to populate the new
          result in the directive.
*** Automatic extended statistics
        In real-world data, there is often a relationship or correlation between the data stored in different
        columns of the same table. For example, in the products table, the values in ~PROD_SUBCATEGORY~ column
        are influenced by the values ~PROD_CATEGORY~. The optimizer could potentially miscalculate the
        cardinality estimate if multiple correlated columns from the same table are used in the where clause
        of a statement. Extended statistics allows capturing the statistics for group of columns and helps the
        optimizer to estimate cardinality more accurately ([[https://blogs.oracle.com/optimizer/post/extended-statistics][Extended Statistics]]). Creation of extended statistics was manual when it
        was introduced in Oracle 11g.

        In Oracle 12c, the extended statistics are automatically created for all the column groups found in
        the SQL constructs that caused the misestimate. This avoids the creation of extended statistics for
        unnecessary group of columns that are not causing a misestimate in cardinality and suboptimal plans.
        The automatic creation of extended statistics relies on the SPD infrastructure explained in section
        3.3.1. The adaptive sampling directives maintain different states depending on whether the
        corresponding construct has the relevant extended statistics or not. It goes through the following
        state changes, as shown in Figure [[ref:f5]].

        #+ATTR_LATEX: :width .8\textwidth :float nil
        #+NAME: f5
        #+CAPTION: SPD State Transition Diagram
        [[../../images/papers/171.png]]

        * ~NEW~: When a directive is created as described in section 3.3.1 it will be in the NEW state.
        * ~MISSING_EXT_STATS~: When optimizer finds directives corresponding to the constructs in the query it
          will check if there is a column group in the construct. If no extended statistics are created yet
          for the group then those column groups will be recorded in the dictionary tables. The state of the directive will be changed to MISSING_EXT_STATS.
        * ~HAS_EXT_STATS~: The statistics gathering process (either manual, or automatic job) creates extended
          statistics for the groups that are monitored. If optimizer finds the extended statististics for the
          column group corresponding to the directive, it will change the state to ~HAS_EXT_STATS~. Statistics
          queries are not executed for the directives with ~HAS_EXT_STATS~ state. If the extended statistics
          produce more accurate estimate, it avoids the overhead of executing statistics queries.
        * ~PERMANENT~: If Execution engine finds misestimate for a construct and if the construct has a
          directive with state ~HAS_EXT_STATS~, it goes throgh a state transition to ~PERMANENT~ and will use
          statistics queries from then onwards for the directive. This is because the extended statistics in
          previous state did not help to correct the misestimate for some queries.

        All the states except ~HAS_EXT_STATS~ execute statistics queries.
* Performance Evaluation
        Adaptive statistics feature is available in Oracle 12c which has been in production for over 4 years
* Problems


* References
<<bibliographystyle link>>
bibliographystyle:alpha

\bibliography{/Users/wu/notes/notes/references.bib}
