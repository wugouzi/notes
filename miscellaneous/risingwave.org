#+title: RisingWave

* Goal
** Week 1 [4/4]
    * [X] debezium：risingwave 类型到 mysql/pg/mongodb https://debezium.io/documentation/reference/2.2/
    * [X] jdbc：risingwave 类型到 mysql/pg/tidb
        https://dev.mysql.com/doc/refman/8.0/en/data-types.html
        https://docs.pingcap.com/tidb/stable/data-type-overview
        https://www.postgresql.org/docs/current/datatype.html
    * [X] sink，source 原理, 了解 source 内部数据流和控制流
    * [X] Confluent Cloud Kafka, Public net, AWS 14:00
** Week2 [1/4]
    * [ ] fix type problem
    * [ ] setup debezium -> kafka -> pg/mysql pipeline
      * [ ] setup debezium
      * [ ] setup kafka
    * [X] Fix pipeline for sinking
        https://github.com/debezium/debezium-examples/tree/main/unwrap-smt
** Week3 [1/7]
    * [ ] source mutation
    * [ ] source mutation
    * [ ] offset 如何记录 enumerator, 上游split变化如何发现，executor如何动
    * [ ] pipeline 需要docker compose
      risingwave -> kafka -> postgresql pipeline
    * [X] integration test
    * [ ] set up corresponding pipeline for risingwave
    * [ ] jdbc可以直接改

*** Fix data mapping
    请问：
    1. 类型问题主要是目标类型的缺失和debezium跟jdbc类型的不一致，
    1. 我需要把mysql/pg/tidb没有的类型和debezium跟jdbc不一致的类型都放在一个issue里吗？
    2.


    3. 从哪里入手？
    4. mysql/pg/tidb没有的类型如何处理？
    5. debezium跟jdbc的不一致如何处理？


    related function：
    [[file:~/miscellaneous/risingwave/src/connector/src/sink/mod.rs::273][datum_to_json_object]],
    [[file:~/miscellaneous/risingwave/src/connector/src/sink/kafka.rs::441][fields_to_json]]

    我目前的思路是对
    请问怎么修改record_to_json比较好？

    enum把数据包起来
** Week 5
    1. [ ] kinesis



    1. 迁移完之后原来的shard还有数据吗
    2. 怎么判断正在split或者merge，以及这种时候的行为是什么
    3. push down是什么意思
    4. 建kinesis



*** Kinesis
    experiment:
    1. 4 shards, 200 records
    2. split,


    目前的问题：
    1. shard iterator is none unexpectedly, renew it，不能正确终止，没有数据的时候没有iterator，但是
       不好说
    2. resharding的问题在哪里
    3. diff的问题

** Week5 [0/0]
    1. upsert avro，有没有schema registry，没有的话用户给文件schema定义，有没有pk
2. 有的话，key不一定有东西，应该拒掉
3. 再检查schema能否对上

接mv，table

query on source，batch查寻

batch执行检查pk
    *
* Knowledge
** Data types
    #+begin_src rust
pub struct StreamChunk {
    // TODO: Optimize using bitmap
    ops: Vec<Op>,
    pub(super) data: DataChunk,
}

pub struct DataChunk {
    columns: Vec<Column>,
    vis2: Vis,
}

pub struct Column {
    array: ArrayRef,
}

pub type ArrayRef = Arc<ArrayImpl>;

/// for column `v1`, [`ArrayRef`] will contain: [1,1,1]
/// | v1 | v2 |
/// |----|----|
/// | 1 |  a |
/// | 1 |  b |
/// | 1 |  c |
    #+end_src

    ~Timestamp~ is from ~chrono::NaiveDateTime~. ~{{date, time:{secs,frac}}}~

    therefore the data in risingwave is stored in column form

    ~Value~: valid JSON value
** Sink
    Three types of sink: ~KAFKA_SINK~, ~BLACKHOLE_SINK~, remote sink: ~jdbc~, ~file~, ~iceberg~
    #+begin_src rust
let sink_type = properties
    .get(CONNECTOR_TYPE_KEY)
    .ok_or_else(|| SinkError::Config(anyhow!("missing config: {}", CONNECTOR_TYPE_KEY)))?;
match sink_type.to_lowercase().as_str() {
    KAFKA_SINK => Ok(SinkConfig::Kafka(Box::new(KafkaConfig::from_hashmap(
        properties,
    )?))),
    BLACKHOLE_SINK => Ok(SinkConfig::BlackHole),
    _ => Ok(SinkConfig::Remote(RemoteConfig::from_hashmap(properties)?)),
}
    #+end_src
** Source
*** Summary
    1. When a source is defined, meta service will register its schema and broadcast to compute
       nodes. Compute node extracts properties from the frontend and builds corresponding components
       and stores them as ~SourceDesc~ in ~source_manager~ identified by table_id. Note that at this
       stage, the source instance is only built but not running.
    2. No ~SourceExecutor~ will be built until a subsequent materialized view is created.
       ~SourceExecutor~ fetches specific source instance from ~source_manager~ identified by table_id and
       holds a copy of it, and initializes the corresponding state store at this stage.
    3. When receiving a barrier, ~SourceExecutor~ will check whether it contains an ~SourceChangeSplit~
       mutation. If the partition assignment in the ~SourceChangeSplit~ mutation is different from the
       current situation, the SourceExecutor needs to rebuild the ConnectorSource and other
       underlying services based on the information in the mutation, then starts reading from the
       new split and offset.
    4. Whenever receiving a barrier, the state handler always takes a snapshot of the
       ConnectorSource then labels the snapshot with an epoch number. When an error occurs,
       SourceExecutor takes a specific state and applies it.


    1. Sources are controlled by ~SourceManager~.
    2. When you ~register_source~, you only create corresponding ~ConnectorSourceWorkerHandle~,  which
       contains split info, thread handler and meta info
    3. ~SourceExecutor~ is created when ~build_actors~, which is called while creating materialized view.
    4. ~SourceExecutor~'s ~stream~ data contains ~source_chunk_reader~ and ~barrier_stream~. Barrier for
       sync and ~source_chunk_reader~ is the actual data stream.
    5. ~source_chunk_reader~ comes from ~SourceDesc.ConnectorSource~'s ~stream_reader~, and
       ConnectorSource unites all connectors via SourceReader trait. Also, a parser is held here,
       which parses raw data to stream chunks according to column description. A ConnectorSource can
       handle multiple splits by spawning a new thread for each split. If the source is assigned no
       split, it will start a dummy reader whose next method never returns as a placeholder.
    6. ~SplitReader~ reads the actual data via ~into_stream~
    7. ~ConnectorSourceWorker~ have ~SplitEnumeratorImpl~. Enumerator periodically requests upstream to
       discover changes in splits, and in most cases the number of splits only increases. Managed by ~SourceManager~.


    1. source mutation
    2. source change
    3. mutation
    4. offset 如何记录 enumerator, 上游split变化如何发现，executor如何动
    5. pipeline 需要docker compose
    6. Integration test
    7. jdbc可以直接改


    Problems
    1. what does ~dispatcher~ do
    2. difference between meta's ~stream manager~  and stream's ~stream_manager~
       1. meta's stream manager is the client, stream's stream manager is the server
    3. How do we use ConnectorSourceWorker
    4. what is table fragment


    StreamingClusterInfo
    1. 节点分配均衡，数据怎么存的：数据在创建mv的时候用statetable存
       分配均衡靠一个scheduler，依靠fragment图，这个图来自前端，靠
    2. 恢复：
       * two kind of failure:
         1. fail in enumerator: 达到次数重启
         2. fail in stream
    3. assignment
    4. add mutation: create streaming job



    1. executor recovery，
    2.
*** Overview of overview
**** System start
    1. risingwave start
    2. compute node [[file:~/miscellaneous/risingwave/src/cmd_all/src/bin/risingwave.rs::56]["start"]]
    3. compute node begin to [[file:~/miscellaneous/risingwave/src/compute/src/lib.rs::199][serve]]
    4. [[file:~/miscellaneous/risingwave/src/compute/src/server.rs::81][compute_node_serve]]
       1. add
          [[file:~/miscellaneous/risingwave/src/compute/src/server.rs::370][StreamServiceServer]]
       2. rpc server start

    1. [[file:~/miscellaneous/risingwave/src/meta/src/rpc/server.rs::322][start_service_as_election_leader]]
    2. Source Manager [[file:~/miscellaneous/risingwave/src/meta/src/rpc/server.rs::421][starts]]

**** Create source
    1. [[file:~/miscellaneous/risingwave/src/meta/src/rpc/ddl_controller.rs::125][run_command]]
       1. [[file:~/miscellaneous/risingwave/src/meta/src/rpc/ddl_controller.rs::191][create_source]]
          1. [[file:~/miscellaneous/risingwave/src/meta/src/rpc/ddl_controller.rs::191][start_create_source_procesdure]]
          2. [[file:~/miscellaneous/risingwave/src/meta/src/rpc/ddl_controller.rs::196][register_source]]
             1. [[file:~/miscellaneous/risingwave/src/meta/src/stream/source_manager.rs::618][SourceManager::create_source_worker]]
                ConnectorSourceWorker begins to work in meta
          3. [[file:~/miscellaneous/risingwave/src/meta/src/rpc/ddl_controller.rs::204][finish_create_source_procesdure]]

**** Create materialized view
    1. front  end use
       [[file:~/miscellaneous/risingwave/src/frontend/src/handler/create_mv.rs::142][handle_create_mv]] -> which send ~CreateMaterializedViewRequest~  rpc, generate graph info
    2. [[file:~/miscellaneous/risingwave/src/meta/src/rpc/service/ddl_service.rs::254][DdlServiceImpl::create_materialized_view]], request has the graph info
    3. [[file:~/miscellaneous/risingwave/src/meta/src/rpc/ddl_controller.rs::138][run_command]], ~MaterializedView~ is one of ~StreamingJob~
    4. [[file:~/miscellaneous/risingwave/src/meta/src/rpc/ddl_controller.rs::246][DdlController::create_streaming_job]]
       1. [[file:~/miscellaneous/risingwave/src/meta/src/rpc/ddl_controller.rs::330][prepare_stream_job]]: make stream fragment graph
       2. [[file:~/miscellaneous/risingwave/src/meta/src/rpc/ddl_controller.rs::353][build_stream_job]]
          1. Resolve the upstream fragments, extend the fragment graph to a complete graph that
             contains all information needed for building the actor graph.
          2. Build the actor graph.
          3. Build the table fragments structure that will be persisted in the stream manager, and
             the context that contains all information needed for building the actors on the compute
             nodes.
          4. Mark creating tables, including internal tables and the table of the stream job.
       3. [[file:~/miscellaneous/risingwave/src/meta/src/stream/stream_manager.rs::207][GlobalStreamManager::create_streaming_job]]
          1. Broadcast the actor info based on the scheduling result in the context, build the
             hanging channels in upstream worker nodes.
          2. (optional) Get the split information of the ~StreamSource~ via source manager and patch
             actors.
          3. Notify related worker nodes to update and build the actors.
             [[file:~/miscellaneous/risingwave/src/meta/src/stream/stream_manager.rs::387][create_streaming_job_impl]]
             1. [[file:~/miscellaneous/risingwave/src/meta/src/stream/stream_manager.rs::320][build_actors]]
                1. Actors on each stream node will need to know where their upstream lies.
                   ~actor_info~ includes such information. It contains: actors in the current
                   create-streaming-job request; all upstream actors.
                2. We send RPC request in two stages:

                   The first stage does 2 things: broadcast actor
                   info, and send local actor ids to different WorkerNodes. Such that each
                   WorkerNode knows the overall actor allocation, but not actually builds it. We
                   initialize all channels in this stage.
                   [[file:~/miscellaneous/risingwave/src/meta/src/stream/stream_manager.rs::361][update_actors]]: register the actor info(e.g. type)

                   In the second stage, each [`WorkerNode`] builds local actors and connect them
                   with channels. Done by [[file:~/miscellaneous/risingwave/src/meta/src/stream/stream_manager.rs::377][build_actors]]
             2. [[file:~/miscellaneous/risingwave/src/meta/src/barrier/schedule.rs::229][BarrierScheduler::run_command]]
          4. Store related meta data.
    5. [[file:~/miscellaneous/risingwave/src/meta/src/manager/catalog/mod.rs::489][CatalogManager::create_view]]

       Now stream service in compute node receives the rpc ~BuildActorsRequest~,
       1. [[file:~/miscellaneous/risingwave/src/compute/src/rpc/service/stream_service.rs::62][StreamServiceImpl::build_actors]]
       2. [[file:~/miscellaneous/risingwave/src/stream/src/task/stream_manager.rs::600][LocalStreamManagerCore::build_actors]]
          1. [[file:~/miscellaneous/risingwave/src/stream/src/task/stream_manager.rs::470][create_nodes_inner]]
**** Split Change
    Summary:
    1. When we create a source, we will run the source enumerator in the background. It will fetch
       the split info at regular intervals.
    2. SourceManager will diff each source at regular intervals.
    3. If Split changes, wrap it into a command
    4. Global Barrier Scheduler push command into a queue
    5. Global Barrier Manager service runs in meta, and take scheduled barriers and send them. In
       this case, it will need to ~handle_new_barrier~ and send it via rpc.
    6. Each compute node has its stream service, which handles this rpc.It will send the barrier to
       target actor in info, actors are checked by [[file:~/miscellaneous/risingwave/src/meta/src/barrier/mod.rs::281][can_actor_send_or_collect]]
    7. Now our actor, a.k.a. ~SourceExecutor~ here, receives the barrier from stream, will do things
       according to the mutation info in barrier, in this case, ~apply_split_change~. It will build a
       new source reader based on ~SplitImpl~


    1. [[file:~/miscellaneous/risingwave/src/meta/src/stream/source_manager.rs::757][SourceManager::run]]
    2. [[file:~/miscellaneous/risingwave/src/meta/src/stream/source_manager.rs::742][SourceManager::tick]]
    3. [[file:~/miscellaneous/risingwave/src/meta/src/stream/source_manager.rs::243][SourceManagerCore::diff]]
    4. [[file:~/miscellaneous/risingwave/src/meta/src/barrier/schedule.rs::229][BarrierScheduler::run_command(SourceSplitAssignment)]]
       1. [[file:~/miscellaneous/risingwave/src/meta/src/barrier/schedule.rs::96][BarrierScheduler::push]]: now the command is sent by rpc, we need to wait now

          Global SourceManager creates a barrier and sends it to BarrierManager, which is achieved
          by a queue.
    5. [[file:~/miscellaneous/risingwave/src/meta/src/rpc/server.rs::322][start_service_as_election_leader]]
    6. [[file:~/miscellaneous/risingwave/src/meta/src/rpc/server.rs::569][GlobalBarrierManager::start]]
    7. [[file:~/miscellaneous/risingwave/src/meta/src/barrier/mod.rs::528][GlobalBarrierManager::run]]
       1. [[file:~/miscellaneous/risingwave/src/meta/src/barrier/mod.rs::1019][GlobalBarrierManager::handle_local_notification]]: handles ~SystemParamsChange~.
       2. [[file:~/miscellaneous/risingwave/src/meta/src/barrier/mod.rs::789][GlobalBarrierManager::handle_local_complete]]: Changes the state to ~Complete~, and try to
          commit all epoch that state is ~Complete~ in order. If commit is err, all nodes will be handled.
       3. [[file:~/miscellaneous/risingwave/src/meta/src/barrier/mod.rs::616][GlobalBarrierManager::handle_new_barrier]]: Handle the new barrier from the scheduled queue
          and inject it.
          1. [[file:~/miscellaneous/risingwave/src/meta/src/barrier/mod.rs::657][Notifier::notify_to_send]]: tell ~SourceManager~ we are handling the barrier? TODO
          2. [[file:~/miscellaneous/risingwave/src/meta/src/barrier/mod.rs::665][inject_barrier]]
             1. [[file:~/miscellaneous/risingwave/src/meta/src/barrier/mod.rs::692][inject_barrier_inner]]
                1. [[file:~/miscellaneous/risingwave/src/meta/src/barrier/command.rs::253][CommandContext::to_mutation]]
                2. send ~InjectBarrierRequest~ by ~StreamClient~
             2. [[file:~/miscellaneous/risingwave/src/meta/src/barrier/mod.rs::748][collect_barrier]]: Send barrier-complete-rpc and wait for responses from all compute
                nodes

        LocalStreamManager get the Barrier and broadcast it to all actors
    1. [[file:~/miscellaneous/risingwave/src/compute/src/rpc/service/stream_service.rs::133][StreamServiceImpl::inject_barrier]]
       1. [[file:~/miscellaneous/risingwave/src/stream/src/task/stream_manager.rs::221][LocalStreamManager::send_barrier]]


    1. [[file:~/miscellaneous/risingwave/src/stream/src/executor/actor.rs::162][Actor::run_consumer]]
       1. [[file:~/miscellaneous/risingwave/src/stream/src/executor/actor.rs::176][Box::pin(Box::new(self.consumer).execute())]]


    1. [[file:~/miscellaneous/risingwave/src/stream/src/executor/source/source_executor.rs::223][SourceExecutor::execute_with_stream_source]]
       1. [[file:~/miscellaneous/risingwave/src/stream/src/executor/source/source_executor.rs::311][while
          let Some(msg) = stream.next().await]]
       2.

**** Upstream changes

*** Source Manager
    [[file:~/miscellaneous/risingwave/src/meta/src/stream/source_manager.rs::48][Source]]
    #+begin_src rust
pub struct SourceManager<S: MetaStore> {
    pub(crate) paused: Mutex<()>,
    barrier_scheduler: BarrierScheduler<S>,
    core: Mutex<SourceManagerCore<S>>,
    connector_rpc_endpoint: Option<String>,
    metrics: Arc<MetaMetrics>,
}

pub struct SourceManagerCore<S: MetaStore> {
    fragment_manager: FragmentManagerRef<S>,

    /// Managed source loops
    managed_sources: HashMap<SourceId, ConnectorSourceWorkerHandle>,
    /// Fragments associated with each source
    source_fragments: HashMap<SourceId, BTreeSet<FragmentId>>,
    /// Revert index for source_fragments
    fragment_sources: HashMap<FragmentId, SourceId>,

    /// Splits assigned per actor
    actor_splits: HashMap<ActorId, Vec<SplitImpl>>,
}

impl<S> SourceManagerCore<S>
where
    S: MetaStore,
{
    async fn diff(&self) -> MetaResult<SplitAssignment> {}

}
    #+end_src
*** Source Executor
    ~stream_manager~
    [[file:~/miscellaneous/risingwave/src/stream/src/from_proto/mod.rs::120][create_executor]]
    How do we build source executor? Implement ~ExcutorBuilder~ trait for ~SourceExecutorBuilder~
    1. Four [[file:~/miscellaneous/risingwave/src/stream/src/from_proto/source.rs::35][ingredients]]:
       1. ~ExecutorParams~
          #+begin_src rust
pub struct ExecutorParams {
    pub env: StreamEnvironment,
    /// Indices of primary keys
    pub pk_indices: PkIndices,
    /// Executor id, unique across all actors.
    pub executor_id: u64,
    /// Operator id, unique for each operator in fragment.
    pub operator_id: u64,
    /// Information of the operator from plan node.
    pub op_info: String,
    /// The output schema of the executor.
    pub schema: Schema,
    /// The input executor.
    pub input: Vec<BoxedExecutor>,
    /// FragmentId of the actor
    pub fragment_id: FragmentId,
    /// Metrics
    pub executor_stats: Arc<StreamingMetrics>,
    /// Actor context
    pub actor_context: ActorContextRef,
    /// Vnodes owned by this executor. Represented in bitmap.
    pub vnode_bitmap: Option<Bitmap>,
}
          #+end_src
       2. ~SourceNode~
       3. ~impl StateStore~
       4. ~&mut LocalStreamManagerCore~
          #+begin_src rust
pub struct LocalStreamManagerCore {
    /// Runtime for the streaming actors.
    runtime: BackgroundShutdownRuntime,
    /// Each processor runs in a future. Upon receiving a `Terminate` message, they will exit.
    /// `handles` store join handles of these futures, and therefore we could wait their
    /// termination.
    handles: HashMap<ActorId, ActorHandle>,
    pub(crate) context: Arc<SharedContext>,
    /// Stores all actor information, taken after actor built.
    actors: HashMap<ActorId, stream_plan::StreamActor>,
    /// Stores all actor tokio runtime monitoring tasks.
    actor_monitor_tasks: HashMap<ActorId, ActorHandle>,
    /// The state store implement
    state_store: StateStoreImpl,
    /// Metrics of the stream manager
    pub(crate) streaming_metrics: Arc<StreamingMetrics>,
    /// Config of streaming engine
    pub(crate) config: StreamingConfig,
    /// Manages the await-trees of all actors.
    await_tree_reg: Option<await_tree::Registry<ActorId>>,
    /// Watermark epoch number.
    watermark_epoch: AtomicU64Ref,
    total_mem_val: Arc<TrAdder<i64>>,
}
          #+end_src
    2. Build ~StreamSourceCore~:
       1. ~source_id~
       2. ~source_name~
       3. ~column_ids~
       4. ~source_desc_builder~
       5. ~state_table_handler~
    3. Then we build executor based on whether we are sourcing from fs:
       1. ~FsSourceExecutor~
       2. ~SourceExecutor~
    Now we build ~SourceExecutor~ by ~SourceExecutor::new~
    1.




    [[file:~/miscellaneous/risingwave/src/stream/src/task/stream_manager.rs::379][LocalStreamManagerCore]]'s
    [[file:~/miscellaneous/risingwave/src/stream/src/task/stream_manager.rs::470][create_nodes_inner]] creates [[file:~/miscellaneous/risingwave/src/stream/src/executor/mod.rs::150][BoxedExecutor]].
    #+begin_src rust
/// `LocalStreamManager` manages all stream executors in this project.
pub struct LocalStreamManager {
    core: Mutex<LocalStreamManagerCore>,

    // Maintain a copy of the core to reduce async locks
    state_store: StateStoreImpl,
    context: Arc<SharedContext>,
    streaming_metrics: Arc<StreamingMetrics>,

    total_mem_val: Arc<TrAdder<i64>>,
}

impl LocalStreamManagerCore {
    async fn create_nodes_inner(
        &mut self,
        fragment_id: FragmentId,
        node: &stream_plan::StreamNode,
        input_pos: usize,
        env: StreamEnvironment,
        store: impl StateStore,
        actor_context: &ActorContextRef,
        vnode_bitmap: Option<Bitmap>,
        has_stateful: bool,
        subtasks: &mut Vec<SubtaskHandle>,
    ) -> StreamResult<BoxedExecutor> {
        let executor = create_executor(executor_params, self, node, store).await?;
    }
}
    #+end_src

    [[file:~/miscellaneous/risingwave/src/stream/src/from_proto/mod.rs::122][create_executor]] create executor based on type.

    ~SourceExecutorBuilder~ builds ~BoxedExecutor~ based on if it's s3.

    [[file:~/miscellaneous/risingwave/src/stream/src/executor/source/source_executor.rs::38][Source]]
    #+begin_src rust
pub struct SourceExecutor<S: StateStore> {
    ctx: ActorContextRef,

    identity: String,

    schema: Schema,

    pk_indices: PkIndices,

    /// Streaming source  for external
    stream_source_core: Option<StreamSourceCore<S>>,

    /// Metrics for monitor.
    metrics: Arc<StreamingMetrics>,

    /// Receiver of barrier channel.
    barrier_receiver: Option<UnboundedReceiver<Barrier>>,

    /// Expected barrier latency.
    expected_barrier_latency_ms: u64,
}

impl<S: StateStore> Executor for SourceExecutor<S> {
    fn execute(self: Box<Self>) -> BoxedMessageStream {
        if self.stream_source_core.is_some() {
            self.execute_with_stream_source().boxed()
        } else {
            self.execute_without_stream_source().boxed()
        }
    }
}

impl<S: StateStore> SourceExecutor<S> {
    async fn build_stream_source_reader(
        &self,
        source_desc: &SourceDesc,
        state: ConnectorState,
    ) -> StreamExecutorResult<BoxSourceWithStateStream> {
        source_desc.source.steam_reader -> SplitReaderImpl::create()
    }

    /// A source executor with a stream source receives:
    /// 1. Barrier messages
    /// 2. Data from external source
    /// and acts accordingly.
    #[try_stream(ok = Message, error = StreamExecutorError)]
    async fn execute_with_stream_source(mut self) {
        //...
        let source_chunk_reader = self
            .build_stream_source_reader(&source_desc, recover_state)
            .instrument_await("source_build_reader")
            .await?;
        let barrier_stream = barrier_to_message_stream(barrier_receiver).boxed();
        let mut stream = StreamReaderWithPause::<true>::new(barrier_stream, source_chunk_reader);
        //...
        while let Some(msg) = stream.next().await {
            match msg? {
                // This branch will be preferred.
                Either::Left(msg) => match &msg {
                    Message::Barrier(barrier) => {
                    }
                    _ => {
                        // For the source executor, the message we receive from this arm should
                        // always be barrier message.
                        unreachable!();
                    }
                },
                Either::Right(StreamChunkWithState {
                    chunk,
                    split_offset_mapping,
                }) => {
                }
            }
        }
    }
}
    #+end_src
*** ConnectorSource

    [[file:~/miscellaneous/risingwave/src/source/src/connector_source.rs::32][Source]]
    #+begin_src rust
pub struct SourceDesc {
    pub source: ConnectorSource,
    pub format: SourceFormat,
    pub columns: Vec<SourceColumnDesc>,
    pub metrics: Arc<SourceMetrics>,
}

impl ConnectorSource {
    // generate the stream based on the desc
    pub async fn stream_reader(
        &self,
        splits: ConnectorState,
        column_ids: Vec<ColumnId>,
        source_ctx: Arc<SourceContext>,
    ) -> Result<BoxSourceWithStateStream> {
        // params..
        let readers = try_join_all(to_reader_splits.into_iter().map(|state| {
            // params..
            async move {
            // params..
                SplitReaderImpl::create(props, state, parser_config, source_ctx, data_gen_columns)
                    .await
            }
        }
        Ok(select_all(readers.into_iter().map(|r| r.into_stream())).boxed())
    }
}
    #+end_src
    #+begin_quote
    ConnectorSource unites all connectors via SourceReader trait. Also, a parser is held here, which
    parses raw data to stream chunks according to column description. A ConnectorSource can handle
    multiple splits by spawning a new thread for each split. If the source is assigned no split, it
    will start a dummy reader whose next method never returns as a placeholder.
    #+end_quote

    ~stream_reader~ builds split readers based on ~ConnectorState~.
    #+begin_src rust
/// [`ConnectorState`] maintains the consuming splits' info. In specific split readers,
/// `ConnectorState` cannot be [`None`] and contains one(for mq split readers) or many(for fs
/// split readers) [`SplitImpl`]. If no split is assigned to source executor, `ConnectorState` is
/// [`None`] and [`DummySplitReader`] is up instead of other split readers.
pub type ConnectorState = Option<Vec<SplitImpl>>;
    #+end_src
    ~SplitImpl~ contains the info for specific split.

    raw_data -> [parser  -> stream chunks ->
*** Connectors
    #+begin_quote
    Connector serves as an interface to upstream data pipeline, including the message queue and file
    system. In the current design, it can only have a limited concurrency. One connector instance
    only reads from one split from the upstream. For example, if upstream is a Kafka and it has
    three partitions so, in RisingWave, there should be three connectors.
    #+end_quote

    All connectors need to implement the following trait and it exposes two methods to the upper
    layer.
    [[file:~/miscellaneous/risingwave/src/connector/src/source/base.rs::191][Source]]
    #+begin_src rust
pub trait SplitReader: Sized {
    type Properties;

    async fn new(
        properties: Self::Properties,
        state: Vec<SplitImpl>,
        parser_config: ParserConfig,
        source_ctx: SourceContextRef,
        columns: Option<Vec<Column>>,
    ) -> Result<Self>;

    fn into_stream(self) -> BoxSourceWithStateStream;
}
    #+end_src
    ~into_stream -> into_chunk_stream -> into_data_stream~

    ~into_chunk_stream~ is implemented by [[file:~/miscellaneous/risingwave/src/connector/src/macros.rs::257][macro]].

    ~BoxSourceWithStateStream~ is a wrapper of ~StreamChunk~ and split info.

    #+begin_quote
    Enumerator periodically requests upstream to discover changes in splits, and in most cases the
    number of splits only increases. The enumerator is a separate task that runs on the meta. If the
    upstream split changes, the enumerator notifies the connector by means of config change to
    change the subscription relationship.
    #+end_quote
    [[file:~/miscellaneous/risingwave/src/connector/src/source/base.rs::75][Source]]
    #+begin_src rust
/// [`SplitEnumerator`] fetches the split metadata from the external source service.
/// NOTE: It runs in the meta server, so probably it should be moved to the `meta` crate.
pub trait SplitEnumerator: Sized {
    type Split: SplitMetaData + Send + Sync;
    type Properties;

    async fn new(properties: Self::Properties) -> Result<Self>;
    async fn list_splits(&mut self) -> Result<Vec<Self::Split>>;
}
    #+end_src

*** Difference from the documentation
    1. ~SplitReader~ doesn't use ~next~ now but use ~into_stream~.
    2. ~assign_split~ is now ~Mutation::SourceChangeSpit~
    3. ~SourceManager~ no longer manages ~SourceDesc~, which is created when executing ~SourceExecutor~
*** Kafka example
    ~rdkafka~ -> message -> ~kafkaSplitReader~
    #+begin_src rust
pub struct KafkaSplitReader {
    consumer: StreamConsumer<PrivateLinkConsumerContext>,
    start_offset: Option<i64>,
    stop_offset: Option<i64>,
    bytes_per_second: usize,
    max_num_messages: usize,
    enable_upsert: bool,

    split_id: SplitId,
    parser_config: ParserConfig,
    source_ctx: SourceContextRef,
}
    #+end_src
** Datatype mapping


    #+begin_src rust
pub struct Field {
    pub data_type: DataType,
    pub name: String,
    /// For STRUCT type.
    pub sub_fields: Vec<Field>,
    /// The user-defined type's name, when the type is created from a protobuf schema file,
    /// this field will store the message name.
    pub type_name: String,
}
    #+end_src
    1. ~jdbc~ is part of remote sink, it only supports ~Int16, Int32, Int64, Float32, Float64,
       Boolean, Decimal, Timestamp and Varchar~.
       1. for ~Json~, each row is converted to a map ~Field name -> Json of value~, and then the map is
          serialized and pushed to ~row_ops~, therefore there is no type info?
       2. for ~streamchunk~, the message is simply serialized and encoded, there is no type cast
    2. for ~KAFKA_SINK~, we can set it to have type ~debezium~, which will output change data capture
       (CDC) log in Debezium format.
       1. schema is converted by ~schema_to_json~
       2. record is converted by ~record_to_json~
       3. the question is: how is schema and record aligned


    summary:
    1. risingwave有三种sink，kafka,blackhole和remote，debezium属于kafka sink，jdbc属于remote sink。
    2. 对于jdbc，它支持有限的risingwave类型，调用record_to_json将转换成 Field string到值的映射，然后序列化到json
    3. 对于debezium，它不但用record_to_json转换值，同时使用
       schema_to_json转换schema


    Current goal:
    1. format of debezium
    2. who uses the sinked data

    我有三个问题：
    1. 对于debezium，因为我们是将数据转换成debezium的格式输出到kafka，因此数据格式应该跟debezium文档
       一致；对于jdbc，我们需要将数据转换成mysql/pg/tidb支持的格式，因此需要跟它们各自的文档一致。请
       问我的理解正确吗？
    2. 对于debezium，在fields_to_json里，Timestamptz的目标类型是string，但是在datum_to_json_object中，Timestamptz保
       持了int64的值，这符合预期吗？
    3. 目前sink到jdbc是不是不支持date,time，是不是以后会支持？



    1. 目前在datum_to_json_object中，
       #+begin_src rust
(DataType::Time, ScalarRefImpl::Time(v)) => {
    // todo: just ignore the nanos part to avoid leap second complex
    json!(v.0.num_seconds_from_midnight() as i64 * 1000)
}
(DataType::Date, ScalarRefImpl::Date(v)) => {
    json!(v.0.num_days_from_ce())
}
(DataType::Timestamp, ScalarRefImpl::Timestamp(v)) => {
    json!(v.0.timestamp_millis())
}
(DataType::Bytea, ScalarRefImpl::Bytea(v)) => {
    json!(hex::encode(v))
}
// P<years>Y<months>M<days>DT<hours>H<minutes>M<seconds>S
(DataType::Interval, ScalarRefImpl::Interval(v)) => {
    json!(v.as_iso_8601())
       #+end_src
       参考的是debezium，并不适合jdbc?
       1. date, 根据文档应该是从unix epoch开始算，目前是从ce开始算


    create different object
    #+begin_src rust
let list_value = datum_to_json_object(
    &Field {
        data_type: DataType::List { datatype: Box::new(DataType::Int32) },
        ..mock_field.clone()
    },
    Some(
        ScalarImpl::List(ListValue::new(vec![
            Some(4i32.to_scalar_value()),
            Some(5i32.to_scalar_value()),
        ])).as_scalar_ref_impl()
    )).unwrap();
println!("List: {}", list_value);

let decimal_value = datum_to_json_object(
    &Field {
        data_type: DataType::Decimal,
        ..mock_field.clone()
    },
    Some(
        ScalarImpl::Decimal(Decimal::Normalized("123.4".parse().unwrap()))
            .as_scalar_ref_impl(),
    ),
).unwrap();
println!("Decimal: {}", decimal_value);

    #+end_src

** Storage
*** Readings
    * [[https://www.notion.so/risingwave-labs/A-Summary-on-Compaction-Strategy-3316b8d507204be48fe5b41868cd0e8f?pvs=4][Summary on Compaction Strategy]]
    *



    RFCs:
    * [[https://www.notion.so/risingwave-labs/RFC-Serverless-Compaction-9684770e2a6948fe86ad51453bec06c2?pvs=4][Serverless Compaction]]
* Miscellaneous
** Confluent
    #+begin_src json
# Required connection configs for Kafka producer, consumer, and admin
bootstrap.servers=pkc-ymrq7.us-east-2.aws.confluent.cloud:9092
security.protocol=SASL_SSL
sasl.mechanisms=PLAIN
sasl.username=7O7BEZWKLJZMBEXX
sasl.password=/4zj0y5zKmEqbapzC5YaXu5aSBvtLbfrV+wETS4Vk7pCuniE7xzKjqHnBQrTuzST

# Best practice for higher availability in librdkafka clients prior to 1.7
session.timeout.ms=45000
    #+end_src

    #+begin_src sql
CREATE TABLE s (
    ordertime timestamp,
    orderid int,
    itemid varchar,
    orderunits double,
    address STRUCT < city varchar,
    state varchar,
    zipcode int >
) WITH (
    connector = 'kafka',
    topic = 'topic1',
    properties.bootstrap.server = 'pkc-ymrq7.us-east-2.aws.confluent.cloud:9092',
    scan.startup.mode = 'earliest',
    properties.sasl.mechanism = 'PLAIN',
    properties.security.protocol = 'SASL_SSL',
    properties.sasl.username = '7O7BEZWKLJZMBEXX',
    properties.sasl.password = '/4zj0y5zKmEqbapzC5YaXu5aSBvtLbfrV+wETS4Vk7pCuniE7xzKjqHnBQrTuzST'
) ROW FORMAT JSON;


create source s with (
  connector = 'kafka',
    topic = 'topic1',
    properties.bootstrap.server = 'pkc-ymrq7.us-east-2.aws.confluent.cloud:9092',
    scan.startup.mode = 'earliest',
    properties.sasl.mechanism = 'PLAIN',
    properties.security.protocol = 'SASL_SSL',
    properties.sasl.username = '7O7BEZWKLJZMBEXX',
    properties.sasl.password = '/4zj0y5zKmEqbapzC5YaXu5aSBvtLbfrV+wETS4Vk7pCuniE7xzKjqHnBQrTuzST'
) row format json;

SELECT * FROM s
WHERE _rw_kafka_timestamp > now() - interval '10 s';
    #+end_src
