#+TITLE: Cppcon
#+EXPORT_FILE_NAME: ../latex/cppcon/cppcon.tex
#+LATEX_HEADER: \input{/Users/wu/notes/preamble.tex}
#+LATEX_HEADER: \graphicspath{{../../books/}}
#+LATEX_HEADER: \makeindex

* 2024
** C++ Relocation - How to Achieve Blazing Fast Save and Restore and More!
        https://www.youtube.com/watch?v=LnGrrfBMotA
        #+begin_src c++
// Row-wise representation
struct Item {
  int id;
  double price;
  Specials *specials;
  Category *category;
  bool is_homemade; // performance-hostile, memory layout inefficient
};

// Column-wise representation
template <std::size_t N>
struct State {
  std::array<int, N> ids;
  std::array<double, N> prices;
  EfficientMap<std::size_t, Specials> specials;
  std::array<Category_handle, N> categories;
  EfficientSet<std::size_t> homemade;
};
        #+end_src

        Handles
        #+begin_src c++
auto state = State<N>;

class ItemHandle {
  std::size index;
  public:
  int id() { return state.ids[index]; }
  double price() { return state.prices[index]; }
  Specials *specials() { return state.specials[id()]; }
  Category_handle category() { return state.categories[id()]; }
  bool &is_homemade() { return state.homemade.contains(id()); }
};
        #+end_src

        But libraries, user code will be in the "normal" way.
        * Manually
        * automating: Code generators, preprocessing

        Why?
        * Computing with homogeneous data can extract full performance from the execution resources
        * No allocations
        * Dis-incentives to communicate via state changes



* 2023
** A Long Journey of Changing std::sort Implementation at Scale
        Gcc now use ~introsort~ for ~sort~.
        * It is almost like a quick sort.
        * If we suspect a lot of recursion calls, use heap sort
        * GCC ~libstdc++~ uses \(2\log_2 n\) depth limit
        * Only 0.01% of all calls got into the heap sort fallback in production

        What is a good sort?
        * \(O(n\log n)\) comparisons.
        * Recognizes almost sorted patterns
        * fast for mordern hardware
        * fewer comparisons for heavy comparison sorting

        How ~libcxx~ has achieved that? Insertion sort on every step up to 8 insertions
        #+begin_src cpp
const unsigned __limit = 8;
unsigned __count       = 0;
for (_RandomAccessIterator __i = __j + difference_type(1); __i != __last; ++__i) {
    if (__comp(*__i, *__j)) {
        value_type __t(_Ops::__iter_move(__i));
        _RandomAccessIterator __k = __j;
        __j                       = __i;
        do {
            ,*__j = _Ops::__iter_move(__k);
            __j  = __k;
        } while (__j != __first && __comp(__t, *--__k));
        ,*__j = std::move(__t);
        if (++__count == __limit)
            return ++__i == __last;
    }
    __j = __i;
}
        #+end_src

        Problem: Ties (when we sort by first dimension, the order of second dimension is undefined, therefore
        we can't use the elements in second dimension in tests)

        Randomization also for ~std:nth_element~, ~std::partial_sort~

        #+begin_src c++
int64_t median(const std::vector<int64_t>& v) {
    int64_t med = v.size() / 2;
    std::nth_element(v.begin(), v.begin() + med, v.end());
    int64_t result = v[med];
    if (v.size() % 2 == 0) {
        std::nth_element(v.begin(), v.begin() + med - 1, v.end());
        result = (v[med] + v[med - 1]) / 2;
    }
}
        #+end_src

        This code has a bug since ~std::nth_element~ doesn't keep the original order of the data, so in the
        second range, ~v[med]~ is not the median we want.

        Use ~_LIBCPP_DEBUG_RANDOMIZE_UNSPECIFIED_STABILITY~ with libcxx
* 2016
** Chandler Carruth â€œHigh Performance Code 201: Hybrid Data Structures"
        #+begin_src c++
template <typename T, int N>
class SmallVector {
    T *Begin, *End;
    size_t Capacity;
    char Buffer[sizeof(T) * N];

    public:
        SmallVector() : Begin((T *)Buffer), End((T *)Buffer), Capacity(N) {}
        //
}
        #+end_src

        Why not ~std::vector~? Iterator invalidation.

        #+begin_src c++
template<typename T>
class SmallVectorImpl {
    T *Begin, *End;
    size_t Capacity;

    protected:
        SmallVectorImpl(T *Begin, T *End, size_t Capacity);

    public:
        iterator begin() { return Begin; }
        iterator end() { return End; }

        void push_back(const T &Element);
        void pop_back();
}


template <typename T, int N>
class SmallVector : public SmallVectorImpl<T> {

    char Buffer[sizeof(T) * N];

    public:
        SmallVector() : SmallVectorImpl((T *)Buffer, (T *)Buffer, N) {}
        //
}

        #+end_src

        Why not just use a allocator?
        1. hard to maintain?

        #+begin_src c++
template<class A, int N>
using SmallVector =
    std::vector<T, short_alloc<T, N>;

void f() {
    SmallVector<int, 4>::allocator_type::arena_type a;
    SmallVector<int> v{a};
}
        #+end_src

        What if?
        #+begin_src c++
template<class A, int N>
using SmallVector =
    std::vector<T, short_alloc<T, N>;

void g(SmallVector<int, 4> &v);

void f() {
    SmallVector<int, 8>::allocator_type::arena_type a;
    SmallVector<int> v{a};

    g(v); // Booom
}
        #+end_src

        #+begin_src c++
template<class A, int N>
using SmallVector =
    std::vector<T, short_alloc<T, N>;

SmallVector<T, 4> f() {
    SmallVector<int, 4>::allocator_type::arena_type a;
    SmallVector<int> v{a}
    // 
    return v; // Hard problem
}
        #+end_src

        How can we make the values small?
        1. Give large objects *address identity*: typically for large objects, there is some unique identity of
           that object, e.g. id. Therefore we could use a stable address with identity as offset
           #+begin_src c++
SmallVector<std::unique_ptr<BigObject>, 4> Objects;
           #+end_src
           But this has no locality.
           #+begin_src c++
class BumpPtrAllocator {
    constexpr int SlabSize = 4096;
    SmallVector<void *, 4> Slabs;
    void *CurPtr, *End;

    public:
        void *Allocate(int Size) {
            if (Size >= (End - CurPtr)) {
                CurPtr = malloc(SlabSize);
                End = CurPtr + SlabSize;
                Slabs.push_back(CurPtr);
            }

            void *Ptr = CurPtr;
            CurPtr += Size;
            return Ptr;
        }
}
           #+end_src
           Bad memory usage but has better locality.

        2. If pointers are too large, use an index

        3. aggressively pack the bit



        #+begin_src c++
template<typename T>
class TinyPtrVector {
    enum State {Inline, Vector};
    typedef SmallVector<T, 4> VecT;
    typedef PointerSumType<State,
        PointerSumTypeMember<Inline, T>,
        PointerSumTypeMember<Vector, std::unique_ptr<VecT>>
        SumT;
    SumT Value;

    public:
    T &Operator[](int i) const {
        if (Value.template is<Inline>()) {
        assert(i == 0);
        return Value.template get<Inline>();
    }
        return (*Value.template get<Vector>())[i];        
    }
}
        #+end_src

        Use bitfields everywhere.

* 2014
** Herb Sutter "Lock-Free Programming (or, Juggling Razor Blades)
        *Key tool: ordered atomic variable*.
        * ~C++11~ ~atomic<T>~ and ~C11 atomic_ *~

        Semantics and operations:
        * Each individual read and write is atomic, no locking required
        * Reads/writes are guarenteed not to be reordered
        * Compare-and-swap
        * ~compare_exchange_weak~ for use in loops (is allowed to fail spuriously)
        * ~exchange~ for when a "blind write" that returns the old value is sufficient
*** Basic Example: Double-Checked Locking
        The Double-Checked Locking (DCL) pattern
        #+begin_src c++
atomic<Widget *> Widget::pInstance{ nullptr };
Widget* Widget::Instance() {
    if (pInstance == nullptr) {         // 1: first check
        lock_guard<mutex> lock{ mutW }; // 2. acquire lock                                         
        if (pInstance == nullptr) {     // 3. second check
            pInsance = new Widget();    // 4. create and assign
        }
    }
    return pInstance;
}
        #+end_src

        Slight opmitization: (1 vs. 2 atomic loads in the main case, the ~return~)
        #+begin_src c++
atomic<Widget *> Widget::pInstance{ nullptr };
Widget* Widget::Instance() {
    Widget *p = pInstance;
    if (p == nullptr) {                  // 1: first check
        lock_guard<mutex> lock{ mutW };  // 2. acquire lock
        if ((p = pInstance) == nullptr) {// 3. second check
            pInsance = p = new Widget(); // 4. create and assign
        }
    }
    return p;
}
        #+end_src

        The general-purpose way to spell lazy initialization in ~C++11~ is:
        #+begin_src c++
static unique_ptr<widget> widget::instance;
static std::once_flag widget::create;

widget& widget::get_instance() {
    std::call_once(create, [=]{ instance = make_unique<widget>();});
    return instance;
}
        #+end_src

        *Best of all*: The special-purpose way that you should use when you can (aks Meyers Singleton):
        #+begin_src c++
widget& widget::get_instance() {
    static widget instance;
    return instance;
}
        #+end_src
*** Producer-Consumer Variations
        Create and Publish Queue Items: 1 producer, many consumers, using locks
        #+begin_src c++
// Thread 1 (producer)
while (ThereAreMoreTasks()) {
    task = AllocateAndBuildNewTask();
    {
        lock_guard<mutex> lock{mut};
        queue.push(task);
    }
    cv.notify();
}

{
    lock_guard<mutex> lock{mut};
    queue.push(done);
}
cv.notify();

// Thread 2..N (consumers)
myTask = null;
while (myTask != done) {
    {
        lock_guard<mutex> lock{mut};
        while (queue.empty())
            cv.wait(mut);
        myTask = queue.first();
        if (myTask != done)
            queue.pop();
    }

    if (myTask != don)
        DoWork(myTask);
}
        #+end_src
* tee
