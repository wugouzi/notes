% Created 2024-07-01 Mon 15:18
% Intended LaTeX compiler: xelatex
\documentclass[11pt]{article}
\usepackage{hyperref}
\input{/Users/wu/notes/preamble.tex}
\graphicspath{{../../../paper/consensus/}}

%% ox-latex features:
%   !announce-start, !guess-pollyglossia, !guess-babel, !guess-inputenc, caption,
%   underline, image, !announce-end.

\usepackage{capt-of}

\usepackage[normalem]{ulem}

\usepackage{graphicx}

%% end ox-latex features


\author{wu}
\date{\today}
\title{Consensus Bridging Theory And Practice}
\hypersetup{
 pdfauthor={wu},
 pdftitle={Consensus Bridging Theory And Practice},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 31.0.50 (Org mode 9.8-pre)}, 
 pdflang={English}}
\begin{document}

\maketitle
\section{Motivation}
\label{sec:orgc81f286}

\subsection{Achieving fault tolerance with replicated state machines}
\label{sec:org0bc4d9b}
Keeping the replicated log consistent is the job of the consensus algorithm. The consensus module on a
server receives commands from clients and adds them to its log. It communicates with the consensus
modules on other servers to ensure that every log eventually contains the same requests in the same
order, even if some servers fail. Once commands are properly replicated, they are said to be
\textbf{committed}. Each server’s state machine processes committed commands in log order, and the outputs are
returned to clients. As a result, the servers appear to form a single, highly reliable state machine.
\section{Basic Raft algorithm}
\label{sec:org982b7f7}
\subsection{Raft overview}
\label{sec:org6eee8cb}
\begin{center}
\includegraphics[width=.99\textwidth]{../../images/papers/17.png}
\label{}
\end{center}
Given the leader approach, Raft decomposes the consensus problem into three relatively independent
subproblems:
\begin{itemize}
\item Leader election: a new leader must be chosen when starting the cluster and when an existing leader fails
\item Log replication: the leader must accept log entries from clients and replicate them across the cluster, forcing the other logs to agree with its own
\item Safety: the key safety property for Raft is the State Machine Safety Property
\end{itemize}


Raft \textbf{SAFETY}:
\begin{itemize}
\item \textbf{Election Safty}: At most one leader can be elected in a given term.
\item \textbf{Leader Append-Only}: A leader never overwrites or deletes entries in its log; it only appends new entries.
\item \textbf{Log Matching}: If two logs contain an entry with the same index and term, then the logs are identical
in all entries up through the given index.
\item \textbf{Leader Completeness}: If a log entry is committed in a given term, then that entry will be present in
the logs of the leaders for all higher-numbered terms.
\item \textbf{State Machine Safety}: If a server has applied a log entry at a given index to its state machine, no
other server will ever apply a different log entry for the same index.
\end{itemize}
\subsection{Log replication}
\label{sec:org7a63981}
The leader decides when it is safe to apply a log entry to the state machines; such an entry is called
\textbf{committed}.
\begin{itemize}
\item Raft guarantees that committed entries are durable and will eventually be executed by all of the
available state machines.
\item A log entry is committed once the leader that created the entry has replicated it on a majority of
the servers. This also commits all preceding entries in the leader’s log, including entries created
by previous leaders.
\item The leader keeps track of the highest index it knows to be committed, and it includes that index in
future AppendEntries RPCs (including heartbeats) so that the other servers eventually find out.
\item Once a follower learns that a log entry is committed, it applies the entry to its local state
machine (in log order).
\end{itemize}

Raft maintains the following properties, which together constitute the \textbf{Log Matching Property}:
\begin{itemize}
\item If two entries in different logs have the same index and term, then they store the same command.
\item If two entries in different logs have the same index and term, then the logs are identical in all
preceding entries.

\item The first property follows from the fact that a leader creates at most one entry with a given log
index in a given term, and log entries never change their position in the log.

If two entries have the same term, then they come from the same leader. If a log is replicated into
a specific entry, then the index of that log is the same as the leader. Therefore the two entries
have the same command as they come from the same entry from the same leader in the same term.
\item The second property is guaranteed by a consistency check performed by AppendEntries. When sending an
AppendEntries RPC, the leader includes the \uline{index and term of the entry} in its log that immediately
\uline{precedes} the new entries\wu{(prev log)}. If the follower does not find an entry in its log with the same index and
term, then it refuses the new entries.

The consistency check acts as an induction step: the initial empty state of the logs satisfies the
Log Matching Property, and the consistency check preserves the Log Matching Property whenever logs
are extended. As a result, whenever AppendEntries returns successfully, the leader knows that the
follower’s log is identical to its own log up through the new entries.
\end{itemize}


A follower may be missing entries that are present on the leader, it may have extra entries that are
not present on the leader, or both. Missing and extraneous entries in a log may span multiple terms.
\begin{center}
\includegraphics[width=.9\textwidth]{../../images/papers/13.png}
\label{3.6}
\end{center}

The leader handles inconsistencies by forcing the followers’ logs to duplicate its own. This means
that conflicting entries in follower logs will be overwritten with entries from the leader's log.

To bring a follower’s log into consistency with its own, the leader must find the latest log entry
where the two logs agree, delete any entries in the follower’s log after that point, and send the
follower all of the leader’s entries after that point. All of these actions happen in response to the
consistency check performed by AppendEntries RPCs:
\begin{itemize}
\item The leader maintains a \textbf{nextIndex} for each follower, which is the index of the next log entry the leader will send to that follower.
\item When a leader first comes to power, it initializes all nextIndex values to the index just after the
last one in its log.
\item If a follower's log is inconsistent with the leader's, the AppendEntries consistency check will fail
in the next AppendEntries RPC. After a rejection, the leader decrements the follower's nextIndex and
retries the AppendEntries RPC. Eventually the nextIndex will reach a point where the leader and
follower logs match.
\end{itemize}

Until the leader has discovered where it and the follower’s logs match, the leader can send
AppendEntries with no entries (like heartbeats) to save bandwidth. Then, once the matchIndex
immediately precedes the nextIndex, the leader should begin to send the actual entries.

If desired, the protocol can be optimized to reduce the number of rejected AppendEntries RPCs:
\begin{itemize}
\item when rejecting an AppendEntries request, the follower can include the term of the conflicting entry
and the first index it stores for that term. With this information, the leader can decrement
nextIndex to bypass all of the conflicting entries in that term;
\item the leader can use a binary search approach to find the first entry where the follower’s log differs
from its own; this has better worst-case behavior.
\end{itemize}
\subsection{Safty}
\label{sec:org69a2b51}
This section completes the Raft algorithm by adding a restriction on which servers may be elected
leader. The restriction ensures that the leader for any given term contains all of the entries committed in previous terms.
\subsubsection{Election restriction}
\label{sec:org8c0a827}
In any leader-based consensus algorithm, the leader \textbf{must} eventually store all of the committed log
entries.

Raft uses the voting process to \uline{prevent a candidate from winning an election unless its log contains}
\uline{all committed entries}:
\begin{itemize}
\item A candidate must contact a majority of the cluster in order to be elected, which means that every
committed entry must be present in at least one of those servers.
\item If the candidate's log is at least as \textbf{up-to-date} as any other log in that majority, then it will
hold all the committed entries.
\end{itemize}

Raft determines which of two logs is more \textbf{up-to-date} by comparing the index and term of the last
entries in the logs.
\begin{itemize}
\item If the logs have last entries with different terms, then the log with the later term is more up-to-date.
\item If the logs end with the same term, then whichever log is longer is more up-to-date.
\end{itemize}

The correctness of this notion of up-to-date comes from Log Matching Property.
\subsubsection{Committing entries from previous terms}
\label{sec:org29e9f89}
A leader cannot immediately conclude that an entry from a previous term is committed once it is stored
on a majority of servers:
\begin{center}
\includegraphics[width=.9\textwidth]{../../images/papers/12.png}
\label{3.7}
\end{center}

To eliminate problems like the one in Figure \ref{3.7}, Raft never commits log entries from previous
terms by counting replicas; once an entry from the current term has been committed in this way, then all prior
entries are committed indirectly because of the Log Matching Property.
\subsubsection{Safety argument}
\label{sec:org2c57b12}
\label{3.3}
Assume Leader Completeness Property does not hold. Suppose the leader for term \(T\) \(leader_T\)
commits a log entry from its term, but that log entry is not stored by the leader of some future term.
Consider the smallest term \(U>T\) whose leader \(leader_U\) does not store the entry.
\begin{enumerate}
\item The committed entry must have been absent from \(leader_U\)'s log at the time of its election.
\item \(leader_T\) replicated the entry on a majority of the cluster, and \(leader_U\) received votes
from a majority of the cluster. Thus at least one server both accepted the entry from \(leader_T\)
and voted for \(leader_U\).
\item The voter must have accepted the committed entry from \(leader_T\) \textbf{before} voting for \(leader_U\);
otherwise it would have rejected the AppendEntries request from \(leader_T\)
\item The voter still stored the entry when it voted for \(leader_U\), since every intervening leader
contained the entry, leaders never remove entries, and followers only remove entries if they
conflict with the leader.
\item The voter granted its vote to \(leader_U\), so \(leader_U\)'s log must have been as up-to-date as
the voter's. This leaders to one of two contradictions.
\item First, if the voter and \(leader_U\) shared the same last log term, then \(leader_U\)'s log must
have been at least as long as the voter's, so its log contained every entry in the voter's log.
\item Otherwise, \(leader_U\)'s last log term must have been larger than the voter's. Moreover, it was
larger than \(T\), since the voter's last log term was at least \(T\). The earlier leader that
created \(leader_U\)'s last log entry must have contained the committed entry in its log. Then by
the Log Matching Property, \(leader_U\)'s log must also contain the committed entry, which is a contradiction.
\item Thus, the leaders of all terms greater than \(T\) must contain all entries from term \(T\) that are
committed in term \(T\).
\item The Log Matching Property guarantees that future leaders will also contain entries that are
committed indirectly.
\end{enumerate}



Given the Leader Completeness Property, we can prove the State Machine Safety Property from, which
states that if a server has applied a log entry at a given index to its state machine, no other server
will ever apply a different log entry for the same index:
\begin{itemize}
\item At the time a server applies a log entry to its state machine, its log must be identical to the
leader’s log up through that entry, and the entry must be committed. Now consider the lowest term in
which any server applies a given log index; the Leader Completeness Property guarantees that the
leaders for all higher terms will store that same log entry, so servers that apply the index in
later terms will apply the same value. Thus, the State Machine Safety Property holds.
\end{itemize}


Finally, Raft requires servers to apply entries in log index order. Combined with the State Machine
Safety Property, this means that all servers will apply exactly the same set of log entries to their
state machines, in the same order.
\subsubsection{Followe rand candidate crashes}
\label{sec:org06db8ff}
\subsubsection{Persisted state and server restarts}
\label{sec:orge29e943}
\begin{itemize}
\item current term and vote: prevent the server from voting twice \wu{means vote for different candidates}
in the same term or replacing log entries from a newer leader with those from a deposed
leader\wu{term}.
\item new log entries before they are committed: prevents committed entries from being lost or
``uncommitted'' when servers restart.
\item \emph{last applied} index
\end{itemize}
\subsubsection{Timing and availability}
\label{sec:org766f3c4}
\subsubsection{Leadership transfer extention}
\label{sec:org265b7e4}
To transfer leadership in Raft, the prior leader sends its log entries to the target server, then the
target server runs an election without waiting for the election timeout to elaspe.
\begin{enumerate}
\item The prior leader stops accepting new client requests.
\item The prior leader fully updates the target server's log to match its own, using the normal log
replication mechanism
\item The prior leader sends a \emph{TimeoutNow} request to the target server.
\end{enumerate}

Once the target server receives the TimeoutNow request, it is highly likely to start an election
before any other server and become leader in the next term. Its next message to the prior leader will
include its new term number, causing the prior leader to step down. At this point, leadership transfer
is complete.

It is also possible for the target server to fail; in this case, the cluster must resume client
operations. If leadership transfer does not complete after about an election timeout, the prior leader
aborts the transfer and resumes accepting client requests. If the prior leader was mistaken and the
target server is actually operational, then at worst this mistake will result in an extra election,
after which client operations will be restored.
\section{Cluster membership changes}
\label{sec:orgb7e7bd4}
\begin{center}
\includegraphics[width=.99\textwidth]{../../images/papers/14.png}
\label{}
\end{center}
\subsection{Safety}
\label{sec:org1032c5d}
\begin{itemize}
\item Goal: no point during the transition where it is possible for two leaders to be elected for the same term.
\item Difficulty: it isn’t possible to atomically switch all of the servers at once, so the cluster can
potentially split into two independent majorities during the transition
\begin{center}
\includegraphics[width=.99\textwidth]{../../images/papers/15.png}
\label{4.2}
\end{center}
\end{itemize}

Raft restricts the types of changes that are allowed: only one server can be added or removed from the
cluster at a time. More complex changes in membership are implemented as a series of single-server
changes.

When adding a single server to a cluster or removing a single server from a cluster, any majority of
the old cluster overlaps with any majority of the new cluster. This overlap prevents the cluster from
splitting into two independent majorities; in terms of the safety argument of Section \ref{3.3}, it
guarantees the existence of ``the voter''. Thus, when adding or removing just a single server, it is
safe to switch directly to the new configuration.

\begin{center}
\includegraphics[width=.99\textwidth]{../../images/papers/16.png}
\label{}
\end{center}

When the leader receives a request to add or remove a server from its current configuration
\(C_{old}\), it appends the new configuration \(C_{new}\)  as an entry in its log and replicates that
entry using the normal Raft mechanism. The new configuration takes effect on each server as soon as it
is added to that server’s log: the \(C_{new}\) entry is replicated to the \(C_{new}\) servers, and a majority of the new
configuration is used to determine the \(C_{new}\) entry’s commitment.

The configuration change is complete once the \(C_{new}\) entry is committed. At this point, the leader knows
that a majority of the \(C_{new}\) servers have adopted \(C_{new}\) . It also knows that any servers that have not
moved to \(C_{new}\) can no longer form a majority of the cluster, and servers without \(C_{new}\) cannot be elected
leader. Commitment of \(C_{new}\) allows three things to continue:
\begin{enumerate}
\item The leader can acknowledge the successful completion of the configuration change.
\item If the configuration change removed a server, that server can be shut down.
\item Further configuration changes can be started. Before this point, overlapped configuration changes
could degrade to unsafe situations as in Fig \ref{4.2}
\end{enumerate}



\begin{itemize}
\item \uline{Servers always use the latest configuration in their logs, regardless of whether that configuration}
\uline{entry has been committed.} This allows leaders to easily avoid overlapping configuration changes (the
third item above), by not beginning a new change until the previous change’s entry has committed.
\item It is only safe to start another membership change once a majority of the old cluster has moved to
operating under the rules of \(C_{new}\). If servers adopted \(C_{new}\) only when they learned that \(C_{new}\)
was committed, Raft leaders would have a difficult time knowing when a majority of the old cluster
had adopted it.
\end{itemize}


In Raft, it is the caller’s configuration that is used in reaching consensus, both for voting and for log replication:
\begin{itemize}
\item A server accepts \texttt{AppendEntries} requests from a leader that is not part of the server’s latest
configuration. Otherwise, a new server could never be added to the cluster (it would never accept any log entries preceding the configuration entry that adds the server).
\item \uline{A server also grants its vote to a candidate that is not part of the server’s latest configuration}
(if the candidate has a sufficiently up-to-date log and a current term). This vote may occasionally
be needed to keep the cluster available. For example, consider adding a fourth server to a
three-server cluster. If one server were to fail, the new server’s vote would be needed to form a
majority and elect a leader.
\end{itemize}

Thus, servers process incoming RPC requests without consulting their current configurations.
\subsection{Availability}
\label{sec:org3862a4a}
\subsubsection{Catching up new servers}
\label{sec:org4f2137e}
When a server is added to the cluster, it typically will not store any log entries. If it is added to
the cluster in this state, its log could take quite a while to catch up to the leader’s, and during
this time, the cluster is more vulnerable to unavailability:
\begin{itemize}
\item A three-server cluster can normally tolerate one failure with no loss in availability. However, if a
fourth server with an empty log is added to the same cluster and one of the original three servers
fails, the cluster will be temporarily unable to commit new entries (\ref{4.4} (a)).
\item Many new servers are added to a cluster in quick succession, where the new servers are needed to
form a majority of the cluster (\ref{4.4} (b)).
\end{itemize}

\begin{center}
\includegraphics[width=.99\textwidth]{../../images/papers/18.png}
\label{4.4}
\end{center}

In order to avoid availability gaps, Raft introduces an additional phase before the configuration
change, in which a new server joins the cluster as a \textbf{non-voting member}. The leader replicates log
entries to it, but it is not yet counted towards majorities for voting or commitment purposes. Once
the new server has caught up with the rest of the cluster, the reconfiguration can proceed as
described above. (The mechanism to support non-voting servers can also be useful in other contexts;
for example, it can be used to replicate the state to a large number of servers, which can serve
read-only requests with relaxed consistency.)

We suggest the following algorithm to determine when a new server is sufficiently caught up to add to
the cluster:
\begin{itemize}
\item The replication of entries to the new server is split into rounds, as shown in Figure \ref{4.5}.
\item Each round replicates all the log entries present in the leader’s log at the start of the round to
the new server’s log. While it is replicating entries for its current round, new entries may arrive
at the leader; it will replicate these during the next round. As progress is made, the round
durations shrink in time.
\item The algorithm waits a fixed number of rounds (such as 10). If the last round lasts less than an
election timeout, then the leader adds the new server to the cluster, under the assumption that
there are not enough unreplicated entries to create a significant availability gap.
\item Otherwise, the leader aborts the configuration change with an error. The caller may always try again
(it will be more likely to succeed the next time, since the new server’s log will already be
partially caught up).
\end{itemize}

\begin{center}
\includegraphics[width=.99\textwidth]{../../images/papers/19.png}
\label{4.5}
\end{center}
\subsubsection{Removing the current leader}
\label{sec:org800d84b}
\subsubsection{Disruptive servers}
\label{sec:org05a34cc}
Without additional mechanism, servers not in \(C_{new}\) can disrupt the cluster.
\begin{itemize}
\item Once the cluster leader has created the \(C_{new}\) entry, a server that is not in \(C_{new}\) will
no longer receive heartbeats, so it will time out and start new elections.
\item Furthermore, it will not receive the \(C_{new}\) entry or learn of that entry’s commitment, so it
will not know that it has been removed from the cluster. The server will send \texttt{RequestVote} RPCs with
new term numbers, and this will cause the current leader to revert to follower state.
\item A new leader from \(C_{new}\) will eventually be elected, but the disruptive server will time out
again and the process will repeat, resulting in poor availability. If multiple servers have been
removed from the cluster, the situation could degrade further.
\end{itemize}


First idea was that, if a server is going to start an election, it would first check that it wouldn't
be wasting everyone's time - that it had a chance to win the election. This introduced a new phase to
elections, called the \textbf{Pre-Vote phase}. A candidate would first ask other servers whether its log was
up-to-date enough to get their vote. Only if the candidate believed it could get votes from a majority
of the cluster would it increment its term and start a normal election.

Unfortunately, the Pre-Vote phase does not solve the problem of disruptive servers: there are
situations where the disruptive server’s log is sufficiently up-to-date, but starting an election
would still be disruptive. Perhaps surprisingly, these can happen even before the configuration change
completes.

\begin{center}
\includegraphics[width=.99\textwidth]{../../images/papers/20.png}
\label{}
\end{center}


Raft’s solution uses heartbeats to determine when a valid leader exists. We modify the \texttt{RequestVote} RPC
to achieve this: if a server receives a \texttt{RequestVote} request within the minimum election timeout of
hearing from a current leader, it does not update its term or grant its vote. It can either drop the
request, reply with a vote denial, or delay the request; the result is essentially the same. This does
not affect normal elections, where each server waits at least a minimum election timeout before
starting an election. However, it helps avoid disruptions from servers not in \(C_{new}\) : while a leader is
able to get heartbeats to its cluster, it will not be deposed by larger term numbers. \label{Problem1}
\subsubsection{Availability}
\label{sec:orgcd9ce06}
\label{Problem2}
We show that the algorithm will be able to maintain and replace leaders during membership changes and
that the leader(s) will both service client requests and complete the configuration changes. We
assume, among other things, that a majority of the old configuration is available (at least until \(C_{new}\)
is committed) and that a majority of the new configuration is available.

\begin{enumerate}
\item A leader can be elected at all steps of the configuration change:
\begin{itemize}
\item If the available server with the most up-to-date log in the new cluster has the \(C_{new}\)
entry, it can collect votes from a majority of \(C_{new}\) and become leader
\item Otherwise, the \(C_{new}\) entry must not yet be committed. The available server with the most
up-to-date log among both the old and new clusters can collect votes from a majority of
\(C_{old}\) and a majority of \(C_{new}\), so no matter which configuration it uses, it can
become leader.
\end{itemize}
\end{enumerate}
\subsection{Arbitrary configuration changes using joint consensus}
\label{sec:org8c295a0}
To ensure safety across arbitrary configuration changes, the cluster first switches to a transitional
configuration we call \textbf{joint consensus}; once the joint consensus has been committed, the system then
transitions to the new configuration. The joint consensus combines both the old and new
configurations:
\begin{itemize}
\item Log entries are replicated to all servers in both configurations
\item Any server from either configuration may serve as leader
\item Agreement (for elections and entry commitment) requires separate majorities from \textbf{both} the old and
new configurations.
\end{itemize}

The joint consensus allows individual servers to transition between configurations at different times
without compromising safety. Furthermore, joint consensus allows the cluster to continue servicing
client requests throughout the configuration change.

This approach extends the single-server membership change algorithm with an intermediate log entry for
the joint configuration; Figure 4.8 illustrates the process.
\begin{center}
\includegraphics[width=.99\textwidth]{../../images/papers/26.png}
\label{}
\end{center}
\begin{itemize}
\item When the leader receives a request to change the configuration from \(C_{old}\) to \(C_{new}\), it
stores the configuration for joint consensus (\(C_{old,new}\) in the figure) as a log entry and
replicates that entry using the normal Raft mechanism.
\item As with the single-server configuration change algorithm, each server starts using a new
configuration as soon as it stores the configuration in its log. This means that the leader will use
the rules of \(C_{old,new}\) to determine when the log entry for \(C_{old,new}\) is committed. If
the leader crashes, a new leader may be chosen under either \(C_{old}\) or \(C_{old,new}\),
depending on whether the winning candidate has received \(C_{old,new}\)
\item Once \(C_{old,new}\) has been committed, neither \(C_{old}\) and \(C_{new}\) can make decisions
without approval of the other, and the Leader Completeness Property ensures that only servers with
the \(C_{old,new}\) log entry can be elected as leader. It is now safe for the leader to create a
log entry describing \(C_{new}\) and replicate it to the cluster. Again this configuration will take
effect on each server as soon as it is seens. When the \(C_{new}\) log entry has been committed
under the rules of \(C_{new}\) , the old configuration is irrelevant and servers not in the new
configuration can be \uline{shut down}.
\end{itemize}
\section{Log compaction}
\label{sec:orgcb769c6}
\begin{center}
\includegraphics[width=.99\textwidth]{../../images/papers/23.png}
\label{}
\end{center}

The various approaches to compaction share several core concepts.
\begin{itemize}
\item Instead of centralizing compaction decisions on the leader, each server compacts the committed
prefix of its log independently.
\item The basic interaction between the state machine and Raft involves transferring responsibility for a
prefix of the log from Raft to the state machine.
\item Once Raft has discarded a prefix of the log, the state machine takes on two new responsibilities.
\begin{enumerate}
\item If the server restarts, the state machine will need to load the state corresponding to the
discarded log entries from disk before it can apply any entries from the Raft log.
\item the state machine may need to produce a consistent image of the state so that it can be sent to a
slow follower.
\end{enumerate}
It is not feasible to defer compaction until log entries have been ``fully replicated'' to every
member in the cluster, since a minority of slow followers must not keep the cluster from being
fully available, and new servers can be added to the cluster at any time.
\end{itemize}
\subsection{Snapshotting memory-based state machines}
\label{sec:org3bdeac1}
Each server takes snapshots independently, covering just the committed entries in its log. Most of the
work in snapshotting involves serializing the state machine’s current state, and this is specific to a
particular state machine implementation. 
\begin{center}
\includegraphics[width=.99\textwidth]{../../images/papers/24.png}
\label{}
\end{center}

Once the state machine completes writing a snapshot, the log can be truncated. Raft first stores the
state it needs for a restart:
\begin{itemize}
\item the index and term of the last entry included in the snapshot
\item the latest configuration as of that index.
\end{itemize}
Then it discards the prefix of its log up through that index. Any previous snapshots can also be
discarded, as they are no longer useful.


The leader may occasionally need to send its state to slow followers and to new servers that are
joining the cluster. In snapshotting, this state is just the latest snapshot, which the leader
transfers using a new RPC called \texttt{InstallSnapshot}:
\begin{center}
\includegraphics[width=.99\textwidth]{../../images/papers/25.png}
\label{}
\end{center}
When a follower receives a snapshot with this RPC, it must decide what to do with its existing log
entries.
\begin{itemize}
\item Usually the snapshot will contain new information not already in the follower’s log. In this case,
the follower discards its entire log; it is all superseded by the snapshot and may possibly have
uncommitted entries that conflict with the snapshot.
\item If the follower receives a snapshot that describes a prefix of its log (due to retransmission or by
mistake), then log entries covered by the snapshot are deleted but entries following the snapshot
are still valid and must be retained.
\end{itemize}
\subsubsection{Snapshotting concurrently}
\label{sec:orgbb45437}
Creating a snapshot can take a long time, both in serializing the state and in writing it to disk.
Thus, both serializing and writing snapshots must be concurrent with normal operations to avoid
availability gaps.

Fortunately, copy-on-write techniques allow new updates to be applied without impacting the snapshot
being written. There are two approaches to this:
\begin{itemize}
\item State machines can be built with immutable (functional) data structures to support this. Because
state machine commands would not modify the state in place, a snapshotting task could keep a
reference to some prior state and write it consistently into a snapshot.
\end{itemize}
• Alternatively, the operating system’s copy-on-write support can be used (where the programming
environment allows it). On Linux for example, in-memory state machines can use fork to make a copy of
the server’s entire address space. Then, the child process can write out the state machine’s state and
exit, all while the parent process continues servicing requests. The LogCabin implementation currently
uses this approach.
\subsubsection{When to snapshot}
\label{sec:org36a2554}
Fortunately, using the size of the \textbf{previous} snapshot rather than the size of the next one results in reasonable behavior.
\subsubsection{Implementation concerns}
\label{sec:org7846f9b}
\begin{itemize}
\item \textbf{Saving and loading snapshots}:
\end{itemize}
\subsection{Snapshotting disk-based state machines}
\label{sec:org7f16a3e}
Disk-based state machines must be able to provide a consistent snapshot of the disk for the purpose of
transmitting it to slow followers.
\section{Client Interaction}
\label{sec:orgf8c2dcc}
\begin{center}
\includegraphics[width=.99\textwidth]{../../images/papers/21.png}
\label{}
\end{center}
\subsection{Finding the cluster}
\label{sec:orgf3a73a9}
Two general approaches:
\subsection{Routing requests to the leader}
\label{sec:org7270ebc}
\subsection{Implementing linearizable semantics}
\label{sec:org206ec42}
Raft provides at-least-once semantics for clients; the replicated state machine may apply a command
multiple times.

In linearizability, each operation appears to execute instantaneously, exactly once, at some point
between its invocation and its response.

To achieve linearizability in Raft, servers must filter out duplicate requests. The basic idea is that
servers save the results of client operations and use them to skip executing the same request multiple
times. To implement this, each client is given a unique identifier, and clients assign unique serial
numbers to every command.

Given this filtering of duplicate requests, Raft provides linearizability. The Raft log provides a
serial order in which commands are applied on every server. Commands take effect instantaneously and
exactly once according to their first appearance in the Raft log, since any subsequent appearances are
filtered out by the state machines as described above.
\subsection{Processing read-only queries more efficiently}
\label{sec:orgb616181}
Fortunately, it is possible to bypass the Raft log for read-only queries and still preserve
linearizability. To do so, the leader takes the following steps:
\begin{enumerate}
\item If the leader has not yet marked an entry from its current term committed, it waits until it has
done so. The Leader Completeness Property guarantees that a leader has all committed entries, but
at the start of its term, it may not know which those are. To find out, it needs to commit an entry
from its term. Raft handles this by having each leader commit a blank \textbf{no-op} entry into the log at
the start of its term. As soon as this no-op entry is committed, the leader's commit index will be at least as large as any other servers' during its term.
\item The leader saves its current commit index in a local variable \texttt{readIndex}.
\item The leader needs to make sure it hasn’t been superseded by a newer leader of which it is unaware.
It issues a new round of heartbeats and waits for their acknowledgments from a majority of the
cluster. Once these acknowledgments are received, the leader knows that there could not have
existed a leader for a greater term at the moment it sent the heartbeats. Thus, the \texttt{readIndex} was,
at the time, the largest commit index ever seen by any server in the cluster \wu\{all later leaders
have the same log before \texttt{readIndex} thanks to Leader Completeness Property\}.
\item The leader waits for its state machine to advance at least as far as the \texttt{readIndex}; this is current enough to satisfy linearizability.
\item The leader issues the query against its state machine and replies to the client with the results
\end{enumerate}


To improve efficiency further, the leader can amortize the cost of confirming its leadership: it can
use a single round of heartbeats for any number of read-only queries that it has accumulated.

Followers could also help offload the processing of read-only queries. However, these reads would also
run the risk of returning stale data without additional precautions. To serve reads safely, the
follower could issue a request to the leader that just asked for a current \texttt{readIndex} (the leader would
execute steps 1–3 above); the follower could then execute steps 4 and 5 on its own state machine for
any number of accumulated read-only queries.
\subsubsection{Using clocks to reduce messaging for read-only queries}
\label{sec:org8aeafc9}
To use clocks instead of messages for read-only queries, the normal heartbeat mechanism would provide
a form of lease. Once the leader’s heartbeats were acknowledged by a majority of the cluster, the
leader would assume that no other server will become leader for about an election timeout, and it
could extend its lease accordingly. The leader would then reply to read-only queries during that
period without any additional communication.
(The leadership transfer mechanism presented in Chapter 3 allows the leader to be replaced early; a
leader would need to expire its lease before transferring leadership.)
\begin{center}
\includegraphics[width=.99\textwidth]{../../images/papers/22.png}
\label{}
\end{center}

The lease approach assumes a bound on clock drift across servers (over a given time period, no
server's clock increases more than this bound times any other). Discovering and maintaining this bound
might present operational challenges.

Fortunately, a simple extension can improve the guarantee provided to clients, so that even under
asynchronous assumptions (even if clocks were to misbehave), each client would see the replicated
state machine progress monotonically (sequential consistency). For example, a client would not
see the state as of log index \(n\), then change to a different server and see only the state as of log
index \(n-1\).
\begin{itemize}
\item To implement this guarantee, servers would include the index corresponding to the state machine
state with each reply to clients. Clients would track the latest index corresponding to results they
had seen, and they would provide this information to servers on each request. If a server received a
request for a client that had seen an index greater than the server’s last applied log index, it
would not service the request (yet).
\end{itemize}
\section{Correctness}
\label{sec:org9d58e0b}
\subsection{Formal specification and proof for basic Raft algorithm}
\label{sec:org4e12c2e}
The specification models an asynchronous system (it has no notion of time) with the following assumptions:
\begin{itemize}
\item Messages may take an arbitrary number of steps (transitions) to arrive at a server. Sending a
message enables a transition to occur (the receipt of the message) but with no particular timeliness.
\item Servers fail by stopping and may later restart from stable storage on disk.
\item The network may reorder, drop, and duplicate messages.
\end{itemize}
\section{Leader election evaluation}
\label{sec:orgdb9fa7e}
\subsection{Preventing disruptions when a server rejoins the cluster}
\label{sec:orged9cfdc}
One downside of Raft’s leader election algorithm is that a server that has been partitioned from the
cluster is likely to cause a disruption when it regains connectivity.

In the Pre-Vote algorithm, a candidate only increments its term if it first learns from a majority of
the cluster that they would be willing to grant the candidate their votes:
\begin{enumerate}
\item if the candidate's log is sufficiently up-to-date
\item voters have not received heartbeats from a valid leader for at least a baseline election timeout
\end{enumerate}
\section{Problem}
\label{sec:org00ca297}
\begin{enumerate}
\item \ref{Problem1}: Not quite understand
\item \ref{Problem2}: same roblem
\end{enumerate}
\end{document}
