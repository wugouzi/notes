#+title: Designing Data-Intensive Applications

#+AUTHOR: Martin Kleppmann
#+EXPORT_FILE_NAME: ../latex/DesigningDataIntensiveApplications/DesigningDataIntensiveApplications.tex
#+LATEX_HEADER: \graphicspath{{../../books/}}
#+LATEX_HEADER: \input{../preamble.tex}
#+LATEX_HEADER: \makeindex

* Storage and Retrieval

** Data Structures That Power Your Database

*** Hash Indexes

*** SSTables and LSM-Trees
* Replication
** Leaders and Followers
*** Setting Up New Followers
    1. Take a consistent snapshot of the leader's database at some point in time
    2. Copy the snapshot to the new follower node
    3. The follower connects to the leader and requests all the data changes that have happened
       since the snapshot was taken. This requires that the snapshot is associated with an exact
       position in the leader's replication log
    4. When the follower has processed the backlog of data changes since the snapshot, we say it has
       *caught up*
*** Handling Node Outages
**** Follower failure: Catch-up recovery
**** Leader failure: Failover
    *Failover*: one of the followers needs to be promoted to be the new leader, clients need to be
    reconfigured to send their writes to the new leader, and the other followers need to start
    consuming data changes from the new leader.
    1. /Determining that the leader has failed/.
    2. /Choosing a new leader/.
    3. /Reconfiguring the system to use the new leader/.

    Failover is fraught with things that can go wrong:
    1. If asynchronous replication is used, the new leader may not have received all the writes from
       the old leader before it failed. If the former leader rejoins the cluster after a new leader
       has been chosen, what should happen to those writes? The new leader may have received
       conflicting writes in the meantime. The most common solution is for the old leader’s
       unreplicated writes to simply be *discarded*, which may violate clients' durability
       expectations.
    2. Discarding writes is especially dangerous if other storage systems outside of the database
       need to be coordinated with the database contents. For example, in one incident at GitHub, an
       out-of-date MySQL follower was promoted to leader. The database used an
       autoincrementing counter to assign primary keys to new rows, but because the new leader’s
       counter lagged behind the old leader’s, it reused some primary keys that were previously
       assigned by the old leader. These primary keys were also used in a Redis store, so the reuse
       of primary keys resul‐ ted in inconsistency between MySQL and Redis, which caused some
       private data to be disclosed to the wrong users.
    3. In certain fault scenarios, it could happen that two nodes both believe that they are the
       leader. This situation is called *split brain*.
    4. What is the right timeout before the leader is declared dead? A longer timeout means a longer
       time to recovery in the case where the leader fails. However, if the timeout is too short,
       there could be unnecessary failovers.
*** Implementation of Replication Logs
**** Statement-based replication
    In the simplest case, the leader logs every write request (/statement/) that it executes and sends
    that statement log to its followers.

    Problems:
    1. Any statement that calls a nondeterministic function, such as ~NOW()~ to get the current date
       and time or ~RAND()~ to get a random number, is likely to generate a different value on each
       replica.
    2. If statements use an autoincrementing column, or if they depend on the existing data in the
       database (e.g., ~UPDATE ... WHERE <some condition>~), they must be executed in exactly the same
       order on each replica, or else they may have a differ‐ ent effect. This can be limiting when
       there are multiple concurrently executing transactions.
    3. Statements that have side effects (e.g., triggers, stored procedures, user-defined functions)
       may result in different side effects occurring on each replica, unless the side effects are
       absolutely deterministic.


    MySQL now switches to row- based replication (discussed shortly) if there is any nondeterminism
    in a statement.
**** Write-ahead log (WAL) shipping
    Usually every write is appended to a log:
    * For log-structured storage engine, the log is the main place for storage
    * For B-tree, which overwrites individual disk blocks, every modification is first written to a
      write-ahead log so that the index can be restored to a consistent state after a crash

    We can use the exact same log to build a replica on another node: besides writing the log to
    disk, the leader also sends it across the network to its followers.

    Main con: a WAL contains details of which bytes were changed in which disk blocks, which
    makes replication closely coupled to the storage engine. If the database changes its storage
    format from one version to another, it is typically not possible to run different versions of
    the database on the leader an the followers.

    That may seem like a minor implementation detail, but it can have a big operational
    impact. If the replication protocol allows the follower to use a newer software version
    than the leader, you can perform a zero-downtime upgrade of the database software
    by first upgrading the followers and then performing a failover to make one of the
    upgraded nodes the new leader. If the replication protocol does not allow this version
    mismatch, as is often the case with WAL shipping, such upgrades require downtime.
**** Logical (row-based) log replication
    A logical log for a relational database is usually a sequence of records describing
    writes to database tables at the granularity of a row.
**** Trigger-based replication
** Problems with Prelication Lag
    Leader-based replication requires all writes to go through a single node, but read-only queries
    can go to any replica. For workloads that consist of mostly reads and only a small percentage of
    writes, this is attractive: create many followers, and distribute the read requests across those
    followers.

    This /read-scaling/ architecture only realistically works with asynchronous replication, and
    follower may have out-dated data. This is /eventual consistency/.
*** Reading Your Own Writes
    In this situation, we need /read-after-write consistency/, also known as /read-your-writes
    consistency/.

    How can we implement read-after-write consistency in a system with leader-based
    replication? There are various possible techniques. To mention a few:
    * When reading something that the user may have modified, read it from the leader; otherwise,
      read it from a follower.
    * If most things in the application are potentially editable by the user, you could track the
      time of the last update and, for one minute after the last update, make all reads from the
      leader. You could also monitor the replication lag on followers and prevent queries on any
      follower that is more than one minute behind the leader.
    * The client can remember the timestamp of its most recent write—then the system can ensure
      that the replica serving any reads for that user reflects updates at least until that
      timestamp. The timestamp could be a *logical timestamp* or the actual system clock.

    Another complication arises when the same user is accessing your service from multiple
    devices,for example a desktop web browser and a mobile app. In this case you may want to provide
    cross-device read-after-write consistency: if the user enters some information on one device and
    then views it on another device, they should see the information they just entered. In this
    case:
    * Approaches that require remembering the timestamp of the user’s last update become more
      difficult. This metadata will need to be centralized.
    * If your replicas are distributed across different datacenters, there is no guarantee that
      connections from different devices will be routed to the same datacenter.
*** Monotonic Reads
    It's possible for a user to see things /moving backward in time/.

    This happens if a user makes several reads from different replicas.

    /Monotonic reads/ is a guarante that this kind of anomaly does not happen. It's weaker than strong
    consistency but stronger than eventual consistency.

    One way of achieving monotonic reads is to make sure that each user always makes their reads
    from the same replica.
*** Consistent Prefix Reads
    /Consistent prefix reads/ guarantees that if a sequence of writes happens in a certain order, then
    anyone reading those writes will see them appear in the same order.

    This is a particular problem in partitioned (sharded) databases.

    One solution is to make sure that any writes that are causally related to each other are written
    to the same partition—but in some applications that cannot be done efficiently.
*** Solutions for Replication Lag

** Multi-Leader Replication
    Leader-based replication has one major downside: there is only one leader, and all writes must
    go through it.

    A natural extension of the leader-based replication model is to allow more than one node to
    accept writes. Replication still happens in the same way: each node that processes a write must
    forward that data change to all the other nodes. We call this a multi-leader configuration (also
    known as master–master or active/active replication). In this setup, each leader simultaneously
    acts as a follower to the other leaders.

*** Use Cases for Multi-Leader Replication
**** Multi-datacenter operation
    In a multi-leader configuration, you can have a leader in /each/ datacenter.

    Downside: the same data may be concurrently modified in two different datacenters, and those
    write conflicts must be resolved.
**** Clients with offline operation
    Another situation in which multi-leader replication is appropriate is if you have an application
    that needs to continue to work while it is disconnected from the internet.
**** Collaborative editing
*** Handling Write Conflicts
    #+ATTR_LATEX: :width .8\textwidth :float nil
    #+NAME:
    #+CAPTION: A write conflict caused by two leaders concurrently updating the smae record
    [[../images/ddia/1.jpg]]
**** Synchronous versus  asynchronous conflict detection
    In a multi-leader setup, both writes are successful, and the conflict is only detected
    asynchronously at some later

    You could make the conflict detetion synchronous - i.e., wait for the write to be replicated to
    all replicas before telling the user that the write was successfull. However, by doing so, you
    would lose the main advantage of multi-leader replication: allowing each replica to accept
    writes independently.
**** Conflict avoidance
**** Converging toward a consistent state
**** Custom conflict resolution logic
*** Multi-Leader Replication Toplogies
    #+ATTR_LATEX: :width .9\textwidth :float nil
    #+NAME:
    #+CAPTION: Three example topologies in which multi-leader replication can be set up
    [[../images/ddia/2.jpg]]

    A problem with circular and star topologies is that if just one node fails, it can interrupt the
    flow of replication messages between other nodes, causing them to be unable to communicate until
    the node is fixed.

    All-to-all topoligies can have issues. In particular, some network links may be faster than
    others.
    #+ATTR_LATEX: :width .9\textwidth :float nil
    #+NAME:
    #+CAPTION: With multi-leader replication, writes may arrive in the wrong order at some replicas
    [[../images/ddia/3.jpg]]

    This is a problem of causality. To order these events correctly, a technique called *version vectors* can be used.
** Leaderless Replication
*** Writing to the Database When a Node is Down
**** Read repair and anti-entropy
    /Read repair/: When a client makes a read from several nodes in parallel, it can detect any stale
    responses.

    /Anti-entropy process/: In addition, some datastores have a background process that constantly
    looks for differences in the data between replicas and copies any missing data from one replica
    to another. Unlike the replication log in leader-based replication, this anti-entropy process
    does not copy writes in any particular order, and there may be a significant delay before data
    is copied.
**** Quorums for reading and writing
    If there are \(n\) replicas, every write mus be performed by \(w\) nodes to be considered
    successful, and we must query at least \(r\) nodes for each read. As long as \(w+r>n\), we
    expect to get an up-to-date value when reading. Reads and writes that obey these \(r\) and \(w\)
    values are called *quorum* reads and writes.

    The quorum condition, \(w+r>n\), allows the system to tolerate unavailable nodes as follows:
    * if \(w<n\), we can still process writes if a node is unavailable
    * if \(r<n\), we can still process reads if a node is unavailable
    * With \(n=3,w=2,r=2\), we can tolerate one unavailable node.
    * With \((n,w,r)=(5,3,2)\), we can tolerate two unavailable nodes.n
*** Limitations of Quorum Consistency
    If you have \(n\) replicas, and you choose \(w\) and \(r\) s.t. \(w+r>n\), you can generally
    expect every read to return the most recent value written for a key.

    Often, \(r\) and \(w\) are chosen to be a majority (more than \(n/2\)) of nodes, because that
    ensures \(w+r>n\) while still tolerating up to \(n/2\) node failures.

    Edge case:
    * If two writes occur concurrently, it is not clear which one happened first.
    * If a node carrying a new value fails, and its data is restored from a replica carrying an old
      value, the number of replicas storing the new value may fall below \(w\), breaking the quorum condition.
**** Monitoring staleness
*** Sloppy Quorums and Hinted Handoff
    A network interruption can easily cut off a client from a large number of database nodes.
    Although those nodes are alive, and other clients may be able to connect to them, to a client
    that is cut off from the database nodes, they might as well be dead. In this situation, it’s
    likely that fewer than \(w\) or \(r\) reachable nodes remain, so the client can no longer reach
    a quorum.

    In a large cluster (with significantly more than n nodes) it’s likely that the client can
    connect to some database nodes during the network interruption, just not to the nodes that it
    needs to assemble a quorum for a particular value. In that case, database designers face a
    trade-off:
    * Is it better to return errors to all requests for which we cannot reach a quorum of \(w\)
      or \(r\) nodes?
    * Or should we accept writes anyway, and write them to some nodes that are reachable but aren’t
      among the \(n\) nodes on which the value usually lives?


    The latter is known as a *sloppy quorum*: writes and reads still require \(w\) and \(r\)
    successful responses, but those may include nodes that are not among the designated \(n\) "home"
    nodes for a value.

    Once the network interruption is fixed, any writes that one node temporarily accepted on behalf
    of another node are sent to the appropriate “home” nodes. This is called *hinted handoff*.
*** Detecting Concurrent Writes
    #+ATTR_LATEX: :width .9\textwidth :float nil
    #+NAME:
    #+CAPTION: Concurrent writes in a Dynamo-style datastore: there is no well-defined ordering
    [[../images/ddia/4.jpg]]
**** Last writes wins (discarding concurrent writes)
    LWW achieves the goal of eventual convergence, but at the cost of durability: if there
    are several concurrent writes to the same key, even if they were all reported as successful to
    the client (because they were written to w replicas), only one of the writes will survive and
    the others will be silently discarded.
**** The "happens-before" relation and concurrency
* Partitioning
** Partitioning and Replication
** Partitioning of Key-Value Data
*** Partitioning by Key Range
*** Partitioning by Hash of Key
*** Skewed Workloads and Relieving Hot Spots
** Partitioning and Secondary Indexes
    The problem with secondary indexes is that they don’t map neatly to partitions.
*** Partitioning Secondary Indexes by Document
    each partition maintains its own secondary indexes, covering only the documents in that
    partition. It doesn’t care what data is stored in other partitions.
*** Partitioning Secondary Indexes by Term
    Rather than each partition having its own secondary index (a /local index/), we can construct a
    /global index/ that covers data in all partitions. A global index must also be partitioned, but it
    can be partitioned differently from the primary key index.
    #+ATTR_LATEX: :width .8\textwidth :float nil
    #+NAME:
    #+CAPTION: Partitioning secondary indexes by term
    [[../images/ddia/5.jpg]]
** Rebalancing Partitions
    The process of moving load from one node in the cluster to another is called *rebalancing*.

    Requirements:
    * after rebalancing, the load (data storage, read and write requests) should be shared fairly
      between the nodes in the cluster
    * while rebalancing is happening, the database should continue accepting reads and writes
    * no more data than necessary should be moved between nodes, to make rebalancing fast and to
      minimize hte network and disk I/O load
*** Strategies for rebalancing
**** How not to do it: hash mod N
    if \(N\) changes, we need to remap most of keys
**** Fixed number of partitions
    We can create many partitions than there are nodes, and assign several partitions to each node.
**** Dynamic partitioning
    When a partition grows to exceed a configured size (on HBase, the default is 10 GB), it is split
    into two partitions so that approximately half  of the data ends up on each side of the split
    [26]. Conversely, if lots of data is deleted and a partition shrinks below some threshold, it
    can be merged with an adjacent partition.
**** Partitioning proportionally to nodes
    With dynamic partitioning, the number of partitions is proportional to the size of the dataset,
    since the splitting and merging processes keep the size of each partition between some fixed
    minimum and maximum.
*** Operations: Automatic or Manual Rebalancing
** Request Routing
*** Parallel Query Execution
* Transactions
** The Slippery Concept of a Transaction
*** The Meaning of ACID
**** Atomicity
    Atomicity describes what happens if a client wants to make several writes, but a fault occurs
    after some of the writes have been processed.

    If the writes are grouped together into an atomic transaction, and the transaction cannot be
    completed (committed) due to a fault, then the transaction is aborted and the database must
    discard or undo any writes it has made so far in that transaction.
**** Consistency
    The idea of ACID consistency is that you have certain statements about your data
    (invariants) that must always be true. If a transaction starts with a database that is valid
    according to these invariants, and any writes during the transaction preserve the validity, then
    you can be sure that the invariants are always satisfied.

    Consistency is a property of the application. The application may rely on the database’s
    atomicity and isolation properties in order to achieve consistency, but it’s not up to the
    database alone.
**** Isolation
    Isolation in the sense of ACID means that concurrently executing transactions are
    isolated from each other: they cannot step on each other’s toes.
**** Durability
    Durability is the promise that once a transaction has committed successfully, any data it has
    written will not be forgotten, even if there is a hardware fault or the database crashes.
*** Single-Object and Multi-Object Operations
** Weak Isolation Levels
*** Read Committed
    It makes two guarantees:
    1. When reading from the database, you will only see data that has been committed (no *dirty
       reads*)
    2. When writing to the database, you will only overwrite data that has been committed (no *dirty
       writes*)

    Most commonly, database prevent dirty writes by using row-level locks: when a transaction wants
    to modify a particular object (row or document), it must first acquire a lock on that object.

    How do we prevent dirty reads? One option would be to use the same lock.
    Otherwise, for every object that is written, the database remembers both the old committed value
    and the new value set by the transaction currently holds the write lock. While the transaction
    is ongoing, any other transactions that read the object are given the old value. Only when the
    new value is committed do transactions switch over to reading the new value.
*** Snapshot Isolation and Repeatable Read
    #+ATTR_LATEX: :width .9\textwidth :float nil
    #+NAME:
    #+CAPTION: Read skew: Alice observes the database in an inconsistent state
    [[../images/ddia/6.jpg]]

    This anomaly is called a *nonrepeatable read* or *read skew*.

    *Snapshot isolation* is the most common solution to this problem. The idea is that each
    transaction reads from a /consistent snapshot/ of the database - that is, the transaction sees all
    the data that was committed in the database at the start of the transaction.
**** Implementing snapshot isolation
    Like read committed isolation, implementations of snapshot isolation typically use write locks
    to prevent dirty writes. However, reads do not require any locks.
    /From a performance point of view, a key principle of snapshot isolation is readers never block
    writers, and writers never block readers./

    To implement it, we use write locks to prevent dirty writes and use *Multiversion concurrency
    control* for read.

    #+ATTR_LATEX: :width .9\textwidth :float nil
    #+NAME:
    #+CAPTION: Implementing snapshot isolation using multi-version objects
    [[../images/ddia/7.jpg]]

    An update is internally translated into a delete and a create.
**** Visibility rules for observing a consistent snapshot
**** Indexes and snapshot isolation
    One option is to have the index simply point to all version of an object.

    Also can use *append-only b-tree*.
**** Repeatable read and naming confusion
    Oracle calls it *serializable* and PG/MySQL calls it *repeatable read*.
*** Preverting Lost Updates
    The read committed and snapshot isolation levels we’ve discussed so far have been primarily
    about the guarantees of what a read-only transaction can see in the presence of concurrent
    writes.

    There are several other interesting kinds of conflicts that can occur between concurrently
    writing transactions. The best known of these is the lost update problem:
    #+ATTR_LATEX: :width .9\textwidth :float nil
    #+NAME:
    #+CAPTION: A race condition between two clients concurrently incrementing a counter
    [[../images/ddia/8.jpg]]

    The lost update problem can occur if an application reads some value from the database, modifies
    it, and writes back the modified value (a /read-modify-write cycle/).
**** Atomic write operations
    Atomic operations are usually implemented by taking an exclusive lock on the objecgt when it is
    read so that no other transaction can read it until the update has been applied. This technique
    is called *cursor stability*.
**** Explicit locking
**** Automatically detecting lost updates
**** Compare-and-set
**** Conflict resolution and replication
*** Write Skew and Phantoms
    #+ATTR_LATEX: :width .9\textwidth :float nil
    #+NAME:
    #+CAPTION: Example of write skew causing an application bug
    [[../images/ddia/9.jpg]]

    Since the database is using snapshot isolation, both checks return 2.
**** Characterizing write skew
    This anomaly is called *write skew*.

    Write skew can occur if two transactions read the same objects, and then update some of those
    objects

    With write skew our options are more restricted.
    * Atomic single-object operations don't help, as multiple objects are involved.
    * If you can't use a serializable isolation level, the second-best option in this case is to
      explicitly lock the row.
**** Phantoms causing write skew
    1. A ~SELECT~ query checks whether some requirements is satisfied by searching for rows that match
       some search condition
    2. Depending on the result of the first query, the application code decides how to continue
    3. If the application decides to go ahead, it makes a write (~INSERT~, ~UPDATE~, or ~DELETE~) to the
       database and commits the transaction.

    This effect, where a write in one transaction changes the result of a search query in another
    transaction, is called a *phantom*
** Serializability
*** Actual Serial Execution
    * Every transaction must be small and fast, because it takes only one slow transaction to stall
      all transaction processing.
    * It is limited to use cases where the active dataset can fit in memory. Rarely accessed data
        could potentially be moved to disk, but if it needed to be accessed in a single-threaded
        transaction, the system would get very slow.
    * Write throughput must be low enough to be handled on a single CPU core, or else transactions
        need to be partitioned without requiring cross-partition coordination.
    * Cross-partition transactions are possible, but there is a hard limit to the extent to which
      they can be used.
*** Two-Phase Locking
    Two-phase locking makes the lock requirements much stronger:
    * If transaction A has read an object and transaction B wants to write to that object, B must
      wait until A commits or aborts before it can continue. (This ensures that B can’t change the
      object unexpectedly behind A’s back.)
    * If transaction A has written an object and transaction B wants to read that object, B must
      wait until A commits or aborts before it can continue. (Reading an old version of the object
      is not acceptable under 2PL.)

    A database with serializable isolation must prevent phantoms.

    In the meeting room booking example this means that if one transaction has searched for existing
    bookings for a room within a certain time window, another transaction is not allowed to concurrently
    insert or update another booking for the same room and time range. (It’s okay to concurrently
    insert  bookings for other rooms, or for the same room at a different time that doesn’t affect
    the proposed booking.)

    How do we implement this? Conceptually, we need a *predicate lock*. It works similarly to the
    shared/exclusive lock described earlier, but rather than belonging to a particular object (e.g.,
    one row in a table), it belongs to all objects that match some search condition,

    Unfortunately, predicate locks do not perform well: if there are many locks by active
    transactions, checking for matching locks becomes time-consuming. For that reason, most
    databases with 2PL actually implement *index-range locking*
*** Serializable Snapshot Isolation
    On the one hand, we have implementations of serializability that don’t perform well (two- phase
    locking) or don’t scale well (serial execution). On the other hand, we have weak isolation
    levels that have good performance, but are prone to various race conditions (lost updates, write
    skew, phantoms, etc.). Are serializable isolation and good performance fundamentally at odds
    with each other?
**** Pessimistic versus optimistic concurrency control
    Two-phase locking is a so-called *pessimistic* concurrency control mechanism. Serial execution is,
    in a sense, pessimistic to the extreme.

    By contrast, serializable snapshot isolation is an *optimistic* currency control technique.
    Optimistic in this context means that instead of blocking if something potentially dangerous
    happens, transactions continue anyway, in the hope that everything will turn out all right. When
    a transaction wants to commit, the database checks whether anything bad happened; if so, the
    transaction action is aborted and has to be retired.
**** Decisions based on an outdated premise
    How does the database know if a query result might have changed? There are two cases:
    * Detecting reads of a stale MVCC object version
    * Detecting writes that affect prior reads (the write occurs after the read)
**** Detecting stale MVCC reads
    The database needs to track when a transaction ignores another transaction's writes due to MVCC
    visibility rules. When the transaction wants to commit, the database checks whether any of the
    ignored writes have now been committed. If so, the transaction must be aborted.

    If the transaction is read-only, there is no risk of write skew.
**** Detecting writes that affect prior reads
    When a transaction writes to the database, it must look in the indexes for any other
    transactions that have recently read the affected data. This process is similar to acquiring a
    write lock on the affected key range, but rather than blocking until the readers have committed,
    the lock acts as a tripwire: it simply notifies the transactions that the data they read may no
    longer be up to date.
* The Trouble with Distributed Systems
** Faults and Partial Failures
*** Cloud Computing and Supercomputing
** Unreliable Networks
*** Network Faults in Practice
*** Deteting Faults
*** Timeouts and Unbounded Delays
*** Synchronous Versus Asynchronous Networks
** Unreliable Clocks
*** Monotonic Versus Time-of-Day Clocks
*** Clocks Synchronization and Accuracy
*** Relying on Synchronized Clocks
*** Process Pauses
** Knowledge, Truth, and Lies
*** The Truth is Defined by the Majority
*** Byzantine Faults
*** System Model and Reality
* Consistency and Consensus
** Consistency Guarantees
** Linearizability
    *Linearizability/atomic consistency/strong consistency/immediate consistency/external
    consistency*: make a system appear as if there were only one copy of the data
*** What Makes a System Linearizable?
    #+ATTR_LATEX: :width .9\textwidth :float nil
    #+NAME:
    #+CAPTION: If a read rquest is concurrent with a write request, it may return either the old or the new value
    [[../images/ddia/10.jpg]]

    To make the system linearizable, we need to add another constraint,
    #+ATTR_LATEX: :width .9\textwidth :float nil
    #+NAME:
    #+CAPTION: After any one read has returned new value, all following reads must also return the new value
    [[../images/ddia/11.jpg]]
    In a linearizable system we imagine that there must be some point in time where the value
    of \(x\) atomically flips from 0 to 1. Thus, if one client's read returns the new value 1, all
    subsequent reads must also return the new value, even if the write operation has not yet
    completed.

    Consider \(cas(x, v_{\text{old}}, v_{\text{new}})\Rightarrow r\) means the client requested an atomic
    /compare-and-set/ operation. \(r\) is the database's response. Each operation is marked with a
    vertical line at the time when we think the operation was executed.
    #+ATTR_LATEX: :width .9\textwidth :float nil
    #+NAME:
    #+CAPTION: Visualizing the points in time at which the reads and writes appear to have taken effect. The final read by \(B\) is not linearizable.
    [[../images/ddia/12.png]]

    The requirement of linearizability is that the lines joining up the operation markers always
    move forward in time (from left to right), never backward.

    #+ATTR_LATEX: :options [Linearizability Versus Serializability]
    #+BEGIN_remark
    Serializability is an isolation property of transactions, where every transaction may read and
    write multiple objects (rows, documents, records) It guarantees that transactions behave the
    same as if they had executed in some serial order (each transaction running to completion before
    the next transaction starts). It is okay for that serial order to be different from the order in
    which transactions were actually run.

    Linearizability is a recency guarantee on reads and writes of a register (an indinpvidual object).
    #+END_remark


*** Relying on Linearizability
**** Locking and leader election
    A system that uses single-leader replication needs to ensure that there is indeed only one
    leader, not several (split brain). One way of electing a leader is to use a lock: every node
    that starts up tries to acquire the lock, and the one that succeeds becomes the  leader. No
    matter how this lock is implemented, it must be linearizable: all nodes must agree which node
    owns the lock; otherwise it is useless.
**** Constraints and uniqueness guarantees
**** Cross-channel timing dependencies
*** Implementing Linearizable Systems
*** The Cost of Linearizability
    #+ATTR_LATEX: :width .9\textwidth :float nil
    #+NAME:
    #+CAPTION: A network interruption forcing a choice between linearizability and availability
    [[../images/ddia/13.png]]

    Consider what happens if there is a network interruption between the two datacenter. Let's assume that the
    network within each datacenter is working, and clients can reach the datacenters, but the datacenters
    cannot connect to each other.

    With a multi-leader database, each datacenter can continue operating normally: since writes from one
    datacenter are asynchronously replicated to the other, the writes are simply queued up and exchanged when
    network connectivity is restored.

    On the other hand, if single-leader replication is used, then the leader must be in one of the
    datacenters. Any writes and any linearizable reads must be sent to the leader - thus, for any clients
    connected to a follower datacenter, those read and write requests must be sent synchronously over the
    network to the leader datacenter.

    If the network between datacenters is interrupted in a single-leader setup, clients connected to follower
    datacenters cannot contact the leader, so they cannot make any writes to the database, nor any
    linearizable reads. They can still make reads from the follower, but they might be stale
    (nonlinearizable). If the application requires linearizable reads and writes, the network interruption
    causes the application to become unavailable in the datacenters that cannot contact the leader.
**** The CAP theorem
    * If your application *requires* linearizability, and some replicas are disconnected from the other replicas
      due to a network problem, then some replicas cannot process requests while they are disconnected: they
      must either wait until the network problem is fixed, or return an error
    * If your application *does not require* linearizability, then it can be written in a way that each replica
      can process requests independently, even if it is disconnected from other replicas
** Ordering Guarantees
*** Ordering and Causality
*** Sequence Number Ordering
*** Total Order Broadcast
** Distributed Transactions and Consensus
*** Atomic Commit and Two-Phase Commit
*** Distributed Transactions in Practice
*** Fault-Tolerant Consensus
*** Membership and Coordination Services
