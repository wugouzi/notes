#+TITLE: Algebra
#+AUTHOR: Serge Lang
#+EXPORT_FILE_NAME: ../latex/Algebra/Algebra.tex
#+LATEX_HEADER: \input{preamble.tex}
#+LATEX_HEADER: \def \Map {\operatorname{Map}}
#+LATEX_HEADER: \def \ev {\text{ev}}
#+LATEX_HEADER: \def \Mor {\text{Mor}}
#+LATEX_HEADER: \def \ord {\operatorname{ord}}
#+LATEX_HEADER: \def \irr {\operatorname{irr}}
* Groups
** Monoids
** Groups
** Normal Subgroups
   Let \(f:G\to G'\) be a group homomorphism, and let \(H\) be its kernel. If \(x\) is an element
   of \(G\), then \(xH=Hx\), because both are equal to \(f^{-1}(f(x))\). We can also rewrite this
   relation as \(xHx^{-1}=H\)

   Conversely, let \(G\) be a group and let \(H\) be a subgroup. Assume that for all
   elements \(x\in G\), we have \(xH\subset Hx\) (or equivalently, \(xHx^{-1}\subset H\)), which
   implies \(H\subset xHx^{-1}\). Thus our condition is equivalent to the condition \(xHx^{-1}=H\) for
   all \(x\in G\). A subgroup \(H\) satisfying this condition will be called *normal*

   Let \(G'\) be the set of cosets of \(H\). (A left coset is equal to a right coset). If \(xH\)
   and \(yH\) are cosets, then their product
   \begin{equation*}
   xHyH=xyHH=xyH
   \end{equation*}
   is also a coset. Hence \(G'\) is a group.

   Let \(f:G\to G'\) be the mapping s.t. \(f(x)\) is the coset \(xH\). Then \(f\) is clearly a
   homomorphism and \(H\) is equal to the kernel.

   The group of cosets of a normal subgroup \(H\) is denoted by \(G/H\) (which we read \(G\)
   modulo \(H\), or \(G\) mod \(H\)). The map \(f\) of \(G\) onto \(G/H\) constructed above is
   called the *canonical map*, and \(G/H\) is called the *factor group* of \(G\) by \(H\)
** Direct Sums and Free Abelian Groups
   Let \(\{A_i\}_{i\in I}\) be a family of abelian groups. We define their *direct sum*
   \begin{equation*}
   A=\bigoplus_{i\in I}A_i
   \end{equation*}
   to be the subset of the direct product \(\prod A_i\) consisting of all families \((x_i)_{i\in I}\)
   with \(x_i\in A_i\) s.t. \(x_i=0\) for all but a finite number of indices \(i\). For each
   index \(j\in I\), we map
   \begin{equation*}
   \lambda_j:A_j\to A
   \end{equation*}
   by letting \(\lambda_j(x)\) be the element whose \(j\)-th component is \(x\), and having all other
   components equal to 0. Then \(\lambda_j\) is an injective homomorphism

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop7.1
   Let \(\{f_i:A_i\to B\}\) be a family of homomorphisms into an abelian group \(B\). Let \(A=\bigoplus A_i\).
   There exists a unique homomorphism
   \begin{equation*}
   f:A\to B
   \end{equation*}
   s.t. \(f\circ\lambda_j=f_j\) for all \(j\)
   #+END_proposition

   #+BEGIN_proof
   Define
   \begin{equation*}
   f((x_i)_{i\in I})=\sum_{i\in I}f_i(x_i)
   \end{equation*}
   #+END_proof

   The property in Proposition ref:prop7.1 is called the *universal property* of the direct sum.

   Let \(A\) be an abelian group and \(B,C\) subgroups. If \(B+C=A\) and \(B\cap C=\{0\}\) then the map
   \begin{equation*}
   B\times C\to A
   \end{equation*}
   given by \((x,y)\mapsto x+y\) is an isomorphism. Instead of writing \(A=B\times C\) we shall
   write \(A=B\oplus C\) and say that \(A\) is the *direct sum* of \(B\) and \(C\). We sue a similar
   notation for the direct sum of a finite number of subgroups \(B_1,\dots,B_n\) s.t.
   \begin{equation*}
   B_1+\dots+B_n=A
   \end{equation*}
   and
   \begin{equation*}
   B_{i+1}\cap(B_1+\dots+B_i)=0
   \end{equation*}
   In that case, we write
   \begin{equation*}
   A=B_1\oplus B_2\oplus\dots\oplus B_n
   \end{equation*}
   Let \(A\) be an abelian group. Let \(\{e_i\}_{i\in I}\) be a family of elements of \(A\). We say that
   this family is a *basis* of \(A\) if the family is not empty, and if every element of \(A\) has a
   unique expression as a linear combination
   \begin{equation*}
   x=\sum x_ie_i
   \end{equation*}
   with \(x_i\in\Z\) and almost all \(x_i=0\). Thus the sum is actually a finite sum. An abelian group is
   *free* if it has a basis. If that is the case, then if we let \(Z_i=\Z\) for all \(i\), then \(A\) is
   isomorphic to the direct sum
   \begin{equation*}
   A\cong\bigoplus_{i\in I}Z_i
   \end{equation*}
   Now let \(S\) be a set. Let \(\Z\la S\ra\) be the set of all maps \(\varphi:S\to\Z\) s.t. \(\varphi(x)=0\) for
   almost all \(x\in S\). Then \(\Z\la S\ra\) is an abelian group. if \(k\) is an integer and \(x\in S\),
   we denote by \(k\cdot x\) the map \varphi s.t. \(\varphi(x)=k\) and \(\varphi(y)=0\) if \(y\neq x\). Then every element
   \varphi of \(\Z\la S\ra\) can be written in the form
   \begin{equation*}
   \varphi=k_1\cdot x_1+\dots+k_n\cdot x_n
   \end{equation*}
   for \(k_i\in\Z\) and \(x_i\in S\), all the \(x_i\) being distinct. Furthermore, \varphi
   *admits a unique such expression*, because if we have
   \begin{equation*}
   \varphi=\sum_{x\in S}k_x\cdot x=\sum_{x\in S}k_x'\cdot x
   \end{equation*}
   then
   \begin{equation*}
   0=\sum_{x\in S}(k_x-k'_x)\cdot x
   \end{equation*}
   whence \(k_x'=k_x\) for all \(x\in S\)

   We map \(S\) into \(\Z\la S\ra\) by the map \(f_S=f\) s.t. \(f(x)=1\cdot x\). \(f(S)\)
   generates \(\Z\la S\ra\). If \(g:S\to B\) is a mapping of \(S\) into some abelian group \(B\), then
   we define a map
   \begin{equation*}
   g_*:\Z\la S\ra\to B
   \end{equation*}
   s.t.
   \begin{equation*}
   g_*\left( \sum_{x\in S}k_x\cdot x \right)=\sum_{x\in S}k_xg(x)
   \end{equation*}
   It's unique for any such homomorphism \(g_*\) must be s.t. \(g_*(1\cdot x)=g(x)\)
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   if \(\lambda:S\to S'\) is a mapping of sets, there is a unique homomorphism \(\bar{\lambda}\) making the
   following diagram commutative
   \begin{center}\begin{tikzcd}
   S\ar[r,"f_S"]\ar[d,"\lambda"]&\Z\la S\ra\ar[d,"\bar{\lambda}"]\\
   S'\ar[r,"f_{S'}"']&\Z\la S'\ra
   \end{tikzcd}\end{center}
   In fact, \(\bar{\lambda}\) is none other than \((f_S\circ\lambda)_*\)
   #+END_proposition

   We shall denote \(\Z\la S\ra\) also \(F_{ab}(S)\) and call \(F_{ab}(S)\) the *free abelian group*
   *generated by* \(S\). We call elements of \(S\) its *free generators*


* Rings
** Rings and Homomorphisms
   A *ring* \(A\) is a set
   1. w.r.t. addition, \(A\) is a commutative group
   2. the multiplication is associative, and has a unit element
   3. for all \(x,y,z\in A\) we have
      \begin{equation*}
      (x+y)z=xz+yz \quad\text{ and }\quad z(x+y)=zx+zy
      \end{equation*}
   (called *distributivity*)

   We denote the unit element for addition by 0, and the unit element for multiplication by 1.
   Observe that \(0x=0\) for all \(x\in A\). /Proof:/ \(0x+x=(0+1)x=x\)

   For any \(x,y\in A\) we have \((-x)y=-(xy)\)

   Let \(A\) be a ring, and let \(U\) be the set of elements of \(A\) which have both a right and
   left inverse. Then \(U\) is a multiplicative group. Indeed, if \(a\) has a right inverse \(b\),
   so that \(ab=1\), and a left inverse \(c\), so that \(ca=1\), then \(cab=b\), whence \(c=b\), and
   we see that \(c\) is a two-sided inverse, and that \(c\) itself has a two-sided inverse,
   namely \(a\). Therefore \(U\) satisfies all the axioms of a multiplicative group, and is called
   the group of *units* of \(A\). It is sometimes denoted by \(A^*\), and is also called the group of
   *invertible* elements of \(A\). A ring \(A\) s.t. \(1\neq 0\) and s.t. every non-zero element is
   invertible is called a *division ring*.

   #+ATTR_LATEX: :options [The Shift Operator]
   #+BEGIN_examplle
   Let \(E\) be the set of all sequences
   \begin{equation*}
   a=(a_1,a_2,a_3,\dots`)
   \end{equation*}
   of integers. One can define addition componentwise. Let \(R\) be the set of all
   mappings \(f:E\to E\) of \(E\) into itself s.t. \(f(a+b)=f(a)+f(b)\). Then \(R\) is a ring. Let
   \begin{equation*}
   T(a_1,a_2,a_3,\dots)=(0,a_1,a_2,a_3,\dots)
   \end{equation*}
   Verify that \(T\) is left invertible but not right invertible
   #+END_examplle

   A ring \(A\) is said to be *commutative* if \(xy=yx\) for all \(x,y\in A\). A commutative division
   ring is called a *field*. By definition, a field contains at least two elements, namely 0 and 1.

   A subset \(B\) of ring \(A\) is called a *subring* if it is an additive subgroup, if it contains
   the multiplicative unit, and if \(x,y\in B\) implies \(xy\in B\). If that is the case, then \(B\) is
n   itself a ring, the laws of operation in \(B\) being the same as the laws of operation in \(A\)

   For example, the *center* of a ring \(A\) is the subset of \(A\) consisting of all
   elements \(a\in A\) s.t. \(ax=xa\) for all \(x\in A\). The center of \(A\) is a subring.

   If \(x,y_1,\dots,y_n\) are elements of a ring, then by induction one sees that
   \begin{equation*}
   x(y_1+\dots+y_n)=xy_1+\dots+xy_n
   \end{equation*}
   If \(x_i(i=1,\dots,n)\) and \(y_j(j=1,\dots,m)\) are elements of \(A\), then it is also easily proved that
   \begin{equation*}
   \left( \sum_{i=1}^nx_i \right)\left( \sum_{j=1}^my_j \right)=
   \sum_{i=1}^n\sum_{j=1}^mx_iy_j
   \end{equation*}
   Furthermore, distributivity holds for subtraction, e.g.
   \begin{equation*}
   x(y_1-y_2)=xy_1-xy_2
   \end{equation*}

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   Let \(S\) be a set and \(A\) a ring. Let \(\Map(S,A)\) be the set of mappings of \(S\)
   into \(A\). Then \(\Map(S,A)\) is a ring if for \(f,g\in\Map(S,A)\) we define
   \begin{equation*}
   (fg)(x)=f(x)g(x)\quad\text{ and }\quad (f+g)(x)=f(x)+g(x)
   \end{equation*}
   for all \(x\in S\).

   Let \(M\) be an additive abelian group, and let \(A\) be the set \(\End(M)\) of
   group-homomorphisms of \(M\) into itself. We define addition in \(A\) to be the addition of
   mappings, and we define multiplication to be *composition* of mappings
   #+END_examplle

   #+ATTR_LATEX: :options [The convolution product]
   #+BEGIN_examplle
   Let \(G\) be a group and let \(K\) be a field. Denote by \(K[G]\) the set of all formal linear
   combinations \(\alpha=\sum a_xx\) with \(x\in G\) and \(a_x\in K\), s.t. all but finite number of \(a_x\) are
   equal to 0. If \(\beta=\sum b_xx\in K[G]\), then one can define the product
   \begin{equation*}
   \alpha\beta=\sum_{x\in G}\sum_{y\in G}a_xb_yxy=\sum_{z\in G}\left( \sum_{xy=z}a_xb_y \right)z
   \end{equation*}
   With this product, the *group ring* \(K[G]\) is a ring. \(K[G]\) is commutative iff \(G\) is
   commutative. The second sum on the right  defines what is called a *convolution product*.
   If \(f,g\) are functions on a group \(G\), we define their *convolution* \(f*g\) by
   \begin{equation*}
   (f*g)(z)=\sum_{xy=z}f(x)g(y)
   \end{equation*}
   #+END_examplle

   A *left ideal* \(\fa\) in a ring \(A\) is a subset of \(A\) which is a subgroup of the additive group
   of \(A\), s.t. \(A\fa\subset\fa\) (and hence \(A\fa=\fa\) since \(A\) contains 1). To define a right ideal, we
   quire \(\fa A=\fa\), and a *two-sided ideal* is a subset which is both a left and right ideal. A
   two-sided ideal is called an *ideal* in this section.

   If \(A\) is a ring and \(a\in A\), then \(Aa\) is a left ideal, called *principal*. We say that \(a\)
   is a generator of \(\fa\) (over \(A\)). \(AaA\) is a principal two-sided ideal
   if \(AaA=\{\sum x_iay_i\mid x_i,y_i\in A\}\). More generally, let \(a_1,\dots,a_n\in A\). We denote by \((a_1,\dots,a_n)\)
   the set of elements of \(A\) which can be written in the form
   \begin{equation*}
   x_1a_1+\dots+x_na_n\quad\text{with}\quad x_i\in A
   \end{equation*}
   Then this set of elements is immediately verified to be a left ideal, and \(a_1,\dots,a_n\) are called
   *generators* of the left ideal.

   If \(\{\fa_i\}_{i\in I}\) is a family of ideals, then their intersection
   \begin{equation*}
   \bigcap_{i\in I}\fa_i
   \end{equation*}
   is also an ideal

   A *commutative* ring s.t. every ideal is principal and s.t. \(1\neq 0\) is called a *principal* ring

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   The integers \(\Z\) form a ring, which is commutative. Let \(\fa\) be an ideal \(\neq\Z\) and \(\neq 0\).
   If \(n\in\fa\) then \(-n\in\fa\). Let \(d\) be the smallest integer \(>0\) lying in \(\fa\). If \(n\in\fa\)
   then there exists integers \(q,r\) with \(0\le r<d\) s.t.
   \begin{equation*}
   n=dq+r
   \end{equation*}
   Since \(\fa\) is an ideal, it follows that \(r\) lies in \(\fa\), hence \(r=0\). Hence \(\fa\) consists
   of all multiples \(qd\) of \(d\), which \(q\in\Z\), and \(\Z\) is a principal ring.
   #+END_examplle

   Let \(\fa,\fb\) be ideals of \(A\). We define \(\fa\fb\) to be the set of all sums
   \begin{equation*}
   x_1y_1+\dots+x_ny_n
   \end{equation*}
   with \(x_i\in\fa\) and \(y_i\in\fb\). \(\fa\fb\) is an ideal, and that the set of ideals forms a multiplicative
   monoid, the unit element being the ring itself. This unit element is called the *unit ideal* and is
   often written (1).

   If \(\fa,\fb\) are left ideals of \(A\), then \(\fa+\fb\) (the sum being taken as additive subgroup
   of \(A\)) is obviously a left ideal. Thus ideals also form a monoid under addition. We also have
   distributivity: if \(\fa_1,\dots,\fa_n,\fb\) are ideals of \(A\), then
   \begin{equation*}
   \fb(\fa_1+\dots+\fa_n)=\fb\fa_1+\dots+\fb\fa_n
   \end{equation*}

   Let \(\fa\) be a left ideal. Define \(\fa A\) to be the set of all sums \(a_1x_1+\dots+a_nx_n\)
   with \(a_i\in\fa\) and \(x_i\in A\). Then \(\fa A\) is an ideal.

   Suppose that \(A\) is commutative. Let \(\fa,\fb\) be ideals. Then trivially
   \begin{equation*}
   \fa\fb\subset\fa\cap\fb
   \end{equation*}
   If \(\fa+\fb=A\) then \(\fa\fb=\fa\cap \fb\).  Suppose \(x\in\fa\cap\fb\) and \(x=a_x+b_x\), where \(a_x\in\fa\) and \(b_x\in\fb\).
   Then \(a_x\in\fb\) and \(b_x\in\fa\). If \(1=a_1+b_1\) then \(x\cdot 1=(a_x+b_x)(a_1+b_1)\in\fa\fb\)

   [[index:ring homomorphism]]
   By a *ring homomorphism* one means a mapping \(f:A\to B\) where \(A,B\) are rings, and s.t. \(f\) is
   a monoid-homomorphism for the multiplicative structures on \(A\) and \(B\), and also a monoid
   homomorphism for the additive structure. In other words
   \begin{alignat*}{2}
   &f(a+a')=f(a)+f(a')\quad&&f(aa')=f(a)f(a')\\
   &f(1)=1&&f(0)=0
   \end{alignat*}
   for all \(a,a'\in A\).

   The kernel of a ring homomorphism \(f:A\to B\) is an ideal of \(A\).

   Conversely, let \(\fa\) be an ideal of the ring \(A\). We can construct the *factor ring* \(A/\fa\) as
   follows. Viewing \(A\) and \(\fa\) as additive groups, let \(A/\fa\) be the factor group. If \(x+\fa\)
   and \(y+\fa\) are two cosets of \(\fa\), we define \((x+\fa)(y+\fa)\) to be the coset \(xy+\fa\). This
   coset is well-defined, for if \(x_1,y_1\) are in the same coset as \(x,y\) respectively, then one
   verifies that \(x_1y_1\) is in the same coset as \(xy\). Unit element is \(1+\fa\).

   We therefore defined a ring structure on \(A/\fa\) and the caonical map
   \begin{equation*}
   f:A\to A/\fa
   \end{equation*}
   is then clearly a ring homomorphism

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If \(g:A\to A'\) is a ring homomorphism whose kernel contains \(\fa\), then there exists a unique
   ring homomorphism \(g_*:A/\fa\to A'\) making the following diagram commutative
   \begin{center}\begin{tikzcd}[column sep=small]
   A\ar[rr,"g"]\ar[rd,"f"']&&A'\\
   &A/\fa\ar[ur,"g_*"']
   \end{tikzcd}\end{center}

   #+END_proposition

   Indeed, viewing \(f,g\) as group homomorphisms, there is a unique group homomorphism \(g_*\)
   making our diagram commutative

   #+BEGIN_proof
   If \(x\in A\) then \(g(x)=g_*f(x)\). Hence for \(x,y\in A\)
   \begin{align*}
   g_*(f(x)f(y))&=g_*(f(xy))=g(xy)=g(x)g(y)\\
   &=g_*f(x)g_*f(y)
   \end{align*}
   Given \(\xi,\eta\in A/\fa\), there exists \(x,y\in A\) s.t. \(f(x)=\xi\) and \(f(y)=\eta\). Since \(f(1)=1\), we
   get \(g_*f(1)=g(1)=1\) and hence the two conditions that \(g_*\) be a multiplicative
   monoid-homomorphism are satisfied
   #+END_proof

   Let \(A\) be a ring, and denote its unit element by \(e\) for the moment. The map
   \begin{equation*}
   \lambda:\Z\to A
   \end{equation*}
   s.t. \(\lambda(n)=ne\) is a ring homomorphism, and its kernel is an ideal \((n)\), generated by an
   integer \(n\ge 0\). We have a canonical injective homomorphism \(\Z/n\Z\to A\) which is a (ring)
   isomorphism between \(\Z/n\Z\) and a subring of \(A\). If \(n\Z\) is a prime ideal, then \(n=0\)
   or \(n=p\) for some prime number \(p\). In the first place, \(A\) contains as a subring a ring
   which is isomorphic to \(\Z\), and which is often identified with \(\Z\). In that case, we say
   that \(A\) has *characteristic* 0. if on the other hand \(n=p\) then we say that \(A\) has
   *characteristic* \(p\), and \(A\) contains (an isomorphic image of) \(\Z/p\Z\) as a subring. We
   abbreviate \(\Z/p\Z\) by \(\F_p\).

   If \(K\) is a field, then \(K\) has characteristic 0 or \(p>0\). (if its characteristic
   is \(a\cdot b\), then \(a\cdot b\cdot 1=0\) but field is an integral domain). In the first case, \(K\)
   contains as a subfield an isomorphic image of the rational numbers, and in the second case, it
   contains an isomorphic image of \(\F_p\). In either case, this subfield will be called the *prime
   field* (contained in \(K\)). Since this prime field is the smallest subfield of \(K\) containing 1
   and has no automorphism except the identity, it is customary to identiy it with \(\Q\) or \(\F_p\)
   as the case may be. By the *prime ring* (in \(K\)) we shall mean either the integers \(\Z\) if \(K\)
   has characteristic 0 or \(\F_p\) if \(K\) has characteristic \(p\). <<Problem1>>

   Let \(A\) be a subring of a ring \(B\). Let \(S\) be a subset of \(B\) commuting with \(A\). We
   denote by \(A[S]\) the set of all elements
   \begin{equation*}
   \sum a_{i_1\dots i_n}s_1^{i_1}\dots s_n^{i_n}
   \end{equation*}
   the sum ranging over a finite number of \(n\)-tuples \((i_1,\dots,i_n)\) of integers \(\ge 0\),
   and \(a_{i_1,\dots,i_n}\in A\), \(s_1,\dots,s_n\in S\). If \(B=A[S]\) , we say that \(S\) is a set of
   *generators* (or *ring generators*) for \(B\) over \(A\), or that \(B\) is *generated* by \(S\)
   over \(A\). If \(S\) is finite, \(B\) is *finitely generated as a ring over* \(A\). Note that \(S\)
   is not commutative.

   Let \(A\) be a ring, \(\fa\) an ideal, and \(S\) a subset of \(A\). We write
   \begin{equation*}
   S\equiv 0\mod \fa
   \end{equation*}
   if \(S\subset\fa\). If \(x,y\in A\) we write
   \begin{equation*}
   x\equiv y\mod\fa
   \end{equation*}
   if \(x-a\in\fa\).  If \(\fa\) is principal, equal to \((a)\), then we also write
   \begin{equation*}
   x\equiv y\mod a
   \end{equation*}
   If \(f:A\to A/\fa\) is the canonical homomorphism, then \(x\equiv y\mod\fa\) means that \(f(x)=f(y)\)

   The factor ring \(A/\fa\) is also called a *residue class ring*. Cosets of \(\fa\) in \(A\) are called
   *residue classes* modulo \(\fa\), and if \(x\in A\), then the coset \(x+\fa\) is called the *residue class*
   *of \(x\) modulo \(\fa\)*

   An injective ring homomorphism \(f:A\to B\) establishes a ring isomorphism between \(A\) and its
   image. Such a homomorphism will be called an *embedding*

   Let \(f:A\to A'\) be a ring homomorphism, and let \(\fa'\) be an ideal of \(A'\). Then \(f^{-1}(a')\)
   is an ideal \(\fa\) in \(A\), and we have an induced injective homomorphism
   \begin{equation*}
   A/\fa\to A'/\fa'
   \end{equation*}

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Products exist in the category of rings
   #+END_proposition

   Let \(A\) be a ring. Elements \(x,y\in A\) are said to be *zero divisors* if \(x\neq 0\), \(y\neq 0\)
   and \(xy=0\). A ring \(A\) is *entire* if \(1\neq 0\), if \(A\) is commutative and if there are no
   zero divisors in the ring. (Entire rings are also called *integral domains*)

   Let \(m\) be a positive integer \(\neq 1\). The ring \(\Z/m\Z\) has zero divisors iff \(m\) is not
   prime.

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(A\) be an entire ring, and let \(a,b\) be non-zero elements of \(A\). Then \(a,b\) generate
   the same ideal iff there exists a unit \(u\) of \(A\) s.t. \(b=au\).
   #+END_proposition

   #+BEGIN_proof
   Assume \(Aa=Ab\). Then \(a=bc\) and \(b=ad\) for some \(c,d\in A\). Hence \(a=adc\)
   whence \(a(1-dc)=0\) and therefore \(dc=1\). Hence \(c\) is a unit
   #+END_proof

** Commutative Rings
   Assume \(A\) is commutative

   A *prime* ideal in \(A\) is an ideal \(\fp\neq A\) s.t. \(A/\fp\) is entire. Equivalently, we could say
   that it is an ideal \(\fp\neq A\) s.t. whenever \(x,y\in A\) and \(xy\in\fp\) then \(x\in\fp\) or \(y\in\fp\). A
   prime ideal is often called simply a *prime*

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Every maximal ideal is prime
   #+END_proposition

   #+BEGIN_proof
   Let \(\fm\) be maximal and let \(x,y\in A\) s.t. \(xy\in\fm\). Suppose \(x\not\in\fm\), then \(\fm+Ax\) is an
   ideal properly containing \(\fm\), hence equal to \(A\). Hence we can write
   \begin{equation*}
   1=u+ax
   \end{equation*}
   with \(u\in\fm\) and \(a\in A\). Multiplying by \(y\) we find
   \begin{equation*}
   y=yu+axy
   \end{equation*}
   whence \(y\in\fm\).
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(\fa\) be an ideal \(\neq A\). Then \(\fa\) is contained in some maximal ideal \(\fm\)
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   The ideal \(\{0\}\) is a prime ideal of \(A\) iff \(A\) is entire
   #+END_proposition

   The only ideals of a field are itself and the zero ideal

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If \(\fm\) is a maximal ideal of \(A\), then \(A/\fm\) is a field
   #+END_proposition

   #+BEGIN_proof
   If \(x\in A\), we denote by \(\barx\) its residue class mod \(\fm\). Since \(\fm\neq A\) we note
   that \(A/\fm\)  has a unit element \(\neq 0\). Any non-zero element of \(A/\fm\) can be written
   as \(\barx\) for some \(x\in A\), \(x\not\in\fm\). To find its inverse, note that \(\fm+Ax\) is an ideal
   of \(A\neq\fm\) and hence equal to \(A\). Hence we can write
   \begin{equation*}
   1=u+yx
   \end{equation*}
   with \(u\in\fm\) and \(y\in A\). This means that \(\bary\barx=1=\bar{1}\) and hence that \(\barx\) has
   an inverse.
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(f:A\to A'\) be a homomorphism of commutative rings. Let \(\fp'\) be a prime ideal of \(A'\) and
   let \(\fp=f^{-1}\fp'\). Then \(\fp\) is prime
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   Let \(\Z\) be the ring of integers. Since an ideal is also an additive subgroup of \(\Z\), every
   ideal \(\neq\{0\}\) is principal, of the form \(n\Z\) for some integer \(n>0\). ([[https://math.stackexchange.com/questions/101348/show-that-every-ideal-of-the-ring-mathbb-z-is-principal][proof]])

   Let \(\fp\) be a prime ideal \(\neq\{0\}\), \(\fp=n\Z\). Then \(n\) must be a prime number. Conversely,
   if \(p\) is a prime number, then \(p\Z\) is a prime ideal. Furthermore, \(p\Z\) is a maximal ideal.
   Suppose \(p\Z\) is contained in some ideal \(n\Z\), then \(p=nm\) for some integer \(m\),
   whence \(n=p\) or \(n=1\), thereby proving \(p\Z\) maximal
   #+END_examplle

   if \(n\) is an integer, the factor ring \(\Z/n\Z\) is called the ring of *integers modulo* \(n\). We
   also denote
   \begin{equation*}
   \Z/n\Z=\Z(n)
   \end{equation*}
   If \(n\) is a prime number \(p\), then the ring of integers modulo \(p\) is in fact a field,
   denoted by \(\F_p\). In particular, the multiplicative group of \(\F_p\) is called the group of
   non-zero integers modulo \(p\). From the elementary properties of groups, we get a standard fact
   of elementary number theory: if \(x\) is an integer \(\neq 0\mod p\), then \(x^{p-1}\equiv 1\mod p\)
   (Fermat's Theorem). Similarly given an integer \(n>1\), the units in the ring \(\Z/n\Z\) consist
   of those residue class mod \(n\Z\) which are represented by integers \(m\neq 0\) and prime to \(n\).
   The order of the group of units in \(\Z/n\Z\) is called by definition \(\varphi(n)\) (where \varphi is known as
   the *Euler phi-function*). Consequently, if \(x\) is an integer prime to \(n\),
   then \(x^{\varphi(n)}\equiv 1\mod n\)


   #+ATTR_LATEX: :options [Chinese Remainder Theorem]
   #+BEGIN_theorem
   Let \(\fa_1,\dots,\fa_n\) be ideals of \(A\) s.t. \(\fa_i+\fa_j=A\) for all \(i\neq j\). Given
   elements \(x_1,\dots,x_n\in A\) ,there exists \(x\in A\) s.t. \(x\equiv x_i\mod\fa_i\) for all \(i\)
   #+END_theorem

   #+BEGIN_proof
   For \(n=2\) we have an expression
   \begin{equation*}
   1=a_1+a_2
   \end{equation*}
   for some \(a_i\in\fa_i\), and we let \(x=x_2a_1+x_1a_2\)

   For each \(i\ge 2\) we can find elements \(a_i\in\fa_1\) and \(b_i\in\fa_i\) s.t.
   \begin{equation*}
   a_i+b_i=1,\quad i\ge 2
   \end{equation*}
   The products \(\prod_{i=2}^n(a_i+b_i)\) is equal to 1, and lies in
   \begin{equation*}
   \fa_1+\prod_{i=2}^n\fa_i
   \end{equation*}
   Hence
   \begin{equation*}
   \fa_1+\prod_{i=2}^n\fa_i=A
   \end{equation*}
   By theorem for \(n=2\), we can find an element \(y_1\in A\) s.t.
   \begin{align*}
   &y_1\equiv 1\mod\fa_1\\
   &y_1\equiv 0\mod\prod_{i=2}^n\fa_i
   \end{align*}
   We find similarly elements \(y_2,\dots,y_n\) s.t.
   \begin{equation*}
   y_j\equiv 1\mod\fa_j \quad\text{ and }\quad y_j\equiv 0\mod\fa_i\text{ for }i\neq j
   \end{equation*}
   Then \(x=x_1y_1+\dots+x_ny_n\) satisfies our requirements
   #+END_proof

   In the same vein as above, we observe that if \(\fa_1,\dots,\fa_n\) are ideals of a ring \(A\) s.t.
   \begin{equation*}
   \fa_1+\dots+\fa_n=A
   \end{equation*}
   and if \(v_1,\dots,v_n\) are positive integers, then
   \begin{equation*}
   \fa_1^{v_1}+\dots+\fa_n^{v_n}=A
   \end{equation*}
   <<Problem2>>

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Let \(\fa_1,\dots,\fa_n\) be ideals of \(A\). Assume that \(\fa_i+\fa_j=A\) for \(i\neq j\). Let
   \begin{equation*}
   f:A\to\prod_{i=1}^nA/\fa_i=(A/\fa_1)\times\dots\times(A/\fa_n)
   \end{equation*}
   be the map of \(A\) into the product induced by the canonical map of \(A\) onto \(A/\fa_i\) for each
   factor. Then the kernel of \(f\) is \(\bigcap_{i=1}^n\fa_i\) and \(f\) is surjective, thus giving an
   isomorphism
   \begin{equation*}
   A/\bigcap\fa_i\cong\prod A/\fa_i
   \end{equation*}
   #+END_corollary

   #+BEGIN_proof
   Surjectivity follows from the theorem
   #+END_proof

   Let \(m\) be an integer \(>1\), and let
   \begin{equation*}
   m=\prod_ip_i^{r_i}
   \end{equation*}
   be a factorization of \(m\) into primes, with exponents \(r_i\ge 1\). Then we have a ring
   isomorphism
   \begin{equation*}
   \Z/m\Z\cong\prod_i\Z/p_i^{r_i}\Z
   \end{equation*}
   If \(A\) is a ring, we denote as usual by \(A^*\) the multiplicative group of invertible elements
   of \(A\)

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   The preceding ring isomorphism of \(\Z/m\Z\) onto the product induces a group isomorphism
   \begin{equation*}
   (\Z/m\Z)^*\cong\prod_i(\Z/p_i^{r_i}\Z)^*
   \end{equation*}
   #+END_proposition

   In view of our isomorphism, we have
   \begin{equation*}
   \varphi(m)=\prod_i\varphi(p_i^{r_i})
   \end{equation*}
   If \(p\) is a prime number and \(r\) an integer \(\ge 1\), then
   \begin{equation*}
   \varphi(p^r)=(p-1)p^{r-1}
   \end{equation*}
   If \(r=1\), then \(\Z/p\Z\) is a field, and the multiplicative group of that field has
   order \(p-1\). Let \(r\) be \(\ge 1\), and consider the canonical ring homomorphism
   \begin{equation*}
   \Z/p^{r+1}\Z\to\Z/p^r\Z
   \end{equation*}
   arising from the inclusion of ideals \((p^{r+1})\subset(p^r)\). We get an induced group homomorphism
   \begin{equation*}
   \lambda:(Z/p^{r+1}\Z)^*\to(\Z/p^r\Z)^*
   \end{equation*}
   which is surjective because any integer \(a\) which represents an element of \(\Z/p^r\Z\) and is
   prime to \(p\) will represent an element of \((\Z/p^{r+1}\Z)^*\). Let \(a\) be an integer
   representing an element of \((\Z/p^{r+1}\Z)^*\) s.t. \(\lambda(a)=1\). Then
   \begin{equation*}
   a\equiv 1\mod p^{r}\Z
   \end{equation*}
   P96

   *Application: The ring of endomorphisms of a cyclic group*.
   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   Let \(A\) be a cyclic group of order \(n\). For each \(k\in\Z\) let \(f_k:A\to A\) be the
   endomorphism \(x\mapsto kx\) (writing \(A\) additively). Then \(k\mapsto f_k\) induces a ring
   homomorphism \(\Z/n\Z\cong\End(A)\), and a group isomorphism \((\Z/n\Z)^*\cong\Aut(A)\)
   #+END_theorem

   #+BEGIN_proof
   The fact that \(k\mapsto f_k\) is ring homomorphism is a restatement of the formulas
   \begin{equation*}
   1a=a,\quad (k+k')a=ka+k'a,\quad (kk')a=k(k'a)
   \end{equation*}
   #+END_proof


** Polynomials and Group Rings
   Consider an infinite cyclic group generated by an element \(X\). We let \(S\) be the subset
   consisting of powers \(X^r\) with \(r\ge 0\). Then \(S\) is a monoid. We define the set of
   *polynomials* \(A[X]\) to be the set of functions \(S\to A\) which are equal to 0 except for a finite
   number  of elements of \(S\). For each element \(a\in A\) we denote by \(aX^n\) the function which
   has the value \(a\) on \(X^n\) and the value 0 for all other elements of \(S\). Then it is
   immediate that a polynomial can be written uniquely as a finite sum
   \begin{equation*}
   a_0X^0+\dots+a_nX^n
   \end{equation*}
   for some integer \(n\in\N\) and  \(a_i\in A\). Such a polynomial is denoted by \(f(X)\). The
   elements \(a_i\in A\) are called the *coefficients* of \(f\). We define the product according to the
   convolution rule. Thus, given polynomials
   \begin{equation*}
   f(X)=\sum_{i=0}^na_iX^i \quad\text{ and }\quad g(X)=\sum_{j=0}^mb_jX^j
   \end{equation*}
   we define the product to be
   \begin{equation*}
   f(X)g(X)=\sum_{k=0}^{m+n}\left( \sum_{i+j=k}a_ib_j \right)X^k
   \end{equation*}
   This product is associative and distributive. \(1X^0\) is the unit element.  There is also an
   embedding
   \begin{gather*}
   A\to A[X]\\
   a\mapsto aX^0
   \end{gather*}
   Let \(A\) be a subring of a commutative ring \(B\). Let \(x\in B\). If \(f\in A[X]\) is a polynomial,
   we define the associated *polynomial function*
   \begin{equation*}
   f_B:B\to B
   \end{equation*}
   by letting
   \begin{equation*}
   f_B(x)=f(x)=a_0+a_1x+\dots+a_nx^n
   \end{equation*}
   Given an element \(b\in B\), directly from the definition of multiplication of polynomials, we find
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   The association
   \begin{equation*}
   \ev_b:f\mapsto f(b)
   \end{equation*}
   is a ring homomorphism of \(A[X]\) into \(B\)
   #+END_proposition

   This homomorphism is called the *evaluation homomorphism*, and is also said to be obtained by
   *substituting* \(b\) for \(X\) in the polynomial

   Let \(x\in B\). We see that the subring \(A[x]\) of \(B\) generated by \(x\) over \(A\) is a ring
   of all polynomial values \(f(x)\) for \(f\in A[X]\). If the evaluation map \(f\mapsto f(x)\) gives an
   isomorphism of \(A[X]\) with \(A[x]\), then we say that \(x\) is *transcendental* over \(A\), or
   that \(x\) is a *variable* over \(A\). In particular, \(X\) is a variable over \(A\)

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   Let \(\alpha=\sqrt{2}\). Then the set of all real numbers of the form \(a+b\alpha\), with \(a,b\in\Z\) is a
   subring of the real numbers, generated by \(\sqrt{2}\). \alpha is not transcendental over \(\Z\),
   because the polynomial \(X^2-2\) lies in the kernel of the evaluation map \(f\mapsto f(\sqrt{2})\). On
   the other hand, it can be shown that \(e\) and \pi are transcendental over \(\Q\)
   #+END_examplle

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   Let \(p\) be a prime number and let \(K=\Z/p\Z\). Then \(K\) is a field. Let \(f(X)=X^p-X\in K[X]\).
   Then \(f\) is not the zero polynomials. But \(f_K\) is the zero function. Indeed, \(f_K(0)=0\).
   If \(x\in K\), \(x\neq 0\), then since the multiplicative group of \(K\) has order \(p-1\). it follows
   that \(x^{p-1}=1\), whence \(x^p=x\), so \(f(x)\). Thus a non-zero polynomial gives rise to the
   zero function on \(K\)
   #+END_examplle

   Let
   \begin{equation*}
   \varphi:A\to B
   \end{equation*}
   be a homomorphism of commutative rings. Then there is an associated homomorphism of the
   polynomial rings \(A[X]\to B[X]\) s.t.
   \begin{equation*}
   f(X)=\sum a_iX^i\mapsto\sum\varphi(a_i)X^i=(\varphi f)(X)
   \end{equation*}
   We call \(f\mapsto\varphi f\) the *reduction map*

   Let \(\fp\) be a prime ideal of \(A\). Let \(\varphi:A\to A'\) be the canonical homomorphism of \(A\)
   onto \(A/\fp\). If \(f(X)\) is a polynomial in \(A[X]\), then \(\varphi f\) will sometimes be called the
   *reduction of \(f\) modulo \(\fp\)*.

   For example, taking \(A=\Z\) and \(\fp=(p)\) for some prime number \(p\), we can speak of the
   polynomial \(3X^4-X+2\) as a polynomial mod 5, viewing the coefficients as elements of \(\Z/5\Z\)

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(\varphi:A\to B\) be a homomorphism of commutative rings. Let \(x\in B\). There is a unique
   homomorphism extending \varphi
   \begin{equation*}
   A[X]\to B\quad\text{ s.t. }\quad X\mapsto x
   \end{equation*}
   and for this homomorphism \(\sum a_iX^i\mapsto\sum\varphi(a_i)x^i\)
   #+END_proposition

   The homomorphism of the above statement may be views as the composite
   \begin{center}\begin{tikzcd}
   A[X\ar[r]]&B[X]\ar[r,"\ev_x"]&B
   \end{tikzcd}\end{center}

   When writing a polynomial \(f(X)=\displaystyle\sum_{i=1}^na_iX^i\), if \(a_n\neq 0\) then we define \(n\) to be the
   *degree* of \(f\). Thus the degree of \(f\) is the smallest integer \(n\) s.t. \(a_r=0\)
   for \(r>n\). If \(f=0\) (i.e. \(f\) is the zero polynomial), then by convention, we define the
   degree of \(f\) to be \(-\infty\). We agree to the convention that
   \begin{equation*}
   -\infty+-\infty=-\infty,\quad-\infty+n=-\infty,\quad-\infty<n
   \end{equation*}
   for all \(n\in\Z\), and no other operation with \(-\infty\) is defined. A polynomial of degree 1 is also
   called a *linear* polynomial. If \(f\neq 0\) and \(\deg f=n\) then we call \(a_n\) the *leading
   coefficient* of \(f\). We call \(a_0\) its *constant term*

   Let
   \begin{equation*}
   g(X)=b_0+\dots+b_mX^m
   \end{equation*}
   be a polynomial in \(A[X]\), of degree \(m\), and assume \(g\neq 0\). Then
   \begin{equation*}
   f(X)g(X)=a_0b_0+\dots+a_nb_mX^{m+n}
   \end{equation*}
   Therefore
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If we assume that at least one of the leading coefficients \(a_n\) or \(b_m\) is not a divisor of
   0 in \(A\), then
   \begin{equation*}
   \deg(fg)=\deg f+\deg g
   \end{equation*}
   and the leading coefficient of \(fg\) is \(a_nb_m\). This holds in particular when \(a_n\)
   or \(b_m\) is a unit in \(A\), or when \(A\) is entire. Consequently, when \(A\) is
   entire, \(A[X]\) is also entire
   #+END_proposition

   If \(f=0\) or \(g=0\) we still have
   \begin{equation*}
   \deg(fg)=\deg f+\deg g
   \end{equation*}
   if we agree that \(-\infty+m=-\infty\) for any integer \(m\)

   Let \(A\) be a subring of a commutative ring \(B\). Let \(x_1,\dots,x_n\in B\). For each \(n\)-tuple of
   integers \((v_1,\dots,v_n)=\bv\in\N^n\), let \(\bx=(x_1,\dots,x_n)\), and
   \begin{equation*}
   M_{\bv}(\bx)=x_1^{v_1}\dots x_n^{v_n}
   \end{equation*}
   The set of such elements forms a monoid under multiplication. Let \(A[x]=A[x_1,\dots,x_n]\) be the
   subring of \(B\) generated by \(x_1,\dots,x_n\) over \(A\). Then every element of \(A[x]\) can be
   written as a finite sum
   \begin{equation*}
   \sum a_{\bv}M_{\bv}(\bx) \quad\text{ and }\quad a_{\bv}\in A
   \end{equation*}

   Using the construction of polynomials in one variable repeatedly, we may form the ring
   \begin{equation*}
   A[X_1,\dots,X_n]=A[X_1][X_2]\dots[X_n]
   \end{equation*}
   selecting \(X_n\) to be variable over \(A[X_1,\dots,X_{n-1}]\). Then every element \(f\)
   of \(A[X_1,\dots,X_n]=A[X]\) has a /unique/ expression as a finite sum
   \begin{equation*}
   f=\sum_{j=0}^{d_n}f_j(X_1,\dots,X_{n-1})X_n^j \quad\text{with}\quad f_j\in A[X_1,\dots,X_{n-1}]
   \end{equation*}
   Therefore by induction we can write \(f\) uniquely as a sum
   \begin{align*}
   f&=\sum_{v_n=0}^{d_n}\left(
   \sum_{v_1,\dots,v_{n-1}}a_{v_1\dots v_n}X_1^{v_1}\dots X_{n-1}^{v_{n-1}} \right)X^{v_n}_n\\
   &=\sum a_{\bv}M_{\bv}(X)=\sum a_{\bv}X_1^{v_1}\dots X_n^{v_n}
   \end{align*}
   with elements \(a_{\bv\in A}\), which are called the *coefficients* of \(f\). The products
   \begin{equation*}
   M_{\bv}(X)=X_1^{v_1}\dots X_n^{v_n}
   \end{equation*}
   will be called *primitive monomials*. Elements of \(A[X]\) are called *polynomials* (in \(n\)
   variables). We call \(a_{\bv}\) its *coefficients*

   GIven \(\bx=(x_1,\dots,x_n)\) and \(f\), we define
   \begin{equation*}
   f(x)=\sum a_{\bv}M_{\bv}(\bx)=\sum a_{\bv}x_1^{v_1}\dots x_n^{v_n}
   \end{equation*}
   Then the *evaluation map*
   \begin{equation*}
   \ev_{\bx}:A[X]\to B \quad\text{with}\quad f\mapsto f(x)
   \end{equation*}
   is a ring homomorphism

   Elements \(x_1,\dots,x_n\in B\) are called *algebraically independent* over \(A\) if the evaluation map
   \begin{equation*}
   f\mapsto f(x)
   \end{equation*}
   is injective. Equivalently, we could say that if \(f\in A[X]\) is a polynomial and \(f(x)=0\)
   then \(f=0\).; in other words, there are no non-trivial polynomial relations among \(x_1,\dots,x_n\)
   over \(A\).

   By the *degree* of a primitive monomial
   \begin{equation*}
   M_{\bv}(X)=X_1^{v_1}\dots X_n^{v_n}
   \end{equation*}
   we shall mean the integer \(\abs{v}=v_1+\dots+v_n\)

   A polynomial
   \begin{equation*}
   aX_1^{v_1}\dots X_n^{v_n}\quad(a\in A)
   \end{equation*}
   will be called a *monomial*

   If \(f(X)\) is a polynomial in \(A[X]\) written as
   \begin{equation*}
   f(X)=\sum a_{\bv}X_1^{v_1}\dots X_n^{v_n}
   \end{equation*}
   we define the *degree* of \(f\) to be the maximum of the degrees of the monomials \(M_{\bv}(X)\)
   s.t. \(a_{\bv}\neq 0\). (Such monomials are said to *occur* in the polynomial)

   For each integer \(d\ge 0\), given a polynomial \(f\), let \(f^{(d)}\) be the sum of all monomials
   occuring in \(f\) and having degree \(d\). Then
   \begin{equation*}
   f=\sum_df^{(d)}
   \end{equation*}
   Suppose \(f\neq 0\), we say that \(f\) is *homogeneous* of degree \(d\) if \(f=f^{(d)}\)

   Algebraically independent elements will also be called *variables*

** Localization
   \(A\) a commutative ring

   By a *multiplicative subset* of \(A\) we shall mean a submonoid of \(A\)

   We shall now construct the *quotient ring of \(A\) by \(S\)*, also known as the *ring of fractions*
   *of \(A\) by \(S\)*

   We consider pairs \((a,s)\) with \(a\in A\) and \(s\in S\). We define a relation
   \begin{equation*}
   (a,s)\sim (a',s')
   \end{equation*}
   if there exists \(s_1\in S\) s.t.
   \begin{equation*}
   s_1(s'a-sa')=0
   \end{equation*}
   The equivalence class containing a pair \((a,s)\) is denoted by \(a/s\). The set of equivalence
   classes is denoted by \(S^{-1}A\)

   if \(0\in  S\), then \(S^{-1}A\) has precisely one element \(0/1\)
   \begin{gather*}
   (a/s)(a'/s')=aa'/ss'\\
   \frac{a}{s}+\frac{a'}{s'}=\frac{s'a+sa'}{ss'}
   \end{gather*}

   Let \(\varphi_S:A\to S^{-1}A\) be the s.t. \(\varphi_S(a)=a/1\). Every element of \(\varphi_S(S)\) is invertible
   in \(S^{-1}(A)\) (the inverse of \(s/1\) is \(1/s\))

   Let \(\calc\) be the category whose objects are ring homomorphism
   \begin{equation*}
   f:A\to B
   \end{equation*}
   s.t. for every \(s\in S\) the elements \(f(s)\) is invertible in \(B\). If \(f:A\to B\) and

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(A\) be an entire ring, and let \(S\) be a multiplicative subset which does not contain 0.
   Then
   \begin{equation*}
   \varphi_S:A\to S^{-1}A
   \end{equation*}
   is injective
   #+END_proposition

   Let \(A\) be an entire ring, and let \(S\) be the set of non-zero elements of \(A\). Then \(S\)
   is a multiplicative set, and \(S^{-1}A\) is then a field, called the *quotient field* or the *field
   of fractions of \(A\).

   if \(A\) is an entire ring, then so is \(A[X_1,\dots,X_n]\). if \(K\) is the quotient field of \(A\),
   the quotient field of \(A[X_1,\dots,X_n]\) is denoted by \(K(X_1,\dots,X_n)\). An element
   of \(K(X_1,\dots,X_n)\) is called a *rational function*. A rational function can be written as a
   quotient \(f(X)/g(X)\) where \(f,g\) are polynomials. If \((b_1,\dots,b_n)\) is in \(K^n\), and a
   rational function admits an expression as a quotient \(f/g\) s.t. \(g(b)\neq 0\), then we say that
   the rational function is *defined* at \(\barb\).

** Principal and Factorial Rings
   Let \(A\) be an entire ring. An element \(a\neq 0\) is called *irreducible* if it is not a unit, and
   if whenever one can write \(a=bc\) with \(b\in A\) and \(c\in A\), then \(b\) or \(c\) is a unit

   /Let \(a\neq 0\) be an element of \(A\) and assume that the principal ideal \((a)\) is prime. Then/
   \((a)\) /is irreducible/. If we write \(a=bc\)., then \(b\) or \(c\) lies in \((a)\), say \(b\).
   Then we can write \(b=ad\) with some \(d\in A\) and hence \(a=acd\). Since \(A\) is entire, it
   follows that \(cd=1\), in other words, \(c\) is a unit.

   The converse of the preceding assertion is not always true. We shall discuss under which
   conditions it is true. An element \(a\in A\), \(a\neq 0\) is said to have a
   *unique factorization into irreducible elements* if there exists a unit \(u\) and there exist
   irreducible elements \(p_i\) in \(A\) s.t.
   \begin{equation*}
   a=u\prod_{i=1}^rp_i
   \end{equation*}
   and if given two factorization into irreducible elements
   \begin{equation*}
   a=u\prod_{i=1}^rp_i=u'\prod_{j=1}^sq_j
   \end{equation*}
   we have \(r=s\) and after a permutation of the indices \(i\), we have \(p_i=u_iq_i\) for some
   unit \(u_i\in A\)

   A ring is called *factorial* (or *unique factorization ring*) if it is entire and if every
   element \(\neq 0\) has a unique factorization into irreducible elements.

   Let \(A\) be an entire ring and \(a,b\in A\), \(ab\neq 0\). We say that \(a\) *divides* \(b\) and
   write \(a\mid b\) if there exists \(c\in A\) s.t. \(ac=b\). We say that \(d\in A\), \(d\neq 0\) is a
   *greatest common divisor* (*g.c.d.*) of \(a\) and \(b\) if \(d\mid a\) and \(d\mid b\) and if any
   element \(e\) of \(A\) \(e\neq 0\) which divides both \(a\) and \(b\) also divides \(d\)

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(A\) be a principal entire ring and \(a,b\in A\), \(a,b\neq 0\). Let \((a,b)=(c)\). Then \(c\) is
   a greatest common divisor of \(a\) and \(b\)
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   Let \(A\) be a principal entire ring. Then \(A\) is factorial
   #+END_theorem

   #+BEGIN_proof
   We first prove that every non-zero element of \(A\) has a factorization into irreducible
   elements. Let \(S\) be the set of principal ideals \(\neq 0\) whose generators do not have a
   factorization into irreducible elements, and suppose \(S\) is not empty. Let \((a_1)\in S\)  be
   in \(S\). Consider an ascending chain
   \begin{equation*}
   (a_1)\subsetneq(a_2)\subsetneq\dots\subsetneq(a_n)\subsetneq\dots
   \end{equation*}
   of ideals in \(S\). We contend that such a chain cannot be infinite. Indeed, the union of such a
   chain is an ideal of \(A\), which is principal, say equal to \((a)\). The generator \(a\) must
   already lie in some element of the chain, say \((a_n)\), and then we see
   that \((a_n)\subset(a)\subset(a_n)\), whence the chain stops at \((a_n)\). Hence \(S\)  is inductively
   ordered, and has a maximal element \((a)\). Therefore any ideal of \(A\) containing \((a)\)
   and \(\neq(a)\) has a generator admitting a factorization.

   We note that \(a_n\) cannot be irreducible and hence we can write \(a=bc\) with neither \(b\)
   nor \(c\) equal to a unit. But then \((b)\neq(a)\) and \((c)\neq(a)\) and hence both \(b\) and \(c\)
   admit factorizations into irreducible elements. The product of these factorizations is a
   factorization for \(a\), contradicting the assumption that \(S\) is not empty

   To prove uniqueness, we first remark that if \(p\) is an irreducible element of \(A\)
   and \(a,b\in A\), \(p\mid ab\), then \(p\mid a\) or \(p\mid b\). /Proof/: if \(p\not\mid a\), then the g.c.d.
   of \(p,a\) is 1 and hence we can write
   \begin{equation*}
   1=xp+ya
   \end{equation*}
   for some \(x,y\in A\). Then \(b=bxp+yab\) and since \(p\mid ab\) we conclude that \(p\mid b\)

   Suppose that \(a\) has two factorizations
   \begin{equation*}
   a=p_1\dots p_r=q_1\dots q_s
   \end{equation*}
   into irreducible elements. Since \(p_1\) divides \(q_1\dots q_s\), \(p_1\) divides one of the factors,
   which we may assume to be \(q_1\) after renumbering these factors. Then there exists a unit \(u_1\)
   s.t. \(q_1=u_1p_1\). We can now cancel \(p_1\) from both factorizations and get
   \begin{equation*}
   p_2\dots p_r=u_1q_2\dots q_s
   \end{equation*}
   #+END_proof

   We could call two elements \(a,b\in A\) equivalent if there exists a unit \(u\) s.t. \(a=bu\). let
   us select irreducible element \(p\) out of each equivalence class belonging to such an
   irreducible element, and let us denote by \(P\) the set of such representatives.
   Let \(a\in A,a\neq 0\). Then there exists a unit \(u\) and integers \(v(p)\ge 0\), equal to 0 for almost
   all \(p\in P\) s.t.
   \begin{equation*}
   a=u\prod_{p\in P}p^{v(p)}
   \end{equation*}
   Furthermore, the unit \(u\) and the integers \(v(p)\) are uniquely determined by \(a\). We
   call \(v(p)\) the *order* of \(a\) at \(p\), also written as \(\ord_pa\)

   If \(A\) is a factorial ring, then an irreducible element \(p\) generates a prime ideal \((p)\).
   Thus in a factorial ring, an irreducible element will also be called a *prime element*, or simply *prime*


* Modules

** Basic Definitions
   Let \(A\) be a ring. A *left module* over \(A\), or a left \(A\)-module \(M\) is an abelian group,
   together with an operation of \(A\) on \(M\), s.t. for all \(a,b\in A\) and \(x,y\in M\)
   \begin{equation*}
   (a+b)x=ax+bx \quad\text{ and }\quad a(x+y)=ax+ay
   \end{equation*}

   Let \(A\) be an entire ring and let \(M\) be an \(A\)-module. We define the *torsion
   submodule* \(M_{tor}\) to be the subset of elements \(x\in M\) s.t. there exist\(a\in A\)s , \(a\neq 0\)
   s.t. \(ax=0\).

      By a *module homomorphism* we means a map
   \begin{equation*}
   f:M\to M'
   \end{equation*}
   which is an additive group homomorphism and s.t.
   \begin{equation*}
   f(ax)=af(x)
   \end{equation*}
   for all \(a\in A\) and \(x\in M\). If we wish to refer to the ring \(A\), we also say that \(f\) is
   an *\(A\)-homomorphism*, or also that it is an *\(A\)-linear map*

   For any module \(M\) and \(M'\), the map \(\zeta:M\to M'\) s.t.
   \(\zeta(x)=0\) for all \(x\in M\) is a homomorphism, called *zero*

   Let \(f:M\to M'\) be a homomorphism. By the *cokernel* of \(f\) we mean the factor module
   \(M'/\im f=M'/f(M)\).

   Like groups
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(N,N'\) be two submodules of a module of \(M\). Then \(N+N'\) is also a submodule, and we
   have an isomorphism
   \begin{equation*}
   N/(N\cap N')\cong(N+N')/N'
   \end{equation*}
   If \(M\supset M'\supset M''\) are modules, then
   \begin{equation*}
   (M/M'')/(M'/M'')\cong M/M'
   \end{equation*}
   If \(f:M\to M'\) is a module homomorphism, and \(N'\) is a submodule of \(M'\), then \(f^{-1}(N')\)
   is a submodule of \(M\) and we have a canonical injective homomorphism
   \begin{equation*}
   \barf:M/f^{-1}(N')\to M'/N'
   \end{equation*}
   If \(f\) is surjective, then \(\barf\) is a module isomorphism
   #+END_proposition

   A sequence of module homomorphisms
   \begin{center}\begin{tikzcd}
   M'\ar[r,"f"]&M\ar[r,"g"]&M''
   \end{tikzcd}\end{center}
   is *exact* if \(\im f=\ker g\). If \(N\) is a submodule of \(M\), then
   \begin{center}\begin{tikzcd}
   0\ar[r]&N\ar[r]&M\ar[r]&M/N\ar[r]&0
   \end{tikzcd}\end{center}

   If a homomorphism \(u:N\to M\) is s.t.
   \begin{center}\begin{tikzcd}
   0\ar[r]&N\ar[r,"u"]&M
   \end{tikzcd}\end{center}

   is exact, then we also say that \(u\) is a *monomorphism* or an *embedding*. Dually
   if
   \begin{center}\begin{tikzcd}
   N\ar[r,"u"]&M\ar[r]&0
   \end{tikzcd}\end{center}
   is exact, we say that \(u\) is an *epimorphism*

   Let \(A\) be a commutative ring. Let \(E,F\) be modules. By a *bilinear map*
   \begin{equation*}
   g:E\times E\to F
   \end{equation*}
   we mean a map s.t. given \(x\in E\) the map \(y\mapsto g(x,y)\) is \(A\)-linear and given \(y\in E\), the
   map \(x\mapsto g(x,y)\) is \(A\)-linear. By an *\(A\)-algebra* we mean a module together with a bilinear
   map \(g:E\times E\to E\) . We view such a map as a law of composition on \(E\).

** The Group of Homomorphisms
   Let \(A\) be a ring, and let \(X,X'\) be \(A\)-modules. We denote by \(\Hom_A(X',X)\) the set
   of \(A\)-homomorphisms of \(X'\) into \(X\). Then \(\Hom_A(X',X)\) is an abelian group, the law
   of addition being that of addition for mappings into an abelian group.

   If \(A\) is /commutative/ then we can make \(\Hom_A(X',X)\) into an \(A\)-module by defining \(af\)
   for \(a\in A\) and \(f\in\Hom_A(X',X)\) to be the map s.t.
   \begin{equation*}
   (af)(x)=af(x)
   \end{equation*}

   Let \(Y\) be an \(A\)-module, and let
   \begin{center}\begin{tikzcd}
   X'\ar[r,"f"]&X
   \end{tikzcd}\end{center}
   be an \(A\)-homomorphism. Then we get an induced homomorphism
   \begin{equation*}
   \Hom_A(f,Y):\Hom_{A}(X,Y)\to\Hom_A(X',Y)
   \end{equation*}
   given by \(g\mapsto g\circ f\). The fact that \(\Hom_A(f,Y)\) is a homomorphism is a rephrasing of the
   \((g_1+g_2)\circ f=g_1\circ f+g_2\circ f\)

   If we have a sequence of \(A\)-homomorphisms
   \begin{center}\begin{tikzcd}
   X'\ar[r]&X\ar[r]&X''
   \end{tikzcd}\end{center}
   then we get an induced sequence
   \begin{center}\begin{tikzcd}
   \Hom_A(X',Y)&\Hom_A(X,Y)\ar[l]&\Hom_A(X'',Y)\ar[l]
   \end{tikzcd}\end{center}

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   A sequence
   \begin{center}\begin{tikzcd}
   X'\ar[r,"\lambda"]&X\ar[r]&X''\ar[r]&0
   \end{tikzcd}\end{center}
   is exact iff the sequence
   \begin{center}\begin{tikzcd}
   \Hom_A(X',Y)&\Hom_A(X,Y)\ar[l]&\Hom_A(X'',Y)\ar[l]&0\ar[l]
   \end{tikzcd}\end{center}
   is exact for all \(Y\)

   #+END_proposition

   #+BEGIN_proof
   Suppose the first sequence is exact. If \(g:X''\to Y\) is an \(A\)-homomorphism, its image
   in \(\Hom_A(X,Y)\) is obtained by composing \(g\) with the surjective map of \(X\) on \(X''\). If
   this composition is 0, it follows that \(g=0\).  Consider a homomorphism \(g:X\to Y\) s.t. the
   composition
   \begin{center}\begin{tikzcd}
   X'\ar[r,"\lambda"]&X\ar[r,"g"]&Y
   \end{tikzcd}\end{center}
   is 0. Then \(g\) vanishes on the image of \lambda. Hence we can factor \(g\) through the factor module
   \begin{center}\begin{tikzcd}[column sep=small]
   &X/\im\lambda\ar[rd]\\
   X\ar[ur]\ar[rr,"g"']&&Y
   \end{tikzcd}\end{center}
   Since \(X\to X''\) is surjective, we have an isomorphism
   \begin{equation*}
   X/\im\lambda\cong X''
   \end{equation*}
   Hence we can factor \(g\) through \(X''\), thereby showing that the kernel of
   \begin{center}\begin{tikzcd}
   \Hom_A(X',Y)&\Hom_A(X,Y)\ar[l]
   \end{tikzcd}\end{center}
   is contained in the image of
   \begin{center}\begin{tikzcd}
   \Hom_A(X,Y)&\Hom_A(X'',Y)\ar[l]
   \end{tikzcd}\end{center}
   #+END_proof

   similarly, we have
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   A sequence
   \begin{center}\begin{tikzcd}
   0\ar[r]&Y'\ar[r]&Y\ar[r]&Y''
   \end{tikzcd}\end{center}
   is exact iff
   \begin{center}\begin{tikzcd}
   0\ar[r]&\Hom_A(X,Y')\ar[r]&\Hom_A(X,Y)\ar[r]&\Hom_A(X,Y'')
   \end{tikzcd}\end{center}
   is exact for all \(X\)

   #+END_proposition

   Let \(\Mod(A)\) and \(\Mod(B)\) be the categories of modules over rings \(A\) and \(B\), and
   let \(F:\Mod(A)\to\Mod(B)\) be a functor. One says that \(F\) is *exact* if \(F\) transforms exact
   sequences into exact sequences.

   let \(M\) be an \(A\)-module. From the relations
   \begin{align*}
   &(g_1+g_2)\circ f=g_1\circ f+g_2\circ f\\
   &g\circ(f_1+f_2)=g\circ f_1+g\circ f_2
   \end{align*}
   and the fact that there is an identity for composition, namely \(id_M\), we conclude
   that \(\Hom_A(M,M)\) is a ring. We call \(\End_A(M)=\Hom_A(M,M)\) the ring of *endomorphisms*

** Direct Products and Sums of Modules
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(M\) be an \(A\)-module and \(n\) an integer \(\ge 1\). For each \(i=1,\dots,n\) let \(\varphi_i:M\to M\)
   be an \(A\)-homomorphism s.t.
   \begin{equation*}
   \sum_{i=1}^n\varphi_i=\id \quad\text{ and }\quad\varphi_i\circ\varphi_j=0\quad \text{ if }i\neq j
   \end{equation*}
   Then \(\varphi_i^2=\varphi_i\) for all \(i\). Let \(M_i=\varphi_i(M)\) , and let \(\varphi:M\to\prod M_i\) be s.t.
   \begin{equation*}
   \varphi(x)=(\varphi_1(x),\dots,\varphi_n(x))
   \end{equation*}
   Then \varphi is an \(A\)-isomorphism of \(M\) onto the direct product \(\prod M_i\)
   #+END_proposition

   #+BEGIN_proof
   for each \(j\), we have
   \begin{equation*}
   \varphi_j=\varphi_j\circ\id=\varphi_j\circ\sum_{i=1}^n\varphi_i=\varphi_j\circ\varphi_j=\varphi_j^2
   \end{equation*}
   thereby proving the first assertion. It is clear that \varphi is an \(A\)-homomorphism.
   Let \(x\in\ker\varphi\). Since
   \begin{equation*}
   x=\id(x)=\sum_{i=1}^n\varphi_i(x)
   \end{equation*}
   we conclude that \(x=0\), so \varphi is injective.
   #+END_proof

   Let \(M\) be a module over a ring \(A\) and let \(S\) be a subset of \(M\). By a *linear
   combination* of elements of \(S\) (with coefficients in \(A\)) one means a sum
   \begin{equation*}
   \sum_{x\in S}a_xx
   \end{equation*}
   where \(\{a_x\}\) is a set of elements of \(A\), almost all of which are equal to 0. Let \(N\) be the
   set of all linear combinations of elements of \(S\). Then \(N\) is a submodule of \(M\), for if
   \begin{equation*}
   \sum_{x\in S}a_xx \quad\text{ and }\quad\sum_{x\in S}b_xx
   \end{equation*}
   are two linear combinations, then their sum is equal to
   \begin{equation*}
   \sum_{x\in S}(a_x+b_x)x
   \end{equation*}
   and if \(c\in A\), then
   \begin{equation*}
   c\left( \sum_{x\in S}a_xx \right)=\sum_{x\in S}ca_xx
   \end{equation*}
   We shall call \(N\) the submodule *generated* by \(S\), and we call \(S\) a set of *generators*
   for \(N\). We sometimes write \(N=A\la S\ra\). If \(S\) consists of one element \(x\), the module
   generated by \(x\) is also written \(Ax\), or simply \((x)\), and sometimes we say that \((x)\)
   is a *principal module*

   A module \(M\) is said to be *finitely generated*, or of *finite type* or *finite* over \(A\), if it
   has a finite number of generators

   A subset \(S\) of a module \(M\) is said to be *linearly independent* (over \(A\)) if whenever we
   have a linear combination
   \begin{equation*}
   \sum_{x\in S}a_xx
   \end{equation*}
   which is equal to 0, then \(a_x=0\) for all \(x\in S\). If \(S\) is linearly independent and if two
   linear combinations
   \begin{equation*}
   \sum a_xx \quad\text{ and }\quad\sum b_xx
   \end{equation*}
   are equal, then \(a_x=b_x\) for all \(x\in S\).

   Let \(M\) be an \(A\)-module, and let \(\{M_i\}_{i\in I}\) be a family of submodules. Since we have
   inclusion-homomorphism
   \begin{equation*}
   \lambda_i:M_i\to M
   \end{equation*}
   we have an induced homomorphism
   \begin{equation*}
   \lambda_*:\bigoplus M_i\to M
   \end{equation*}
   which is s.t. for any family of elements \((x_i)_{i\in I}\) all but a finite number of which are 0,
   we have
   \begin{equation*}
   \lambda_*((x_i))=\sum_{i\in I}x_i
   \end{equation*}
   if \(\lambda_*\) is an isomorphism, then we say that \(\{M_i\}_{i\in I}\) is a *direct sum decomposition*
   of \(M\). This is equivalent to saying that every element of \(M\) has a unique expression as a
   sum
   \begin{equation*}
   \sum x_i
   \end{equation*}
   with \(x_i\in M\) and almost all \(x_i=0\). By abuse of notation, we also write
   \begin{equation*}
   M=\bigoplus M_i
   \end{equation*}
   in this case

   If \(M\) is a module and \(N,N'\) are two submodules s.t. \(N+N'=M\) and \(N\cap N'=0\), then we
   have a module isomorphism
   \begin{equation*}
   M\cong N\oplus N'
   \end{equation*}
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(M,M',N\) be modules. Then we have an isomorphism of abelian groups
   \begin{equation*}
   \Hom_A(M\oplus M',N)\cong\Hom_A(M,N)\times\Hom_A(M',N)
   \end{equation*}
   and
   \begin{equation*}
   \Hom_A(N,M\times M')\cong\Hom_A(N,M)\times\Hom_A(N,M')
   \end{equation*}
   #+END_proposition

   #+BEGIN_proof
   if \(f:M\oplus M'\to N\) is a homomorphism, then \(f\) induces a homomorphism \(f_1:M\to N\) and a
   homomorphism \(f_2:M'\to N\) by composing injections
   \begin{align*}
   &M\to M\oplus\{0\}\subset M\oplus M'\xrightarrow{f}N\\
   &M'\to\{0\}\oplus M'\subset M\oplus M'\xrightarrow{f}N
   \end{align*}
   Then
   \begin{equation*}
   f\mapsto(f_1,f_2)
   \end{equation*}
   is an isomorphism
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(0\to M'\xrightarrow{f}M\xrightarrow{g}M''\to 0\) be an exact sequence of modules. The following are equivalent
   1. there exists a homomorphism \(\varphi:M''\to M\) s.t. \(g\circ\varphi=\id\)
   2. there exists a homomorphism \(\psi:M\to M'\) s.t. \(\psi\circ f=\id\)


   if these conditions are satisfied, then we have isomorphisms
   \begin{gather*}
   M=\im f\oplus\ker\psi,\hspace{1cm}M=\ker g\oplus\im\varphi\\
   M\cong M'\oplus M''
   \end{gather*}
   #+END_proposition

   #+BEGIN_proof
   Let \(x\in M\), then \(x-\varphi(g(x))\in\ker g\), and hence \(M=\ker g+\im\varphi\). If \(x\in\ker g\cap\im\varphi\),
   then \(x=\varphi(w)\) and \(g(x)=g(\varphi(w))=w=0\), thus \(\ker g\cap\im\varphi=\{0\}\)

   <<Problem3>>
   #+END_proof

   when these conditions are satisfied, the exact sequence is said to *split*. \psi *splits* \(f\) and
   \varphi *splits* \(g\)

   Consider first a category \(\fC\) s.t. \(\Mor(E,F)\) is an abelian group for each pair of
   objects \(E,F\) of \(\fC\), satisfying the following two conditions
   | AB 1 | The law of composition of morphisms is bilinear, and there exists          |
   |      | a zero object 0, i.e., s.t. \(\Mor(0,E)\) and \(\Mor(E,0)\) have precisely |
   |      | one element for each object \(E\)                                          |
   | AB 2 | Finite products and finite coproducts exists in the category               |

   Then we say that \(\fC\) is an *additive category*

   Given a morphism \(E\xrightarrow{f}F\) in \(\fC\), we define a *kernel* of \(f\) to be a morphism \(E'\to E\) s.t.
   for all objects \(X\) in the category, the following sequence is exact
   \begin{center}\begin{tikzcd}
   0\ar[r]&\Mor(X,E')\ar[r]&\Mor(X,E)\ar[r]&\Mor(X,F)
   \end{tikzcd}\end{center}
   we define a *cokernel* for \(f\) to be a morphism \(F\to F''\) s.t. for all objects \(X\) in the
   category, the following sequence is exact
   \begin{center}\begin{tikzcd}
   0\ar[r]&\Mor(F'',X)\ar[r]&\Mor(F,X)\ar[r]&\Mor(E,X)
   \end{tikzcd}\end{center}
   | AB 3 | Kernels and cokernels exist                                             |
   | AB 4 | If \(f:E\to F\) is a morphism whose kernel is 0, then \(f\) is the kernel |
   |      | of its cokernel. If \(f:E\to F\) is a morphism whose cokernel is 0,       |
   |      | then \(f\) is the cokernel of its kernel. A morphism whose kernel       |
   |      | and cokernel are 0 is an isomorphism                                            |


   A category \(\fC\) satisfying the above four axioms is called an *abelian category*

   In an abelian category, the group of morphisms is usually denote by Hom, so
   \begin{equation*}
   \Mor(E,F)=\Hom(E,F)
   \end{equation*}
   The morphisms are usually called *homomorphisms*. Given an exact sequence
   \begin{center}\begin{tikzcd}
   0\ar[r]&M'\ar[r]&M
   \end{tikzcd}\end{center}
   we say that \(M'\) is a *subobject* of \(M\), or that the homomorphism of \(M'\) into \(M\) is a
   *monomorphism*. Dually, in an exact sequence
   \begin{center}\begin{tikzcd}
   M\ar[r]&M''\ar[r]&0
   \end{tikzcd}\end{center}
   we say that \(M''\) is a *quotient object* of \(M\), or that the homomorphism of \(M\) to \(M''\)
   is an *epimorphism*

** Free Modules
   Let \(M\) be a module over a ring \(A\) and let \(S\) be a subset of \(M\). \(S\) is a *basis*
   of \(M\) if \(S\) is not empty, if \(S\) generates \(M\), and if \(S\) is linearly independent.
   If \(S\) is a basis of \(M\), then in particular \(M\neq\{0\}\) if \(A\neq\{0\}\) and every element
   of \(M\) has a unique expression as a linear combination of elements of \(S\)

   If \(A\) is a ring, then as a module over itself, \(A\) admits a basis, consisting of the unit
   element 1.

   Let \(I\) be a non-empty set, and for each \(i\in I\), let \(A_i=A\), viewed as an \(A\)-module. Let
   \begin{equation*}
   F=\bigoplus_{i\in I}A_i
   \end{equation*}
   then \(F\) admits a basis, which consists of the elements \(e_i\) of \(F\) whose \(i\)-th
   component is the unit element of \(A_i\), and having all other components equal to 0

   By a *free* module we mean a module which admits a basis, or the zero module

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   Let \(A\) be a ring and \(M\) a module over \(A\). Let \(I\) be a non-empty set, and
   let \(\{x_i\}_{i\in I}\) be a basis of \(M\). Let \(N\) be an \(A\)-module, and let \(\{y_i\}_{i\in I}\) be
   a family of elements of \(N\). Then there exists a unique homomorphism \(f:M\to N\)
   s.t. \(f(x_i)=y_i\) for all \(i\).
   #+END_theorem

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Let the notation be as in the theorem, and assume that \(\{y_i\}_{i\in I}\) is a basis of \(N\). Then
   the homomorphism \(f\) is an isomorphism
   #+END_corollary

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Two modules having bases whose cadinalities are equal are isomorphic
   #+END_corollary

   Let \(M\) be a free module over \(A\), with basis \(\{x_i\}_{i\in I}\), so that
   \begin{equation*}
   M=\bigoplus_{i\in I}Ax_i
   \end{equation*}
   Let \(\fa\) be a two sided ideal of \(A\). Then \(\fa M\) is a submodule of \(M\). Each \(\fa x_i\) is a
   submodule of \(Ax_i\). We /have an isomorphism/
      \begin{equation*}
   M/\fa M\cong\bigoplus_{i\in I}Ax_i/\fa x_i
      \end{equation*}

    A module \(M\) is called *principal* if there exists an element \(x\in M\) s.t. \(M=Ax\). The map
    \begin{equation*}
   a\mapsto ax
    \end{equation*}
    is an \(A\)-module homomorphism of \(A\) onto \(M\), whose kernel is a left ideal \(\fa\).

** Vector Spaces
   A module over a field is called a *vector space*

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   Let \(V\) be a vector space over a field \(K\), and assume that \(V\neq\{0\}\). Let \Gamma be a set of
   generators of \(V\) over \(K\) and let \(S\) be a subset of \Gamma which is linearly independent. Then
   there exists a basis \(\fB\) of \(V\) s.t. \(S\subset\fB\subset\Gamma\).
   #+END_theorem

   #+BEGIN_proof
   Zorn's lemma
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   Let \(V\) be a vector space over a field \(K\). Then two bases of \(V\) over \(K\) have the same cardinality
   #+END_theorem

   #+BEGIN_proof
   First assume that there exists a basis of \(V\) with a finite number of elements,
   say \(\{v_1,\dots,v_m\}\), \(m\ge 1\). It is suffice to prove: if \(w_1,\dots,w_n\) are elements of \(V\) which
   are linearly independent over \(K\), then \(n\le m\) (for then we can use symmetry). We proceed by
   induction. There exist elements \(c_1,\dots,c_m\) of \(K\) s.t.
   \begin{equation*}
   w_1=c_1v_1+\dots+c_mv_m
   \end{equation*}
   and some \(c_i\), say \(c_1\) is not equal to 0. Then \(v_1\) lies in the space generated
   by \(w_1,v_2,\dots,v_m\) over \(K\), and this space must therefore be equal to \(V\) itself.
   Furthermore, \(w_1,v_2,\dots,v_m\) are linearly independent, for suppose \(b_1,\dots,b_m\) are elements
   of \(K\) s.t.
   \begin{equation*}
   b_1w_1+\dots+b_mv_m=0
   \end{equation*}
   if \(b_1\neq 0\), divide by \(b_1\) and express \(w_1\) as a linear combination of \(v_2,\dots,v_m\), would
   yield a relation of linear dependence among the \(v_i\). Hence \(b_1=0\), and again we must have
   all \(b_i=0\)

   Suppose inductively that after a suitable renumbering of the \(v_i\), we have found \(w_1,\dots,w_r\)
   (\(r<n\)) s.t.
   \begin{equation*}
   \{w_1,\dots,w_r,v_{r+1},\dots,v_m\}
   \end{equation*}
   is a basis of \(V\).
   \begin{equation*}
   w_{r+1}=c_1w_1+\dots+c_rw_r+c_{r+1}w_{r+1}+\dots+c_mv_m
   \end{equation*}
   with \(c_i\in K\). Similarly we still can replace \(v_{r+1}\) by \(w_{r+1}\).
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   Let \(V\) be a vector space over a field \(K\), and let \(W\) be a subspace. Then
   \begin{equation*}
   \dim_KV=\dim_KW+\dim_KV/W
   \end{equation*}
   If \(f:V\to U\) is a homomorphism of vector spaces over \(K\), then
   \begin{equation*}
   \dim V=\dim\ker f+\dim\im f
   \end{equation*}
   #+END_theorem

   #+BEGIN_proof
   The first statement is a special case of the second, taking for \(f\) the canonical map.
   Let \(\{u_i\}_{i\in I}\) be a basis of \(\im f\) and \(\{w_i\}_{i\in J}\) a basis of \(\ker f\).
   Let \(\{v_i\}_{i\in I}\) be a family of \(V\) s.t. \(f(v_i)=u_i\) for each \(i\in I\). We contend that
   \begin{equation*}
   \{v_i,w_j\}_{i\in I,j\in J}
   \end{equation*}
   is a basis for \(V\)

   Let \(x\in V\). Then there exist elements \(\{a_i\}_{i\in I}\) of \(K\) almost all of which are 0 s.t.
   \begin{equation*}
   f(x)=\sum_{i\in I}a_iu_i
   \end{equation*}
   Hence \(f(x-\sum a_iv_i)=0\). Thus
   \begin{equation*}
   x-\sum a_iv_i\in\ker f
   \end{equation*}
   thus there exists elements \(\{b_j\}_{j\in J}\) of \(K\) almost all of which are 0 s.t.
   \begin{equation*}
   x-\sum a_iv_i=\sum b_jw_j
   \end{equation*}
   From this we see that \(x=\sum a_iv_i+\sum b_jw_j\), and that \(\{v_i,w_j\}\) generated \(V\). It remains to
   show that the family is linearly independent. Suppose that there exists elements \(c_i,d_j\) s.t.
   \begin{equation*}
   0=\sum c_iv_i+\sum d_jw_j
   \end{equation*}
   applying \(f\) yields
   \begin{equation*}
   0=\sum c_if(v_i)=\sum c_iu_i
   \end{equation*}
   whence all \(c_i=0\).From this we conclude that all \(d_j=0\)
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Let \(V\) be a vector space and \(W\) a subspace. Then
   \begin{equation*}
   \dim W\le\dim V
   \end{equation*}
   If \(V\) is finite dimensional and \(\dim W=\dim V\) then \(W=V\)
   #+END_corollary


* Polynomials
** Basic Properties for Polynomials in One Variable
   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   Let \(A\) be a commutative ring, let \(f,g\in A[X]\)  be polynomials in one variable, of
   degree \(\ge 0\), and assume that the leading coefficient of \(g\) is a unit in \(A\). Then there
   exist unique polynomials \(q,r\in A[X]\) s.t.
   \begin{equation*}
   f=gq+r
   \end{equation*}
   and \(\deg r<\deg g\)
   #+END_theorem

   #+BEGIN_proof
   Write
   \begin{align*}
   &f(X)=a_nX^n+\dots+a_0\\
   &g(X)=b_dX^d+\dots+b_0
   \end{align*}
   where \(n=\deg f\), \(d=\deg g\) so that \(a_n,b_d\neq 0\) and \(b_d\) is a unit in \(A\). We use
   induction on \(n\)

   if \(n=0\) and \(\deg g>\deg f\), we let \(q=0\), \(r=f\). If \(\deg g=\deg f=0\), then
   let \(r=0\) and \(q=a_nb_d^{-1}\)

   Assume the theorem proved for polynomials of degree \(<n\). We may assume \(\deg g\le\deg f\)
   (otherwise take \(q=0\) and \(r=f\)). Then
   \begin{equation*}
   f(X)=a_nb^{-1}_dX^{n-d}g(X)+f_1(X)
   \end{equation*}
   where \(f_1(X)\) has degree \(<n\). By induction, we can find \(q_1,r\) s.t.
   \begin{equation*}
   f(X)=a_nb_d^{-1}X^{n-d}g(X)+q_1(X)g(X)+r(X)
   \end{equation*}
   and \(\deg r<\deg g\). Then we let
   \begin{equation*}
   q(X)=a_nb_d^{-1}X^{n-d}+q_1(X)
   \end{equation*}

   For uniqueness, suppose that
   \begin{equation*}
   f=q_1g+r_1=q_2g+r_2
   \end{equation*}
   with \(\deg r_1<\deg g\) and \(\deg r_2<\deg g\). Subtracting yields
   \begin{equation*}
   (q_1-q_2)g=r_2-r_1
   \end{equation*}
   Since the leading coefficient of \(g\) is assumed to be a unit, we have
   \begin{equation*}
   \deg(q_1-q_2)g=\deg(q_1-q_2)+\deg g
   \end{equation*}
   Since \(\deg(r_2-r_1)<\deg g\), this relation can hold only if \(q_1-q_2=0\). Hence \(r_1=r_2\)
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   Let \(k\) be a field. Then the polynomial ring in one variable \(k[X]\) is principal
   #+END_theorem

   #+BEGIN_proof
   Let \(\fa\) be an ideal of \(k[X]\) and assume \(\fa\neq 0\). Let \(g\) be an element of \(\fa\) of
   smallest degree \(\ge 0\). Let \(f\) be an element of \(\fa\) s.t. \(f\neq 0\). By the Euclidean
   algorithm we can find \(q,r\in k[X]\) s.t.
   \begin{equation*}
   f=qg+r
   \end{equation*}
   and \(\deg r<\deg g\). But \(r=f-qg\) whence \(r\in\fa\). It follows that \(r=0\), hence that \(\fa\)
   consists of all polynomials \(qg\).
   #+END_proof

   A polynomial \(f(X)\in k[X]\) is called *irreducible* if it has degree \(\ge 1\), and if one cannot
   write \(f(X)\) as a product
   \begin{equation*}
   f(X)=g(X)h(X)
   \end{equation*}
   with \(g,h\in k[X]\) and both \(g,h\not\in k\). Elements of \(k\) are usually called *constant
   polynomials*. A polynomial is called *monic* if it has leading coefficient 1

   Let \(A\) be a commutative ring and \(f(X)\) a polynomial in \(A[X]\). Let \(A\) be
   a subring of \(B\). An element \(b\in B\) is called a *root* or a *zero* of \(f\) in \(B\)
   if \(f(b)=0\).

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   Let \(k\) be a field and \(f\) a polynomial in one variable \(X\) in \(k[X]\) of degree \(n\ge 0\).
   Then \(f\) has at most \(n\) roots in \(k\) and if \(a\) is a root of \(f\) in \(k\),
   then \(X-a\) divides \(f(X)\)
   #+END_theorem

   #+BEGIN_proof
   Suppose \(f(a)=0\). Find \(q,r\) s.t.
   \begin{equation*}
   f(X)=q(X)(X-a)+r(X)
   \end{equation*}
   and \(\deg r<1\). Then
   \begin{equation*}
   0=f(a)=r(a)
   \end{equation*}
   Since \(r=0\) or \(r\) is a non-zero constant, we must have \(r=0\), whence \(X-a\)
   divides \(f(X)\).
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Let \(k\) be a field and \(T\) an infinite subset of \(k\). Let \(f(X)\in k[X]\) be a polynomial in
   one variable. If \(f(a)=0\) for all \(a\in T\), then \(f=0\)
   #+END_corollary

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Let \(k\) be a field, and let \(S_1,\dots,S_n\) be infinite subsets of \(k\). Let \(f(X_1,\dots,X_n)\) be a
   polynomial in \(n\) variables over \(k\). If \(f(a_1,\dots,a_n)=0\) for all \(a_i\in S_i\) (\(i=1,\dots,n\)),
   then \(f=0\)
   #+END_corollary

   #+BEGIN_proof
   By induction. Let \(n\ge 2\) and write
   \begin{equation*}
   f(X_1,\dots,X_n)=\sum_jf_i(X_1,\dots,X_{n-1})X^j_n
   \end{equation*}
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Let \(k\) be an infinite field and \(f\) a polynomial in \(n\) variables over \(k\) . If \(f\)
   induces the zero function on \(k^{(n)}\), then \(f=0\)
   #+END_corollary

   Let \(k\) be a finite field with \(q\) elements. Let \(f(X_1,\dots,X_n)\) be a polynomial in \(n\)
   variables over \(k\). Write
   \begin{equation*}
   f(X_1,\dots,X_n)=\sum a_{\barv}X_1^{v_1}\dots X_n^{v_n}
   \end{equation*}
   If \(a_{\barv}\neq 0\)  we recall that the monomial \(M_{\barv}(X)\) *occurs* in \(f\). Suppose this
   is the case, and that in this monomial \(M_{\barv}(X)\) some variable \(X_i\) occurs with an
   exponent \(v_i\ge q\). We can write
   \begin{equation*}
   X_i^{v_i}=X_i^{q+\mu}
   \end{equation*}
   If we replace \(X_i^{v_i}\) by \(X_i^{\mu+1}\) in this monomial, then we obtain a new polynomial which
   gives rise to the same function as \(f\). The degree of this new polynomial is at most equal to
   the degree of \(f\)

   Performing the above operation a finite number of times, for all the monomials occuring in \(f\)
   and all the variables \(X_1,\dots,X_n\) we obtain some polynomial \(f^*\) giving rise to the same
   function as \(f\), but whose degree in each variable is \(<q\)

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Let \(k\) be a finite field with \(q\) elements. Let \(f\) be a polynomial in \(n\) variables
   over \(k\) s.t. the degree of \(f\) in each variable is \(<q\). If \(f\) induces the zero
   function on \(k^n\), then \(f=0\)
   #+END_corollary

   Let \(f\) be a polynomial in \(n\) variables over the finite field \(k\). A polynomial \(g\)
   whose degree in each variable is \(<q\) will be said to be *reduced*. There exists a unique reduced
   polynomial \(f^*\) which gives the same function as \(f\) on \(k^n\)

   Let \(k\) be a field. By a *multiplicative subgroup* of \(k\) we shall mean a subgroup of the
   group \(k^*\) (non-zero elements of \(k\))

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   label:thm1.4.1.9
   Let \(k\) be a field and let \(U\) be a finite multiplicative subgroup of \(k\). Then \(U\) is cyclic
   #+END_theorem

   #+BEGIN_proof
   ref:prop1.4.3
   Write \(U\) as a product of subgroups \(U(p)\) for each prime \(p\), where \(U(p)\) is a \(p\)-group.
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   If \(k\) is a finite field, then \(k^*\) is cyclic
   #+END_corollary

   An element \zeta in a field \(k\) s.t. there exists an integer \(n\ge 1\) s.t. \(\zeta^n=1\) is called a
   *root of unity*, or \(n\)-th root of unity. Thus the set of \(n\)-th roots of unity is the set of
   roots of the polynomial \(X^n-1\). There are at most \(n\) such roots, and they form a group,
   which is cyclic by Theorem ref:thm1.4.1.9

   The group of roots of unity is denoted by \(\mbfmu\). The group of roots of unity in a field \(K\)
   is denoted by \(\bmu(K)\)


   A field \(k\) is said to be *algebraically closed* if every polynomial in \(k[X]\) of
   degree \(\ge 1\) has a root in \(k\). If \(k\) is algebraically closed then the irreducible
   polynomials in \(k[X]\) are the polynomials of degree 1. In such a case, the unique factorization
   of a polynomial \(f\) of degree \(\ge 0\)  can be written in the form
   \begin{equation*}
   f(X)=c\prod_{i=1}^r(X-\alpha_i)^{m_i}
   \end{equation*}

   Let \(A\) be a commutative ring. We define a map
   \begin{equation*}
   D:A[X]\to A[X]
   \end{equation*}
   if \(f(X)=a_nX^n+\dots+a_0\) with \(a_i\in A\), we define the *derivative*
   \begin{equation*}
   Df(X)=f'(X)=\sum_{v=1}^nva_vX^{v-1}
   \end{equation*}
   Let \(K\) be a field and \(f\) a non-zero polynomial in \(K[X]\). Let \(a\) be a root of \(f\)
   in \(K\). We can write
   \begin{equation*}
   f(X)=(X-a)^mg(X)
   \end{equation*}
   with some polynomial \(g(X)\) relatively prime to \(X-a\). We call \(m\) the *multiplicity*
   of \(a\) in \(f\), and say that \(a\) is a *multiple root* if \(m>1\)

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(K,f\) be as above. The element \(a\) of \(K\) is a multiple root of \(f\) iff it is a root
   and \(f'(a)=0\)
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(f\in K[X]\). If \(K\) has characteristic 0, and \(f\) has degree \(\ge 1\), then \(f'\neq 0\).
   Let \(K\) have characteristic \(p>0\) and \(f\) have degree \(\ge 1\). Then \(f'=0\) iff in the
   expression for \(f(X)\) given by
   \begin{equation*}
   f(X)=\sum_{v=1}^na_vX^v
   \end{equation*}
   \(p\) divides each integer \(v\) s.t. \(a_v\neq 0\)
   #+END_proposition

   Since the binomial coefficients \(\binom{p}{v}\) are divisible by \(p\) for \(1\le v\le p-1\) we see
   that if \(K\) has characteristic \(p\), then for \(a,b\in K\) we have
   \begin{equation*}
   (a+b)^p=a^p+b^p
   \end{equation*}
   Since obviously \((ab)^p=a^pb^p\) the map
   \begin{equation*}
   x\mapsto x^p
   \end{equation*}
   is a homomorphism of \(K\) into itself, which has trivial kernel, hence is injective. Iterating,
   we conclude that for each integer \(r\ge 1\), the map \(x\mapsto x^{p^r}\) is an endomorphism of \(K\),
   called the *Frobenius endomorphism*.
** Polynomials Over a Factorial Ring

* Algebraic Extensions
** Finite and Algebraic Extensions
   Let \(F\) be a field. If \(F\) is a subfield of a field \(E\), then we also say that \(E\) is an
   *extension field* of \(F\). We may view \(E\) as a vector space over \(F\), and we say \(E\) is
   *finite* or *infinite* extension of \(F\) according as the dimension of this vector space is finite
   or infinite.

   Let \(F\) be a subfield of a field \(E\). An element \alpha of \(E\) is said to be *algebraic*
   over \(F\) if there exists elements \(a_0,\dots,a_n\in F\), not all equal to 0, s.t.
   \begin{equation*}
   a_0+a_1\alpha+\dots+a_n\alpha^n=0
   \end{equation*}
   If \(\alpha\neq 0\), and \alpha is algebraic, then we can always find elements \(a_i\) as above s.t. \(a_0\neq 0\)

   Let \(X\) be a variable over \(F\). We can also say that \alpha is algebraic over \(F\) if the
   homomorphism
   \begin{equation*}
   F[X]\to E
   \end{equation*}
   which is the identity on \(F\) and maps \(X\) on \alpha has a non-zero kernel. In that case the kernel
   is an ideal which is principal, generated by a single polynomial \(p(X)\), which we may assume
   has leading coefficient 1. We then have an isomorphism
   \begin{equation*}
   F[X]/(p(X))\cong F[\alpha]
   \end{equation*}
   and since \(F[\alpha]\) is entire, it follows that \(p(X)\) is irreducible. <<Problem4>> Having
   normalized \(p(X)\) so that its leading coefficient is 1, we see that \(p(X)\) is uniquely
   determined by \alpha and will be called the *irreducible polynomial of \alpha over \(F\)*, denoted by \(\irr(\alpha,F,X)\)

   An extension \(E\) of \(F\) is said to be *algebraic* if every element of \(E\) is algebraic
   over \(F\)

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop5.1.1
   Let \(E\) be a finite extension of \(F\). Then \(E\) is algebraic over \(F\)
   #+END_proposition

   #+BEGIN_proof
   Let \(\alpha\in E,\alpha\neq 0\). The powers of \alpha
   \begin{equation*}
   1,\alpha,\alpha^2,\dots,\alpha^n
   \end{equation*}
   cannot be linearly independent over \(F\) for all positive integers \(n\), otherwise the
   dimension of \(E\) over \(F\) would be infinite. A linear relation between these powers shows
   that \alpha is algebraic over \(F\).
   #+END_proof

   If \(E\) is an extension of \(F\), we denote by
   \begin{equation*}
   [E:F]
   \end{equation*}
   the dimension of \(E\) as a vector space over \(F\).

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(k\) be a field and \(F\subset E\) extension fields of \(k\). Then
   \begin{equation*}
   [E:k]=[E:F][F:k]
   \end{equation*}
   if \(\{x_i\}_{i\in I}\) is a basis for \(F\) over \(k\) and \(\{y_j\}_{j\in J}\) is a basis for \(E\)
   over \(F\), then \(\{x_iy_j\}_{(i,j)\in I\times J}\) is a basis for \(E\) over \(k\)
   #+END_proposition

   #+BEGIN_proof
   Let \(z\in E\). By hypothesis there exist elements \(\alpha_j\in F\), almost all \(\alpha_j=0\), s.t.
   \begin{equation*}
   z=\sum_{j\in J}\alpha_jy_j
   \end{equation*}
   For each \(j\in J\) there exists elements \(b_{ji}\in k\), almost all of which are equal to 0, s.t.
   \begin{equation*}
   \alpha_j=\sum_{i\in I}b_{ji}x_i
   \end{equation*}
   and hence
   \begin{equation*}
   z=\sum_j\sum_ib_{ji}x_iy_j
   \end{equation*}
   This shows that \(\{x_iy_j\}\) is a family of generators for \(E\) over \(k\). We must show that it
   is linearly independent. Let \(\{c_{ij}\}\) be a family of elements of \(k\), almost all of which
   are 0, s.t.
   \begin{equation*}
   \sum_j\sum_ic_{ij}x_iy_j=0
   \end{equation*}
   Then for each \(j\)
   \begin{equation*}
   \sum_ic_{ij}x_i=0
   \end{equation*}
   since the elements \(y_j\) are linearly independent over \(F\). Hence \(c_{ij}=0\)
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   label:cor5.1.3
   The extension \(E\) of \(k\) is finite iff \(E\) is finite over \(F\) and \(F\) is finite over \(k\)
   #+END_corollary

   A *tower* of fields is a sequence
   \begin{equation*}
   F_1\subset F_2\subset\dots\subset F_n
   \end{equation*}
   of extension fields. The tower is called *finite* iff each step is finite

   Let \(k\) be a field, \(E\) an extension field, and \(\alpha\in E\). We denote by \(k(\alpha)\) the smallest
   subfield of \(E\) containing both \(k\) and \alpha. It consists of all quotients \(f(\alpha)/g(\alpha)\)
   where \(f,g\) are polynomials with coefficients in \(k\) and \(g(\alpha)\neq 0\).

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop5.1.4
   Let \alpha be algebraic over \(k\). Then \(k(\alpha)=k[\alpha]\), and \(k(\alpha)\) is finite over \(k\). The
   degree \([k(\alpha):k]\) is equal to the degree of \(\irr(\alpha,k,X)\)
   #+END_proposition

   Let \(E,F\) be extensions of a field \(k\). If \(E\) and \(F\) are contained in some field \(L\)
   then we denote by \(EF\) the smallest subfield of \(L\) containing both \(E\) and \(F\), and call
   it the *compositum* of \(E\) and \(F\), in \(L\).

   Let \(k\) be a subfield of \(E\) and let \(\alpha_1,\dots,\alpha_n\in E\). We denote by
   \begin{equation*}
   k(\alpha_1,\dots,\alpha_n)
   \end{equation*}
   the smallest subfield of \(E\) containing \(k\) and \(\alpha_1,\dots,\alpha_n\). Its elements consist of all
   quotients
   \begin{equation*}
   \frac{f(\alpha_1,\dots,f_n)}{g(\alpha_1,\dots,\alpha_n)}
   \end{equation*}
   where \(f,g\) are polynomials in \(n\) variables with coefficients in \(k\), and
   \begin{equation*}
   g(\alpha_1,\dots,\alpha_n)\neq 0
   \end{equation*}

   We observe that \(E\) is the union of all its subfields \(k(\alpha_1,\dots,\alpha_n)\) as \((\alpha_1,\dots,\alpha_n)\) ranges
   over finite subfamilies of elements of \(E\). We could define the *compositum of an arbitrary*
   *subfamily of subfields of a field \(L\)* as the smallest subfield containing all fields in the
   family. We say that \(E\) is *finitely generated* over \(k\) if there is a finite family of
   elements \(\alpha_1,\dots,\alpha_n\) of \(E\) s.t.
   \begin{equation*}
   E=k(\alpha_1,\dots,\alpha_n)
   \end{equation*}

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(E\) be a finite extension of \(k\). Then \(E\) is finitely generated
   #+END_proposition

   #+BEGIN_proof
   Let \(\{\alpha_1,\dots,\alpha_n\}\) be a basis of \(E\) as vector space over \(k\). Then certainly
   \begin{equation*}
   E=k(\alpha_1,\dots,\alpha_n)
   \end{equation*}
   #+END_proof

   If \(E=k(\alpha_1,\dots,\alpha_n)\) is finitely generated, and \(F\) is an extension of \(k\), both \(F,E\)
   contained in \(L\), then
   \begin{equation*}
   EF=F(\alpha_1,\dots,\alpha_n)
   \end{equation*}
   and \(EF\) is finitely generated over \(F\)
   \begin{center}\begin{tikzcd}
   &EF&\\
   &&F\ar[ul,dash]\ar[ddl,dash]\\
   E\ar[uur,dash]\ar[dr,dash]&&\\
   &k
   \end{tikzcd}\end{center}

   Lines slanting up indicate an inclusion relation between fields. We also call the
   extension \(EF\) of \(F\) the *translation* of \(E\) to \(F\), or also the *lifting* of \(E\)
   to \(F\)

   Let \alpha be algebraic over the field \(k\). Let \(F\) be an extension of \(k\), and
   assume \(k(\alpha)\), \(F\) both contained in some field \(L\). Then \alpha is algebraic over \(F\).
   Consider the irreducible polynomial for \alpha.

   Suppose that we have a tower of fields
   \begin{equation*}
   k\subset k(\alpha_1)\subset k(\alpha_1,\alpha_2)\subset\dots\subset k(\alpha_1,\dots,\alpha_n)
   \end{equation*}
   each one generated from the preceding field by a single element. Assume that each \(\alpha_i\) is
   algebraic over \(k\), \(i=1,\dots,n\). As a special case of our preceding remark, we note
   that \(\alpha_{i+1}\) is algebraic over \(k(\alpha_1,\dots,\alpha_i)\). Hence each step of the tower is algebraic

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(E=k(\alpha_1,\dots,\alpha_n)\) be a finitely genrated extension of a field \(k\), and assume \(\alpha_i\)
   algebraic over \(k\) for each \(i=1,\dots,n\). Then \(E\) is finite algebraic over \(k\)
   #+END_proposition

   #+BEGIN_proof
   \(E\) is finite by Proposition ref:prop5.1.4 and Corollary ref:cor5.1.3. Algebraic by Proposition ref:prop5.1.1
   #+END_proof

   Let \(\calc\) be a certain class of extension fields \(F\subset E\). \(\calc\) is *distinguished* if it satisfies
   the following conditions
   1. Let \(k\subset F\subset E\) be a tower of fields. The extension \(k\subset E\) is in \(\calc\) iff \(k\subset F\) is
      in \(\calc\) and \(F\subset E\) is in \(\calc\)
   2. if \(k\subset E\) is in \(\calc\), if \(F\) is any extension of \(k\), and \(E,F\) are both contained in
      some field, then \(F\subset EF\) is in \(\calc\)
   3. if \(k\subset F\) and \(k\subset E\) are in \(\calc\) and \(F,E\) are subfields of a common field,
      then \(k\subset FE\) is in \(\calc\)



   #+BEGIN_center
      \begin{tikzcd}
   E\ar[d,dash]\\
   F\ar[d,dash]\\
   k
   \end{tikzcd}\hspace{1cm}
   \begin{tikzcd}
   &EF\ar[ddl,dash]\ar[dr,dash]\\
   &&F\\
   E\\
   &k\ar[ul,dash]\ar[uur,dash]
   \end{tikzcd}\hspace{1cm}
   \begin{tikzcd}
   &EF\ar[dl,dash]\ar[rd,dash]\\
   E&&F\\
   &k\ar[ul,dash]\ar[ur,dash]
   \end{tikzcd}
   #+END_center


   It is convenient to write \(E/F\) instead of \(F\subset E\) to denote an extension

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   The class of algebraic extensions is distinguished, and so is the class of finite extensions
   #+END_proposition
** Algebraic Closure
   Let \(E\) be an extension of a field \(F\) and let
   \begin{equation*}
   \sigma:F\to L
   \end{equation*}
   be an embedding (i.e. an injective homomorphism) of \(F\) into \(L\). Then \sigma induces an
   isomorphism of \(F\) with its image \(\sigma F\), which is sometimes written \(F^\sigma\). An embedding
   \tau of \(E\) in \(L\) will be said to be *over* \sigma if the restriction of \tau to \(F\) is equal
   to \sigma. We also say that \tau *extends* \sigma. If \sigma is the identity then we say that \tau is an
   embedding of \(E\) *over* \(F\)
   #+BEGIN_center
   \begin{tikzcd}
   E\ar[r,"\tau"]&L\\
   F\ar[u,"\text{inc}"]\ar[r,"\sigma"']&L\ar[u,"\id"']
   \end{tikzcd}\hspace{1.5cm}
   \begin{tikzcd}[column sep=small]
   E\ar[rr,"\tau"]&&L\\
   &F\ar[ul,"\text{inc}"]\ar[ur,"\text{inc}"']
   \end{tikzcd}
   #+END_center

   #+BEGIN_remark
   Let \(f(X)\in F[X]\) be a polynomial, and let \alpha be a root of \(f\) in \(E\).
   Say \(f(X)=a_0+\dots+a_nX^n\). Then
   \begin{equation*}
   0=f(\alpha)=a_0+a_1\alpha+\dots+a_n\alpha^n
   \end{equation*}
   If \tau extends \sigma as above, then we see that \tau\alpha is a root of \(f^\sigma\) because
   \begin{equation*}
   0=\tau(f(\alpha))=a_0^\sigma+a_1^\sigma(\tau\alpha)+\dots+a_n^\sigma(\tau\alpha)^n
   \end{equation*}
   Here we write \(a^\sigma\) instead of \(\sigma(a)\).
   #+END_remark

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   Let \(E\) be an algebraic extension of \(k\), and let \(\sigma:E\to E\) be an embedding of \(E\) into
   itself over \(k\). Then \sigma is an automorphism
   #+END_lemma

   #+BEGIN_proof
   Since \alpha is injective, it will suffice to prove that \sigma is surjective. Let \(\alpha\in E\), let \(p(X)\)
   be its irreducible polynomial over \(k\), and let \(E'\) be the subfield of \(E\) generated by
   all the roots of \(p(X)\) which lie in \(E\). Then \(E'\) is finitely generated, hence is a
   finite extension of \(k\). Furthermore, \sigma must map a root of \(p(X)\) on a root of \(p(X)\),
   hence \sigma maps \(E'\) into itself.
   #+END_proof

   Let \(E,F\) be extensions of a field \(k\), contained in some bigger field \(L\). We can form the
   ring \(E[F]\) generated by the elements of \(F\) over \(E\). Then \(E[F]=F[E]\) and \(EF\) is the
   quotient field of this ring. it is clear that the elements of \(E[F]\) can be written in the form
   \begin{equation*}
   a_1b_1+\dots+a_nb_n
   \end{equation*}
   with \(a_i\in E\) and \(b_i\in F\). Hence \(EF\) is the field of quotients of these elements

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   Let \(E_1,E_2\) be extensions of a field \(k\), contained in some bigger field \(E\), and let
   \sigma be an embedding of \(E\) in some field \(L\). Then
   \begin{equation*}
   \sigma(E_1E_2)=\sigma(E_1)\sigma(E_2)
   \end{equation*}
   #+END_lemma

   #+BEGIN_proof
   We apply \sigma to a quotient of elements of the above type, say
   \begin{equation*}
   \sigma\left( \frac{a_1b_1+\dots+a_nb_n}{a_1'b_1'+\dots+a_m'b_m'} \right)=
   \frac{a_1^\sigma b_1^\sigma+\dots+a_n^\sigma b_n^\sigma}{a_1^{\prime\sigma}b_1^{\prime\sigma}+\dots+a_m^{\prime\sigma}b_m^{\prime\sigma}}
   \end{equation*}
   and see that the image is an element of \(\sigma(E_1)\sigma(E_2)\)
   #+END_proof

   Let \(k\) be a field, \(f(X)\) a polynomial of degree \(\ge 1\) in \(k[X]\). We consider the
   problem of finding an extension \(E\) of \(k\) in which \(f\) has a root. If \(p(X)\) is
   an irreducible polynomial in \(k[X]\) which divides \(f(X)\), then any root of \(p(X)\) will also
   be a root of \(f(X)\), so we may restrict ourselves to irreducible polynomials

   Let \(p(X)\) be irreducible, and consider the canonical homomorphism
   \begin{equation*}
   \sigma:k[X]\to k[X]/(p(X))
   \end{equation*}
   The kernel is 0 because every nonzero element of \(k\) is invertible in \(k\), generates the unit
   ideal, and 1 does not lie in the kernel. let \xi be the image of \(X\) under \sigma, i.e., \(\xi=\sigma(x)\) is
   the residue class of \(X\) mod \(p(X)\). Then
   \begin{equation*}
   p^\sigma(\xi)=p^\sigma(X^\sigma)=(p(X))^\sigma=0
   \end{equation*}

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(k\) be a field and \(f\) a polynomial in \(k[X]\) of degree \(\ge 1\). Then there exists an
   extension \(E\) of \(k\) in which \(f\) has a root
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Let \(k\) be a field and let \(f_1,\dots,f_n\) be polynomials in \(k[X]\) of degrees \(\ge 1\). Then
   there exists an extension \(E\) of \(k\) in which each \(f_i\) has a root, \(i=1,\dots,n\)
   #+END_corollary

   We define a field \(L\) to be *algebraically closed* if every polynomial in \(L[X]\) of
   degree \(\ge 1\) has a root in \(L\)

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   Let \(k\) be a field. Then there exists an algebraically closed field containing \(k\) as a
   subfield
   #+END_theorem

   #+BEGIN_proof
   We first construct an extension \(E_1\) of \(k\) in which every polynomial in \(k[x]\) of
   degree \(\ge 1\) has a root. To each polynomial \(f\) in \(k[X]\) of degree \(\ge 1\) we associate a
   letter \(X_f\) and we let \(S\) be the set of all such letters. We form the polynomial
   ring \(k[S]\), and content that the ideal generated by all the polynomials \(f(X_f)\) in \(k[S]\)
   is not the unit ideal. If it is, then there is a finite combination of elements in our ideal
   which is equal to 1
   \begin{equation*}
   g_1f_1(X_{f_1})
   \end{equation*}
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Let \(k\) be a field. There exists an extension \(k^a\) which is algebraic over \(k\) and
   algebraically closed
   #+END_corollary

   if \(L\) is an algebraically closed field, and \(f\in L[X]\) has degree \(\ge 1\), then there
   exists \(c\in L\) and \(\alpha_1,\dots,\alpha_n\in L\) s.t.


* Real Fields
** Ordered Fields
    Let \(K\) be a field. An *ordering* of \(K\) is a subset \(P\) of \(K\)
    having the following properties
    \bigskip
    \begin{itemize}[itemindent=3em]
    \item[\textbf{ORD 1.}] Given \(x\in K\), we have either \(x\in P\) ,or \(x=0\) or
    \(-x\in P\), and these three possibilities are mutually exclusive
    \item[\textbf{ORD 2.}] If \(x,y\in P\), then \(x+y,xy\in P\)
    \end{itemize}

    \(K\) is *ordered by* \(P\), and we call \(P\) the set of *positive
    elements*

    Suppose \(K\) is ordered by \(P\). Since \(1\neq0\) and \(1=1^2=(-1)^2\), we
    see that \(1\in P\). By *ORD 2*, it follows that \(1+\dots+1\in P\), whence \(K\)
    has characteristic 0. If \(x\in P\) and \(x\neq0\), then \(xx^{-1}=1\in P\) implies
    that \(x^{-1}\in P\)

    #+BEGIN_center
    /Let \(E\) be a field. Then a product of sums of squares in \(E\) is a sum
    of squares./

    /If \(a,b\in E\) are sum of squares and \(b\neq0\), then \(a/b\) is a sum of
    squares/
    #+END_center

    Consider complex number:)

    Let \(x,y\in K\). We define \(x<y\) to mean that \(y-x\in P\). If \(x<0\) we say
    that \(x\) is *negative*.

    If \(K\) is ordered and \(x\in K\), \(x\neq0\), then \(x^2\) is positive

    If \(E\) has characteristic \(\neq2\), and \(-1\) is a sum of squares in \(E\),
    then every element \(a\in E\) is a sum of squares, because
    \(4a=(1+a)^2-(1-a)^2\)

    If \(K\) is a field with an ordering \(P\), and \(F\) is a subfield, then
    obviously, \(P\cap F\) defines an ordering of \(F\), which is called the
    *induced* ordering

    Let \(K\) be an ordered field and let \(F\) be a subfield with the induced
    ordering. We put \(\abs{x}=x\) if \(x>0\) and \(\abs{x}=-x\) if \(x<0\). An
    element \(\alpha\in K\) is *infinitely large* over \(F\) if \(\abs{\alpha}\ge x\) for all
    \(x\in F\). It is *infinitely small* over \(F\) if \(0\le\abs{\alpha}\le\abs{x}\) for
    all \(x\in F\), \(x\neq0\). \alpha is infinitely large if and only if \(\alpha^{-1}\) is
    infinitely small. \(K\) is *archimedean* over \(F\) if \(K\) has no elements
    which are infinitely large over \(F\). An intermediate field \(F_1\),
    \(K\supset F_1\supset F\) is *maximal archimedean over* \(F\) in \(K\) if it is
    archimedean over \(F\) and no other intermediate field containing \(F_1\) is
    archimedean over \(F\). We say that \(F\) is *maximal archimedean in* \(K\)
    if it is maximal archimedean over itself in \(K\)

    Let \(K\) be an ordered field and \(F\) a subfield. Let \(K\) be an ordered
    field and \(F\) a subfield. Let \(\fo\) be the set of elements of \(K\)
    which are not infinitely large over \(F\). Then \(\fo\) is a ring and that
    for any \(\alpha\in K\), we have \(\alpha\) or \(\alpha^{-1}\in\fo\). Hence \(\fo\) is what is
    called a valuation ring, containing \(F\). Let \(\fm\) be the ideal of all
    \(\alpha\in K\) which are infinitely small over \(F\). Then \(\fm\) is the unique
    maximal ideal of \(\fo\), because any element in \(\fo\) which is not in
    \(\fm\) has an inverse in \(\fo\). We call \(\fo\) the
    *valuation ring determined by the ordering of* \(K/F\)

    #+ATTR_LATEX: :options []
    #+BEGIN_proposition
    Let \(K\) be an ordered field and \(F\) a subfield. Let \(\fo\) be the
    valuation ring determined by the ordering of \(K/F\), and let \(\fm\) be its
    maximal ideal. Then \(\fo/\fm\) is a real field.
    #+END_proposition

    #+BEGIN_proof
    Otherwise, we could write
    \begin{equation*}
    -1=\displaystyle\sum\alpha_i^2+a
    \end{equation*}
    with \(\alpha_i\in\fo\) and \(a\in\fm\). Since \(\sum\alpha_i^2\) is positive and \(a\) is
    infinitely small, such a relation is clearly impossible
    #+END_proof
* COMMENT Problem
  <<Problem1>>

  <<Problem2>>

  <<Problem3>>
