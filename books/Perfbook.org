#+title: Is Parallel Programming Hard, And, If So, @@latex:\\@@What Can You Do About It?

#+AUTHOR: Paul E. McKenny
#+EXPORT_FILE_NAME: ../latex/perfbook/perfbook.tex
#+LATEX_HEADER: \input{/Users/wu/notes/preamble.tex}
#+LATEX_HEADER: \graphicspath{{../../books/}}
#+LATEX_HEADER: \makeindex
#+STARTUP: shrink

#+LATEX_HEADER: \definecolor{mintedbg}{rgb}{0.99,0.99,0.99}
#+LATEX_HEADER: \usepackage[cachedir=\detokenize{~/miscellaneous/trash}]{minted}
#+LATEX_HEADER: \setminted{breaklines,
#+LATEX_HEADER:   mathescape,
#+LATEX_HEADER:   bgcolor=mintedbg,
#+LATEX_HEADER:   fontsize=\footnotesize,
#+LATEX_HEADER:   frame=single,
#+LATEX_HEADER:   linenos}
* Tools of the Trade
** POSIX Multiprocessing
** Alternatives to POSIX Operations
** Accessing Shared Variables
        #+CAPTION: Living Dangerously Early 1990s Style
        #+NAME: l4.14
        #+BEGIN_SRC c
ptr = global_ptr;
if (ptr != NULL && ptr < high_address)
    do_low(ptr);
        #+END_SRC

        #+CAPTION: C Compilers Can Invent Loads
        #+NAME: l4.15
        #+BEGIN_SRC c
if (global_ptr != NULL &&
    global_ptr < high_address)
    do_low(global_ptr);
        #+END_SRC
*** Shared-Variable Shenanigans
        Given code that does plain loads and stores,the compiler is within its rights to assume that the
        affected variables are neither accessed nor modified by any other thread. This assumption allows the
        compiler to carry out a large number of transformations, including load tearing, store tearing, load
        fusing, store fusing, code reordering, invented loads, invented stores, store-to-load transformations,
        and dead- code elimination.

        * *Load tearing* occurs when the compiler uses multiple load instructions for a single access.
        * *Store tearing* occurs when the compiler uses multiple store instructions for a single access.
        * *Load fusing* occurs when the compiler uses the result of a prior load from a given variable instead
          of repeating the load.
          #+CAPTION: Inviting Load Fusing
          #+NAME: l4.16
          #+BEGIN_SRC c
while (!need_to_stop)
    do_something_quickly();
          #+END_SRC
          #+CAPTION: C Compilers Can Fuse Loads
          #+NAME: l4.17
          #+BEGIN_SRC c
if (!need_to_stop)
    for (;;) {
        do_something_quickly();
        do_something_quickly();
        do_something_quickly();
        do_something_quickly();
        do_something_quickly();
        do_something_quickly();
        do_something_quickly();
        do_something_quickly();
        do_something_quickly();
        do_something_quickly();
        do_something_quickly();
        do_something_quickly();
        do_something_quickly();
        do_something_quickly();
        do_something_quickly();
        do_something_quickly();
    }
          #+END_SRC
        * *Store fusing* can occur when the compiler notices a pair of successive stores to a given variable
          with no intervening loads from that variable.
          #+CAPTION: C Compilers Can Fuse Stores
          #+NAME: l4.19
          #+BEGIN_SRC c
void shut_it_down(void)
{
    status = SHUTTING_DOWN; /* BUGGY!!! */
    start_shutdown();
    while (!other_task_ready) /* BUGGY!!! */
        continue;
    finish_shutdown();
    status = SHUT_DOWN; /* BUGGY!!! */
    do_something_else();
}

void work_until_shut_down(void)
{
    while (status != SHUTTING_DOWN) /* BUGGY!!! */
        do_more_work();
    other_task_ready = 1; /* BUGGY!!! */
}
          #+END_SRC
        * *Code reordering*. It might seem futile to prevent the compiler from changing the order of accesses in
          cases where the underlying hardware is free to reorder them. However, modern machines have /exact
          exceptions/ and /exact interrupts/, meaning that any interrupt or exception will appear to have
          happened at a specific place in the instruction stream. This means that the handler will see the
          effect of all prior instructions, but won't see the effect of any subsequent instructions.
        * *Invented loads* were illustrated by the code in Listings ref:l4.14 and ref:l4.15, in which the compiler
          optimized away a temporary variable, thus loading from a shared variable more often than intended.
          * *Invented stores*: For example, a compiler emitting code for ~work_until_shut_down()~ in Listing
            ref:l4.19 might notice that ~other_task_ready~ is not accessed by ~do_more_work()~, and stored to on
            line 16. If ~do_more_work()~ was a complex inline function, it might be necessary to do a register
            spill, in which case one attractive place to use for temporary storage is ~other_task_ready~. After
            all, there are no accesses to it, so what is the harm?
          #+CAPTION: Inviting an Invented Store
          #+NAME: l4.20
          #+BEGIN_SRC c
if (condition)
    a = 1;
else
    do_a_bunch_of_stuff(&a);
          #+END_SRC
          #+CAPTION: Compiler Invents an Invited Store
          #+NAME: l4.21
          #+BEGIN_SRC c
a = 1;
if (!condition) {
    a = 0;
    do_a_bunch_of_stuff(&a);
}
          #+END_SRC
          * *Store-to-load transformations* can occur when the compiler notices that a plain store might not
            actually change the value in memory.
            #+CAPTION: Inviting a Store-to-Load Conversion
            #+NAME: l4.22
            #+BEGIN_SRC c
r1 = p;
if (unlikely(r1))
    do_something_with(r1);
barrier();
p = NULL;
            #+END_SRC
            #+CAPTION: Compiler Converts a Store to a Load
            #+NAME: l4.23
            #+BEGIN_SRC c
r1 = p;
if (unlikely(r1))
    do_something_with(r1);
barrier();
if (p != NULL)
    p = NULL;
            #+END_SRC
          * *Dead-code elimination*
*** A Volatile Solution
        To summarize, the ~volatile~ keyword can prevent load tearing and store tearing in cases where the loads
        and stores are machine-sized and properly aligned. It can also prevent load fusing, store fusing,
        invented loads, and invented stores. However, although it does prevent the compiler from reordering
        volatile accesses with each other, it does nothing to prevent the CPU from reordering these accesses.
        Furthermore, it does nothing to prevent either compiler or CPU from reordering non-~volatile~ accesses
        with each other or with ~volatile~ accesses. Preventing these types of reordering requires the techniques described in the next section.
*** Assembling the Rest of a Solution
        #+begin_src c
#define barrier() __asm__ __volatile__ ("" : : : "memory")
        #+end_src
        In the barrier() macro, the ~__asm__~ introduces the asm directive, the ~__volatile__~ prevents the
        compiler from optimizing the asm away, the empty string specifies that no actual instructions are to
        be emitted, and the final ~"memory"~ tells the compiler that this do-nothing asm can arbitrarily change
        memory. In response, the compiler will avoid moving any memory references across the barrier() macro.
        This means that the real-time- destroying loop unrolling shown in Listing ref:l4.17 can be prevented
        by adding ~barrier()~ calls as shown on lines 2 and 4 of Listing 4.28.
        #+LATEX: \wu{
        ~barrier()~ is for compiler. For hardware, we need ~smp_mb~
        #+LATEX: }
        #+CAPTION: Preventing C Compilers From Fusing Loads
        #+NAME: l4.28
        #+BEGIN_SRC c
while (!need_to_stop) {
    barrier();
    do_something_quickly();
    barrier();
}
        #+END_SRC
        These two lines of code prevent the compiler from pushing the load from ~need_to_stop~ into or past
        ~do_something_quickly()~ from either direction.

        However, this does nothing to prevent the CPU from reordering the references.
        #+begin_src c
// arch-arm/arch-arm.h
#define smp_mb()  __asm__ __volatile__("dmb" : : : "memory")

// arch-x86/arch-x86.h
#define smp_mb() __asm__ __volatile__("mfence" : : : "memory")

// arch-ppc64/arch-ppc64.h
#define smp_mb()  __asm__ __volatile__("sync" : : : "memory")

// arch-arm64/arch-arm64.h
#define smp_mb()  __asm__ __volatile__("dmb ish" : : : "memory")
        #+end_src
* Partitioning and Synchronization Design
** Partitioning Exercises
*** Dining Philosophers Problem
*** Double-Ended Queue
**** Double-Ended Queue Validation
**** Left- and Right-Hand Locks
        Far better consider other designs as locks' domain must overlap when there are fewer than four
        elements on the list.
        #+ATTR_LATEX: :width .7\textwidth :float nil
        #+NAME: 6.5
        #+CAPTION: Double-Ended Queue With Left- and Right-Hand Locks
        [[../images/perfbook/13.png]]
**** Compound Double-Ended Queue
        #+ATTR_LATEX: :width .7\textwidth :float nil
        #+NAME: 6.6
        #+CAPTION: Compound Double-Ended Queue
        [[../images/perfbook/14.png]]
        Two separate double-ended queues are run in tandem, each protected by its own lock. This means that
        elements must occasionally be shuttled from one of the double-ended queues to the other, in which case
        both locks must be held.

        A simple lock hierarchy may be used to avoid deadlock, for example, always acquiring the left-hand
        lock before acquiring the right-hand lock.

        The main complication arises when dequeuing from an empty queue, in which case it is necessary to:
        1. If holding the right-hand lock, release it and acquire the left-hand lock.
        2. Acquire the right-hand lock.
        3. Rebalance the elements across the two queues.
        4. Remove the required element if there is one.
        5. Release both locks.
**** Hashed Double-Ended Queue
        We assign one lock to guard the left-hand index, one to guard the right-hand index, and one lock for
        each hash chain.

        #+ATTR_LATEX: :width .7\textwidth :float nil
        #+NAME: 6.7
        #+CAPTION: Hashed Double-Ended Queue
        [[../images/perfbook/15.png]]

        #+ATTR_LATEX: :width .7\textwidth :float nil
        #+NAME: 6.8
        #+CAPTION: Hashed Double-Ended Queue After Insertions
        [[../images/perfbook/16.png]]

* Deferred Processing
** Running Example
        The value looked up and returned will also be a simple integer, so that the data structure is as shown
        in Figure ref:9.1, which directs packets with address 42 to interface 1, address 56 to interface 3,
        and address 17 to interface 7.

        #+ATTR_LATEX: :width .99\textwidth :float nil
        #+NAME: 9.1
        #+CAPTION: Pre-BSD Packet Routing List
        [[../images/perfbook/3.png]]

        #+NAME: l9.1
        #+CAPTION: Sequential Pre-BSD Routing Table
        #+begin_src c
struct route_entry {
    struct cds_list_head re_next;
    unsigned long addr;
    unsigned long iface;
};
CDS_LIST_HEAD(route_list);

unsigned long route_lookup(unsigned long addr)
{
    struct route_entry *rep;
    unsigned long ret;

    cds_list_for_each_entry(rep, &route_list, re_next) {
        if (rep->addr == addr) {
            ret = rep->iface;
            return ret;
        }
    }
    return ULONG_MAX;
}

int route_add(unsigned long addr, unsigned long interface)
{
    struct route_entry *rep;

    rep = malloc(sizeof(*rep));
    if (!rep)
        return -ENOMEM;
    rep->addr = addr;
    rep->iface = interface;
    cds_list_add(&rep->re_next, &route_list);
    return 0;
}

int route_del(unsigned long addr)
{
    struct route_entry *rep;

    cds_list_for_each_entry(rep, &route_list, re_next) {
        if (rep->addr == addr) {
            cds_list_del(&rep->re_next);
            free(rep);
            return 0;
        }
    }
    return -ENOENT;
}
        #+end_src

        Listing ref:l9.1 (~route_seq.c~) shows a simple single-threaded implementation corresponding to Figure ref:9.1.
** Reference Counting
        <<c9.2>>
        #+NAME: l9.2
        #+CAPTION: Reference-Counted Pre-BSD Routing Table Lookup (BUGGY)
        #+begin_src c
struct route_entry {
    atomic_t re_refcnt;
    struct route_entry *re_next;
    unsigned long addr;
    unsigned long iface;
    int re_freed;
};
struct route_entry route_list;
DEFINE_SPINLOCK(routelock);

static void re_free(struct route_entry *rep)
{
    WRITE_ONCE(rep->re_freed, 1);
    free(rep);
}

unsigned long route_lookup(unsigned long addr)
{
    int old;
    int new;
    struct route_entry *rep;
    struct route_entry **repp;
    unsigned long ret;

retry:
    repp = &route_list.re_next;
    rep = NULL;
    do {
        if (rep && atomic_dec_and_test(&rep->re_refcnt))
            re_free(rep);
        rep = READ_ONCE(*repp);
        if (rep == NULL)
            return ULONG_MAX;
        do {
            if (READ_ONCE(rep->re_freed))
                abort();
            old = atomic_read(&rep->re_refcnt);
            if (old <= 0)
                goto retry;
            new = old + 1;
        } while (atomic_cmpxchg(&rep->re_refcnt,
                                old, new) != old);
        repp = &rep->re_next;
    } while (rep->addr != addr);
    ret = rep->iface;
    if (atomic_dec_and_test(&rep->re_refcnt))
        re_free(rep);
    return ret;
}
        #+end_src

        Starting with Listing ref:l9.2, line 2 adds the actual reference counter, line 6 adds a ~->re_freed~
        use-after-free check field, line 9 adds the ~routelock~ that will be used to synchronize concurrent
        updates, and lines 11–15 add ~re_free()~, which sets ~->re_freed~, enabling ~route_lookup()~ to check for
        use-after-free bugs. In ~route_lookup()~ itself, lines 29–30 release the reference count of the prior
        element and free it if the count becomes zero, and lines 34–42 acquire a reference on the new element,
        with lines 35 and 36 performing the use-after-free check.

        #+NAME: l9.3
        #+CAPTION: Reference-Counted Pre-BSD Routing Table Add/Delete (BUGGY)
        #+begin_src c
int route_add(unsigned long addr, unsigned long interface)
{
    struct route_entry *rep;

    rep = malloc(sizeof(*rep));
    if (!rep)
        return -ENOMEM;
    atomic_set(&rep->re_refcnt, 1);
    rep->addr = addr;
    rep->iface = interface;
    spin_lock(&routelock);
    rep->re_next = route_list.re_next;
    rep->re_freed = 0;
    route_list.re_next = rep;
    spin_unlock(&routelock);
    return 0;
}

int route_del(unsigned long addr)
{
    struct route_entry *rep;
    struct route_entry **repp;

    spin_lock(&routelock);
    repp = &route_list.re_next;
    for (;;) {
        rep = *repp;
        if (rep == NULL)
            break;
        if (rep->addr == addr) {
            ,*repp = rep->re_next;
            spin_unlock(&routelock);
            if (atomic_dec_and_test(&rep->re_refcnt))
                re_free(rep);
            return 0;
        }
        repp = &rep->re_next;
    }
    spin_unlock(&routelock);
    return -ENOENT;

}
        #+end_src

        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        Why bother with a use-after-free check?

        To greatly increase the probability of finding bugs
        #+END_remark

        In Listing ref:l9.3, lines 11, 15, 24, 32, and 39 introduce locking to synchronize concurrent updates.
        Line 13 initializes the ~->re_freed~ use-after-free-check field, and finally lines 33–34 invoke
        ~re_free()~ if the new value of the reference count is zero.

        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        Why doesn't ~route_del()~ in Listing ref:l9.3 use reference counts to protect the traversal to the
        element to be freed?

        Because the traversal is already protected by the lock, so no additional protection is required.
        #+END_remark

        #+ATTR_LATEX: :width .99\textwidth :float nil
        #+NAME: 9.2
        #+CAPTION: Pre-BSD Routing Table Protected by Reference Counting
        [[../images/perfbook/4.png]]

        Ideal is from Listing ref:l9.1. Refcnt performance is abysmal.
        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        Why the break in the “ideal” line at 224 CPUs in Figure 9.2? Shouldn’t it be a straight line?

        The break is due to hyperthreading. On this particular system, the first hardware thread in each core
        within a socket have consecutive CPU numbers, followed by the first hardware threads in each core for
        the other sockets, and finally followed by the second hardware thread in each core on all the sockets.
        On this particular system, CPU numbers 0–27 are the first hardware threads in each of the 28 cores in
        the first socket, numbers 28–55 are the first hardware threads in each of the 28 cores in the second
        socket, and so on, so that numbers 196–223 are the first hardware threads in each of the 28 cores in
        the eighth socket. Then CPU numbers 224–251 are the second hardware threads in each of the 28 cores of
        the first socket, numbers 252–279 are the second hardware threads in each of the 28 cores of the
        second socket, and so on until numbers 420–447 are the second hardware threads in each of the 28 cores
        of the eighth socket.

        Why does this matter?

        Because the two hardware threads of a given core share resources, and this workload seems to allow a
        single hardware thread to consume more than half of the relevant resources within its core. Therefore,
        adding the second hardware thread of that core adds less than one might hope. Other workloads might
        gain greater benefit from each core’s second hardware thread, but much depends on the details of both
        the hardware and the workload.
        #+END_remark

        One sequence of events leading to the use-after-free bug is as follows, given the list shown in Figure ref:l9.1:
        1. Thread A looks up address 42, reaching line 32 of ~route_lookup()~ in Listing ref:l9.2. In other
           words, Thread A has a pointer to the first element, but has not yet acquired a reference to it.
        2. Thread B invokes ~route_del()~ in Listing ref:l9.2 to delete the route entry for address 42. It
           completes successfully, and because this entry’s ~->re_refcnt~ field was equal to the value one, it
           invokes ~re_free()~ to set the ~->re_freed~ field and to free the entry.
        3. Thread A continues execution of ~route_lookup()~. Its rep pointer is non-~NULL~, but line 35 sees that
           its ~->re_freed~ field is non-zero, so line 36 invokes ~abort()~
** Hazard Pointers
        <<c9.3>>
        One way of avoiding problems with concurrent reference counting is to implement the reference counters
        inside out, that is, rather than incrementing an integer stored in the data element, instead store a
        pointer to that data element in per-CPU (or per-thread) lists. Each element of these lists is called a
        *hazard pointer*.

        The value of a given data element’s “virtual reference counter” can then be obtained by counting the
        number of hazard pointers referencing that element. Therefore, if that element has been rendered
        inaccessible to readers, and there are no longer any hazard pointers referencing it, that element may
        safely be freed.

        #+begin_src c
/* Parameters to the algorithm:
 ,*  K: Number of hazard pointers per thread.
 ,*  H: Number of hazard pointers required.
 ,*  R: Chosen such that R = H + Omega(H).
 ,*/
#define K 2
#define H (K * NR_THREADS)
#define R (100 + 2*H)

/* Must be the first field in the hazard-pointer-protected structure. */
/* It is illegal to nest one such structure inside another. */
typedef struct hazptr_head {
	struct hazptr_head *next;
} hazptr_head_t;

typedef struct hazard_pointer_s {
	void *  __attribute__ ((__aligned__ (CACHE_LINE_SIZE))) p;
} hazard_pointer;

/* Must be dynamically initialized to be an array of size H. */
hazard_pointer *HP;

void hazptr_init(void);
void hazptr_thread_exit(void);
void hazptr_scan();
void hazptr_free_later(hazptr_head_t *);
void hazptr_free(void *ptr); /* supplied by caller. */

#define HAZPTR_POISON 0x8

static hazptr_head_t __thread *rlist;
static unsigned long __thread rcount;
static hazptr_head_t __thread **gplist;
        #+end_src

        #+NAME: l9.4
        #+CAPTION: Hazard-Pointer Recording and Clearing
        #+begin_src c
static inline void *_h_t_r_impl(void **p,
                                hazard_pointer *hp)
{
    void *tmp;

    tmp = READ_ONCE(*p);
    if (!tmp || tmp == (void *)HAZPTR_POISON)
        return tmp;
    WRITE_ONCE(hp->p, tmp);
    smp_mb();
    if (tmp == READ_ONCE(*p))
        return tmp;
    return (void *)HAZPTR_POISON;
}

#define hp_try_record(p, hp) _h_t_r_impl((void **)(p), hp)

static inline void *hp_record(void **p,
                              hazard_pointer *hp)
{
    void *tmp;

    do {
        tmp = hp_try_record(p, hp);
    } while (tmp == (void *)HAZPTR_POISON);
    return tmp;
}


static inline void hp_clear(hazard_pointer *hp)
{
    smp_mb();
    WRITE_ONCE(hp->p, NULL);
}
        #+end_src

        The ~hp_try_record()~ macro on line 16 is simply a casting wrapper for the ~_h_t_r_impl()~ function, which
        attempts to store the pointer referenced by ~p~ into the hazard pointer referenced by ~hp~. If successful,
        it returns the value of the stored pointer. If it fails due to that pointer being ~NULL~, it returns
        ~NULL~. Finally, if it fails due to racing with an update, it returns a special ~HAZPTR_POISON~ token.

        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        Given that papers on hazard pointers use the bottom bits of each pointer to mark deleted elements,
        what is up with ~HAZPTR_POISON~?
        #+END_remark

        Line 6 reads the pointer to the object to be protected. If line 8 finds that this pointer was either
        ~NULL~ or the special ~HAZPTR_POISON~ deleted-object token, it returns the pointer’s value to inform the
        caller of the failure. Otherwise, line 9 stores the pointer into the specified hazard pointer, and
        line 10 forces full ordering of that store with the reload of the original pointer on line 11. If the
        value of the original pointer has not changed, then the hazard pointer protects the pointed-to object,
        and in that case, line 12 returns a pointer to that object, which also indicates success to the
        caller. Otherwise, if the pointer changed between the two ~READ_ONCE()~ invocations, line 13 indicates
        failure. (\wu{the second read ensures that \texttt{p} is not changed between the read and write})

        The ~hp_clear()~ function is even more straightforward, with an ~smp_mb()~ to force full ordering between
        the caller’s uses of the object protected by the hazard pointer and the setting of the hazard pointer
        to ~NULL~.

        Once a hazard-pointer-protected object has been removed from its linked data structure, so that it is
        now inaccessible to future hazard-pointer readers, it is passed to ~hazptr_free_later()~, which is shown
        on lines 48–56 of Listing ref:l9.5. Lines 50 and 51 enqueue the object on a per-thread list rlist and
        line 52 counts the object in rcount. If line 53 sees that a sufficiently large number of objects are
        now queued, line 54 invokes ~hazptr_scan()~ to attempt to free some of them.

        #+NAME: l9.5
        #+CAPTION: Hazard-Pointer Scanning and Freeing
        #+begin_src c
int compare(const void *a, const void *b)
{
    return ( *(hazptr_head_t **)a - *(hazptr_head_t **)b );
}

void hazptr_scan()
{
    hazptr_head_t *cur;
    int i;
    hazptr_head_t *tmplist;
    hazptr_head_t **plist = gplist;
    unsigned long psize;

    if (plist == NULL) {
        psize = sizeof(hazptr_head_t *) * K * NR_THREADS;
        plist = (hazptr_head_t **)malloc(psize);
        BUG_ON(!plist);
        gplist = plist;
    }
    smp_mb();
    psize = 0;
    for (i = 0; i < H; i++) {
        uintptr_t hp = (uintptr_t)READ_ONCE(HP[i].p);

        if (!hp)
            continue;
        plist[psize++] = (hazptr_head_t *)(hp & ~0x1UL);

    }
    smp_mb();
    qsort(plist, psize, sizeof(hazptr_head_t *), compare);
    tmplist = rlist;
    rlist = NULL;
    rcount = 0;
    while (tmplist != NULL) {
        cur = tmplist;
        tmplist = tmplist->next;
        if (bsearch(&cur, plist, psize,
                    sizeof(hazptr_head_t *), compare)) {
            cur->next = rlist;
            rlist = cur;
            rcount++;
        } else {
            hazptr_free(cur);
        }
    }
}

void hazptr_free_later(hazptr_head_t *n)
{
    n->next = rlist;
    rlist = n;
    rcount++;
    if (rcount >= R) {
        hazptr_scan();
    }
}
        #+end_src


        The ~hazptr_scan()~ function is shown on lines 6–46 of the listing. This function relies on a fixed
        maximum number of threads (~NR_THREADS~) and a fixed maximum number of hazard pointers per thread (~K~),
        which allows a fixed-size array of hazard pointers to be used. Because any thread might need to scan
        the hazard pointers, each thread maintains its own array, which is referenced by the per-thread
        variable ~gplist~. If line 14 determines that this thread has not yet allocated its gplist, lines 15–18
        carry out the allocation. The memory barrier on line 20 ensures that _all threads see the removal of
        all objects by this thread_ before lines 22–28 scan all of the hazard pointers, accumulating non-~NULL~
        pointers into the ~plist~ array and counting them in ~psize~. The memory barrier on line 29 ensures that
        the reads of the hazard pointers happen before any objects are freed. Line 30 then sorts this array to
        enable use of binary search below.

        Lines 31 and 32 remove all elements from this thread’s list of to-be-freed objects, placing them on
        the local tmplist and line 33 zeroes the count. Each pass through the loop spanning lines 34–45
        processes each of the to-be-freed objects. Lines 35 and 36 remove the first object from tmplist, and
        if lines 37 and 38 determine that there is a hazard pointer protecting this object, lines 39–41 place
        it back onto rlist. Otherwise, line 43 frees the object.

        The Pre-BSD routing example can use hazard pointers as shown in Listing ref:l9.6 for data structures
        and ~route_lookup()~, and in Listing 9.7 for ~route_add()~ and ~route_del()~ (~route_hazptr.c~). As with
        reference counting, the hazard-pointers implementation is quite similar to the sequential algorithm
        shown in Listing ref:l9.1, so only differences will be discussed.
        #+NAME: l9.6
        #+CAPTION: Hazard-Pointer Pre-BSD Routing Table Lookup
        #+begin_src c
struct route_entry {
    struct hazptr_head hh;
    struct route_entry *re_next;
    unsigned long addr;
    unsigned long iface;
    int re_freed;
};
struct route_entry route_list;
DEFINE_SPINLOCK(routelock);
hazard_pointer __thread *my_hazptr;

unsigned long route_lookup(unsigned long addr)
{
    int offset = 0;
    struct route_entry *rep;
    struct route_entry **repp;

retry:
    repp = &route_list.re_next;
    do {
        rep = hp_try_record(repp, &my_hazptr[offset]);
        if (!rep)
            return ULONG_MAX;
        if ((uintptr_t)rep == HAZPTR_POISON)
            goto retry;
        repp = &rep->re_next;
    } while (rep->addr != addr);
    if (READ_ONCE(rep->re_freed))
        abort();
    return rep->iface;
}
        #+end_src

        Starting with Listing ref:l9.6, line 2 shows the ~->hh~ field used to queue objects pending
        hazard-pointer free, line 6 shows the ~->re_freed~ field used to detect use-after-free bugs, and line 21
        invokes ~hp_try_record()~ to attempt to acquire a hazard pointer. If the return value is ~NULL~, line 23
        returns a not-found indication to the caller. If the call to ~hp_try_record()~ raced with deletion, line
        25 branches back to line 18’s retry to re-traverse the list from the beginning. The do–while loop
        falls through when the desired element is located, but if this element has already been freed, line 29
        terminates the program. Otherwise, the element’s ~->iface~ field is returned to the caller.

        Note that line 21 invokes ~hp_try_record()~ rather than the easier-to-use ~hp_record()~, restarting the
        full search upon ~hp_try_record()~ failure. And such restarting is absolutely required for correctness.
        To see this, consider a hazard-pointer-protected linked list containing elements A, B, and C that is
        subjected to the following sequence of events:
        1. Thread 0 stores a hazard pointer to element B (having presumably traversed to element B from element A).
        2. Thread 1 removes element B from the list, which sets the pointer from element B to element C to the
           special ~HAZPTR_POISON~ value in order to mark the deletion. Because Thread 0 has a hazard pointer to
           element B, it cannot yet be freed.
        3. Thread 1 removes element C from the list. Because there are no hazard pointers referencing element
           C, it is immediately freed.
        4. Thread 0 attempts to acquire a hazard pointer to now-removed element B’s successor, but
           ~hp_try_record()~ returns the ~HAZPTR_POISON~ value, forcing the caller to restart its traversal from
           the beginning of the list.

        Therefore, hazard-pointer readers must typically restart the full traversal in the face of a
        concurrent deletion.

        These hazard-pointer restrictions result in great benefits to readers, courtesy of the fact that the
        hazard pointers are stored local to each CPU or thread, which in turn allows traversals to be carried
        out without any writes to the data structures being traversed.

        #+NAME: l9.7
        #+CAPTION: Hazard-Pointer Pre-BSD Routing Table Add/Delete
        #+begin_src c
int route_add(unsigned long addr, unsigned long interface)
{
    struct route_entry *rep;

    rep = malloc(sizeof(*rep));
    if (!rep)
        return -ENOMEM;
    rep->addr = addr;
    rep->iface = interface;
    rep->re_freed = 0;
    spin_lock(&routelock);
    rep->re_next = route_list.re_next;
    route_list.re_next = rep;
    spin_unlock(&routelock);
    return 0;
}


int route_del(unsigned long addr)
{
    struct route_entry *rep;
    struct route_entry **repp;

    spin_lock(&routelock);
    repp = &route_list.re_next;
    for (;;) {
        rep = *repp;
        if (rep == NULL)
            break;
        if (rep->addr == addr) {
            ,*repp = rep->re_next;
            rep->re_next = (struct route_entry *)HAZPTR_POISON;
            spin_unlock(&routelock);
            hazptr_free_later(&rep->hh);
            return 0;
        }
        repp = &rep->re_next;
    }
    spin_unlock(&routelock);
    return -ENOENT;
}
        #+end_src

        #+ATTR_LATEX: :width .88\textwidth :float nil
        #+NAME: 9.3
        #+CAPTION: Pre-BSD Routing Table Protected by Hazard Pointers
        [[../images/perfbook/56.png]]

        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        * Figure ref:9.3 shows no sign of hyperthread-induced flattening at 224 threads. Why is that?

          Modern microprocessors are complicated beasts, so signif- icant skepticism is appropriate for any
          simple answer. That aside, the most likely reason is the full memory barriers required by
          hazard-pointers readers. Any delays resulting from those memory barriers would make time available
          to the other hardware thread sharing the core, resulting in greater scalability at the expense of
          per-hardware-thread performance.
        * [[cite:&10.1145/2483852.2483867]] shows that hazard pointers have near-ideal performance. Whatever
          happened in Figure ref:9.3?
        #+END_remark
** Sequence Locks
        <<c9.4>>
        The key component of sequence locking is the sequence number, which has an even value in the absence
        of updaters and an odd value if there is an update in progress. Readers can then snapshot the value
        before and after each access. If either snapshot has an odd value, or if the two snapshots differ,
        there has been a concurrent update, and the reader must discard the results of the access and then
        retry it. Readers therefore use the ~read_seqbegin()~ and ~read_seqretry()~ functions shown in Listing ref:l9.8
        when accessing data protected by a sequence lock. Writers must increment the value before and after
        each update, and only one writer is permitted at a given time. Writers therefore use the
        ~write_seqlock()~ and ~write_sequnlock()~ functions shown in Listing ref:l9.9 when updating data protected
        by a sequence lock.

        #+NAME: l9.8
        #+CAPTION: Sequence-Locking Reader
        #+begin_src c
do {
    seq = read_seqbegin(&test_seqlock);
    /* read-side access. */
} while (read_seqretry(&test_seqlock, seq));
        #+end_src

        #+NAME: l9.9
        #+CAPTION: Sequence-Locking Writer
        #+begin_src c
write_seqlock(&test_seqlock);
/* Update */
write_sequnlock(&test_seqlock);
        #+end_src

        #+NAME: l9.10
        #+CAPTION: Sequence-Locking Implementation
        #+begin_src c
typedef struct {
    unsigned long seq;
    spinlock_t lock;
} seqlock_t;

static inline void seqlock_init(seqlock_t *slp)
{
    slp->seq = 0;
    spin_lock_init(&slp->lock);
}

static inline unsigned long read_seqbegin(seqlock_t *slp)
{
    unsigned long s;

    s = READ_ONCE(slp->seq);
    smp_mb();
    return s & ~0x1UL;
}

static inline int read_seqretry(seqlock_t *slp,
                                unsigned long oldseq)
{
    unsigned long s;

    smp_mb();
    s = READ_ONCE(slp->seq);
    return s != oldseq;
}


static inline void write_seqlock(seqlock_t *slp)
{
    spin_lock(&slp->lock);
    ++slp->seq;
    smp_mb();
}
static inline void write_sequnlock(seqlock_t *slp)
{
    smp_mb();
    ++slp->seq;
    spin_unlock(&slp->lock);
}
        #+end_src

        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        Why not have ~read_seqbegin()~ in ref:l9.10 check for the low-order bit being set, and retry internally,
        rather than allowing a doomed read to start?

        That would be a legitimate implementation. However, if the workload is read-mostly, it would likely
        increase the overhead of the common-case successful read, which could be counter-productive. However,
        given a sufficiently large fraction of updates and sufficiently high-overhead readers, having the
        check internal to ~read_seqbegin()~ might be preferable
        #+END_remark

        Line 17 orders this snapshot before the caller's critical section. Line 26 orders the caller's prior
        critical section before line 27's fetch.
        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        * Why is the ~smp_mb()~ on line 26 of Listing ref:l9.10 needed?

          If it was omitted, both the compiler and the CPU would be within their rights to move the critical
          section Preceding the call to ~read_seqretry()~ down below this function. This would prevent the
          sequence lock from protecting the critical section. The ~smp_mb()~ primitive prevents such reordering.
        * Can’t weaker memory barriers be used in the code in Listing ref:l9.10?
        * Why isn’t seq on line 2 of Listing ref:l9.10 unsigned rather than ~unsigned long~? After all, if
          ~unsigned~ is good enough for the Linux kernel, shouldn’t it be good enough for everyone?

          Overflow issue, 32 bit can be easily overflowed
        #+END_remark

        #+CAPTION: Sequence-Locked Pre-BSD Routing Table Lookup (BUGGY)
        #+NAME: l9.11
        #+BEGIN_SRC c
struct route_entry {
    struct route_entry *re_next;
    unsigned long addr;
    unsigned long iface;
    int re_freed;
};

struct route_entry route_list;
DEFINE_SEQ_LOCK(sl);

unsigned long route_lookup(unsigned long addr)
{
    struct route_entry *rep;
    struct route_entry **repp;
    unsigned long ret;
    unsigned long s;

retry:
    s = read_seqbegin(&sl);
    repp = &route_list.re_next;
    do {
        rep = READ_ONCE(*repp);
        if (rep == NULL) {
            if (read_seqretry(&sl, s))
                goto retry;
            return ULONG_MAX;
        }
        repp = &rep->re_next;
    } while (rep->addr != addr);
    if (READ_ONCE(rep->re_freed))
        abort();
    ret = rep->iface;
    if (read_seqretry(&sl, s))
        goto retry;
    return ret;
}
        #+END_SRC

        It suffers use-after-free failures. The problem is that the reader might encounter a segmentation
        violation due to accessing an already-freed structure before ~read_seqretry()~ has a chance to warn of
        the concurrent update.

        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        Can this bug be fixed? In other words, can you use sequence locks as the only synchronization
        mechanism protecting a linked list supporting concurrent addition, deletion, and lookup?

        One trivial way of accomplishing this is to surround all accesses, including the read-only accesses,
        with ~write_ seqlock()~ and ~write_sequnlock()~. Of course, this solution also prohibits all read-side
        parallelism, resulting in massive lock contention, and furthermore could just as easily be implemented using
        simple locking.

        If you do come up with a solution that uses ~read_ seqbegin()~ and ~read_seqretry()~ to protect read-side
        accesses, make sure that you correctly handle the following sequence of events:
        1. CPU 0 is traversing the linked list, and picks up a pointer to list element A.
        2. CPU 1 removes element A from the list and frees it.
        3. CPU 2 allocates an unrelated data structure, and gets the memory formerly occupied by element A. In
           this unrelated data structure, the memory previously used for element A’s ->next pointer is now
           occupied by a floating-point number.
        4. CPU 0 picks up what used to be element A’s -> next pointer, gets random bits, and therefore gets a
           segmentation fault.

        One way to protect against this sort of problem requires use of “type-safe memory”. Roughly similar
        solutions are possible using the hazard pointers. But in either case, you would be using some other
        synchronization mechanism in addition to sequence locks!
        #+END_remark

        #+ATTR_LATEX: :width .8\textwidth :float nil
        #+NAME:
        #+CAPTION: Pre-BSD Routing Table Protected by Sequence Locking
        [[../images/perfbook/5.png]]
** Read-Copy Update (RCU)
        <<c9.5>>
        All of the mechanisms discussed in the preceding sections used one of a number of approaches to defer
        specific actions until they may be carried out safely. The reference counters discussed in Section ref:c9.2
        use explicit counters to defer actions that could disturb readers, which results in read-side
        contention and thus poor scalability. The hazard pointers covered by Section ref:c9.3 uses implicit
        counters in the guise of per-thread lists of pointer. This avoids read-side contention, but requires
        readers to do stores and conditional branches, as well as either full memory barriers in read-side
        primitives or real-time-unfriendly inter-processor interrupts in update-side primitives. The sequence
        lock presented in Section ref:c9.4 also avoids read-side contention, but does not protect pointer
        traversals and, like hazard pointers, requires either full memory barriers in read-side primitives, or
        inter-processor interrupts in update-side primitives.
*** Introduction to RCU
        To minimize implementability concerns, we focus on a minimal data structure, which consists of a
        single global pointer that is either ~NULL~ or references a single structure.
        #+ATTR_LATEX: :width .8\textwidth :float nil
        #+NAME: 9.6
        #+CAPTION: Insertion With Concurrent Readers
        [[../images/perfbook/6.png]]

        #+ATTR_LATEX: :width .8\textwidth :float nil
        #+NAME: 9.7
        #+CAPTION: Insertion With Concurrent Readers
        [[../images/perfbook/7.png]]

        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        * Why does Figure ref:9.7 use ~smp_store_release()~ given that it is storing a ~NULL~ pointer? Wouldn’t
          ~WRITE_ONCE()~ work just as well in this case, given that there is no structure initialization to
          order against the store of the ~NULL~ pointer?

          Yes, it would.

          Because a ~NULL~ pointer is being assigned, there is nothing to order against, so there is no need for
          ~smp_store_ release()~. In contrast, when assigning a non-~NULL~ pointer, it is necessary to use
          ~smp_store_release()~ in order to ensure that initialization of the pointed-to structure is carried
          out before assignment of the pointer.

          In short, ~WRITE_ONCE()~ would work, and would save a little bit of CPU time on some architectures.
          However, as we will see, software-engineering concerns will motivate use of a special
          ~rcu_assign_pointer()~ that is quite similar to ~smp_store_release()~.
        * Readers running concurrently with each other and with the procedure outlined in Figure ref:9.7 can
          disagree on the value of ~gptr~. Isn’t that just a wee bit problematic???

          Not necessarily.

          As hinted at in Sections 3.2.3 and 3.3, speed-of-light delays mean that a computer’s data is always
          stale com- pared to whatever external reality that data is intended to model.

          Real-world algorithms therefore absolutely must tolerate inconsistancies between external reality
          and the in-computer data reflecting that reality. Many of those algorithms are also able to tolerate
          some degree of inconsistency within the in-computer data. Section 10.3.4 discusses this point in
          more detail.

          Please note that this need to tolerate inconsistent and stale data is not limited to RCU. It also
          applies to reference counting, hazard pointers, sequence locks, and even to some locking use cases.
          For example, if you compute some quantity while holding a lock, but use that quantity after
          releasing that lock, you might well be using stale data. After all, the data that quantity is based
          on might change arbitrarily as soon as the lock is released.

          So yes, RCU readers can see stale and inconsistent data, but no, this is not necessarily
          problematic. And, when needed, there are RCU usage patterns that avoid both staleness and
          inconsistency
        #+END_remark

        But how can we tell when all of the pre-existing readers have in fact completed?
**** Core RCU API
        The ~rcu_read_lock()~ and ~rcu_read_unlock()~ functions delimit RCU read-side critical sections. These may
        be nested, so that one ~rcu_read_lock()~ – ~rcu_read_unlock()~ pair can be enclosed within another. In
        this case, the nested set of RCU read-side critical sections act as one large critical section
        covering the full extent of the nested set.

        The third read-side API member, ~rcu_dereference()~, fetches an RCU-protected pointer.
        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        What is an RCU-protected pointer?

        A pointer to RCU-protected data. RCU-protected data is in turn a block of dynamically allocated memory
        whose freeing will be *deferred* such that an RCU grace period will elapse between the time that there
        were no longer any RCU-reader-accessible pointers to that block and the time that that block is freed.
        This ensures that no RCU readers will have access to that block at the time that it is freed.

        RCU-protected pointers must be handled carefully. For example, any reader that intends to dereference
        an RCU-protected pointer must use ~rcu_dereference()~ (or stronger) to load that pointer. In addition,
        any updater must use ~rcu_assign_pointer()~ (or stronger) to store to that pointer.
        #+END_remark

        The ~synchronize_rcu()~ function implements the “wait for readers” operation from Figure ref:9.7. The
        ~call_rcu()~ function is the asynchronous counterpart of ~synchronize_rcu()~ by invoking the specified
        function after all pre-existing RCU readers have completed. Finally, the ~rcu_assign_pointer()~ macro is
        used to update an RCU-protected pointer.

        #+CAPTION: Core RCU API
        |----------+----------------------+------------------------------------------------------|
        |          | Primitive            | Purpose                                              |
        |----------+----------------------+------------------------------------------------------|
        | Readers  | ~rcu_read_lock()~      | Start an RCU read-side critical section.             |
        |          | ~rcu_read_unlock()~    | End an RCU read-side critical section.               |
        |          | ~rcu_dereference()~    | Safely load an RCU-protected pointer.                |
        |----------+----------------------+------------------------------------------------------|
        | Updaters | ~synchronize_rcu()~    | Wait for all pre-existing RCU read-side critical     |
        |          |                      | sections to complete.                                |
        |          | ~call_rcu()~           | Invoke the specified function after all pre-existing |
        |          |                      | RCU read-side critical sections complete.            |
        |          | ~rcu_assign_pointer()~ | Safely update an RCU-protected pointer.              |
        |----------+----------------------+------------------------------------------------------|
*** Waiting for Readers
        It is tempting to base the reader-waiting functionality of ~synchronize_rcu()~ and ~call_rcu()~ on a
        reference counter updated by ~rcu_read_lock()~ and ~rcu_read_unlock()~, but Figure 5.1 in Chapter 5 shows
        that concurrent reference counting results in extreme overhead.

        A second approach observes that memory synchronization is expensive, and therefore uses registers
        instead, namely each CPU’s or thread’s program counter (PC), thus imposing no overhead on readers, at
        least in the absence of concurrent updates. The updater polls each relevant PC, and if that PC is not
        within read-side code, then the corresponding CPU or thread is within a quiescent state, in turn
        signaling the completion of any reader that might have access to the newly removed data element. Once
        all CPU’s or thread’s PCs have been observed to be outside of any reader, the grace period has
        completed. Please note that this approach poses some serious challenges, including memory ordering,
        functions that are sometimes invoked from readers, and ever-exciting code-motion optimizations.
        Nevertheless, this approach is said to be used in production

        The sixth approach is stopping one CPU or thread at a time. Furthermore, numerous applications already
        have states (termed *quiescent states*) that can be reached only after all pre-existing readers are done.
        Once all CPUs and/or threads have passed through a quiescent state, the system is said to have
        completed a *grace period*, at which point all readers in existence at the start of that grace period
        are guaranteed to have completed. As a result, it is also guaranteed to be safe to free any removed
        data items that were removed prior to the start of that grace period.

        Within a non-preemptive operating-system kernel, for context switch to be a valid quiescent state,
        readers must be prohibited from blocking while referencing a given instance data structure obtained
        via the gptr pointer shown in Figures ref:9.6 and ref:9.7.

        Again, this same constraint is imposed on reader threads dereferencing ~gptr~: Such threads are not
        allowed to block until after they are done using the pointed-to data item.

        <<P1>>

        This approach is termed *quiescent-state-based reclamation* (QSBR). A QSBR schematic is shown in Figure
        9.8, with time advancing from the top of the figure to the bottom.
        #+ATTR_LATEX: :width .8\textwidth :float nil
        #+NAME: 9.8
        #+CAPTION: QSBR: Waiting for Pre-Existing Readers
        [[../images/perfbook/8.png]]
      The cyan-colored boxes depict RCU read-side critical sections, each of which begins with ~rcu_read_lock()~
      and ends with ~rcu_read_unlock()~. CPU 1 does the ~WRITE_ONCE()~ that removes the current data item
      (presumably having previously read the pointer value and availed itself of appropriate synchronization),
      then waits for readers. This wait operation results in an immediate context switch, which is a quiescent
      state (denoted by the pink circle), which in turn means that all prior reads on CPU 1 have completed.
      Next, CPU 2 does a context switch, so that all readers on CPUs 1 and 2 are now known to have completed.
      Finally, CPU 3 does a context switch. At this point, all readers throughout the entire system are known
      to have completed, so the grace period ends, permitting ~synchronize_rcu()~ to return to its caller, in
      turn permitting CPU 1 to free the old data item.
**** Toy Implementation
        Toy non-preemptive Linux-kernel implementation:
        #+begin_src c
void synchronize_rcu(void)
{
    int cpu;

    for_each_online_cpu(cpu)
        sched_setaffinity(current->pid, cpumask_of(cpu));
}
        #+end_src
        The ~for_each_online_cpu()~ primitive iterates over all CPUs, and the ~sched_setaffinity()~ function
        causes the current thread to execute on the specified CPU, which forces the destination CPU to execute
        a context switch.

        #+CAPTION: Insertion and Deletion With Concurrent Readers
        #+NAME: l9.13
        #+BEGIN_SRC c
struct route *gptr;

int access_route(int (*f)(struct route *rp))
{
    int ret = -1;
    struct route *rp;

    rcu_read_lock();
    rp = rcu_dereference(gptr);
    if (rp)
        ret = f(rp);
    rcu_read_unlock();
    return ret;
}

struct route *ins_route(struct route *rp)
{
    struct route *old_rp;

    spin_lock(&route_lock);
    old_rp = gptr;
    rcu_assign_pointer(gptr, rp);
    spin_unlock(&route_lock);
    return old_rp;
}

int del_route(void)
{
    struct route *old_rp;

    spin_lock(&route_lock);
    old_rp = gptr;
    RCU_INIT_POINTER(gptr, NULL);
    spin_unlock(&route_lock);
    synchronize_rcu();
    free(old_rp);
    return !!old_rp;
}
        #+END_SRC

        Listing ref:l9.13 shows how reading, insertion and deletion can be implemented.

        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        * What is the point of ~rcu_read_lock()~ and ~rcu_read_unlock()~ in Listing ref:l9.13? Why not just let
          the quiescent states speak for themselves?

          Recall that readers are not permitted to pass through a quiescent state. For example, within the
          Linux kernel, RCU readers are not permitted to execute a context switch. Use of ~rcu_read_lock()~ and
          ~rcu_read_unlock()~ enables debug checks for improperly placed quiescent states, making it easy to
          find bugs that would otherwise be difficult to find, intermittent, and quite destructive.
        * What is the point of ~rcu_dereference()~, ~rcu_assign_pointer()~ and ~RCU_INIT_POINTER()~ in Listing
          ref:l9.13? Why not just use ~READ_ONCE()~, ~smp_store_release()~, and ~WRITE_ONCE()~, respectively?

          The RCU-specific APIs do have similar semantics to the suggested replacements, but also enable
          static-analysis debugging checks that complain if an RCU-specific API is invoked on a non-RCU
          pointer and vice versa.
        #+END_remark
**** RCU Properties
        A key RCU property is that reads need not wait for updates.
*** RCU Fundamentals
**** Publish-Subscribe Mechanism
        Publication is carried out by ~rcu_assign_pointer()~, met by the C11 store-release operation.

        Note that if concurrent updates are required, some sort of synchronization mechanism will be required
        to mediate among multiple concurrent ~rcu_assign_pointer()~ calls on the same pointer.

        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        Wouldn’t use of data ownership for RCU updaters mean that the updates could use exactly the same
        sequence of instructions as would the corresponding single-threaded code?

        Sometimes, for example, on TSO systems such as x86 or the IBM mainframe where a store-release
        operation emits a single store instruction. However, weakly ordered systems must also emit a memory
        barrier or perhaps a store-release instruction. In addition, removing data requires quite a bit of
        additional work because it is necessary to wait for pre-existing readers before freeing the removed
        data.
        #+END_remark

        Subscription is carried out by ~rcu_dereference()~, which orders the subscription operation’s load from
        the pointer is before the dereference.

        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        But suppose that updaters are adding and removing multiple data items from a linked list while a
        reader is iterating over that same list. Specifically, suppose that a list initially contains elements
        A, B, and C, and that an updater removes element A and then adds a new element D at the end of the
        list. The reader might well see {A, B, C, D}, when that sequence of elements never actually ever
        existed! In what alternate universe would that qualify as “not disrupting concurrent readers”???

        In the universe where an iterating reader is only required to traverse elements that were present
        throughout the full duration of the iteration. In the example, that would be elements B and C. Because
        elements A and D were each present for only part of the iteration, the reader is permitted to iterate
        over them, but not obliged to. Note that this supports the common case where the reader is simply
        looking up a single item, and does not know or care about the presence or absence of other items.  If
        stronger consistency is required, then higher-cost synchronization mechanisms are required, for
        example, sequence locking or reader-writer locking. But if stronger consistency is not required (and
        it very often is not), then why pay the higher cost?
        #+END_remark
**** Wait For Pre-Existing RCU Readers
        The relationship between an RCU read-side critical section and a later RCU grace period is an if-then
        relationship, as illustrated by Figure ref:9.11. If any portion of a given critical section precedes
        the beginning of a given grace period, then RCU guarantees that all of that critical section will
        precede the end of that grace period.
        #+ATTR_LATEX: :width .7\textwidth :float nil
        #+NAME: 9.11
        #+CAPTION: RCU Reader and Later Grace Period
        [[../images/perfbook/9.png]]


        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        What other final values of ~r1~ and ~r2~ are possible in Figure ref:9.11?

        If ~r1==0~ then ~r2==0~.

        And ~r1==1&&r2==0~ and ~r1==1&&r2==1~ are both possible.
        #+END_remark

        The relationship between an RCU read-side critical section and an earlier RCU grace period is also an
        if-then relationship, as illustrated by Figure ref:9.12. If any portion of a given critical section
        follows the end of a given grace period, then RCU guarantees that all of that critical section will
        follow the beginning of that grace period. If ~r2==1~, then ~r1==1~.
        #+ATTR_LATEX: :width .7\textwidth :float nil
        #+NAME: 9.12
        #+CAPTION: RCU Reader and Earlier Grace Period
        [[../images/perfbook/10.png]]

        Finally, as shown in Figure ref:9.13, an RCU read-side critical section can be completely overlapped by an
        RCU grace period. In this case, ~r1~'s final value is 1 and ~r2~'s final value is 0.
        #+ATTR_LATEX: :width .7\textwidth :float nil
        #+NAME: 9.13
        #+CAPTION: RCU Reader Within Grace Period
        [[../images/perfbook/11.png]]
        However, it cannot be the case that ~r1~'s final value is 0 and ~r2~'s final value is 1. This would mean
        that an RCU read-side critical section had completely overlapped a grace period, which is forbidden.

        RCU’s wait-for-readers guarantee therefore has two parts:
        1. If any part of a given RCU read-side critical section precedes the beginning of a given grace
           period, then the entirety of that critical section precedes the end of that grace period.
        2. If any part of a given RCU read-side critical section follows the end of a given grace period, then
           the entirety of that critical section follows the beginning of that grace period.


        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        What would happen if ~P0()~'s accesses in Figures ref:9.11 -ref:9.13 were stores?

        The exact same ordering rules would apply
        #+END_remark

        #+ATTR_LATEX: :width .7\textwidth :float nil
        #+NAME: 9.14
        #+CAPTION: Summary of RCU Grace-Period Ordering Guarantees
        [[../images/perfbook/12.png]]
**** Maintain Multiple Versions of Recently Updated Objects

* Appendices :ignore:
#+LATEX: \appendix
** Why Memory Barriers
*** Cache Structure
    #+ATTR_LATEX: :width .7\textwidth :float nil
    #+NAME:
    #+CAPTION: Modern Computer System Cache Structure
    [[../images/perfbook/1.png]]

    Data flows among the CPUs’ caches and memory in fixed-length blocks called “cache lines”, which
    are normally a power of two in size, ranging from 16 to 256 bytes. When a given data item is
    first accessed by a given CPU, it will be absent from that CPU’s cache, meaning that a “cache
    miss” (or, more specifically, a “startup” or “warmup” cache miss) has occurred. The cache miss
    means that the CPU will have to wait (or be “stalled”) for hundreds of cycles while the item is
    fetched from memory. However, the item will be loaded into that CPU’s cache, so that subsequent
    accesses will find it in the cache and therefore run at full speed.

    #+ATTR_LATEX: :width .8\textwidth :float nil
    #+NAME:
    #+CAPTION: CPU Cache Structure
    [[../images/perfbook/2.png]]

    This cache has sixteen “sets” and two “ways” for a total of 32 “lines”, each entry containing a
    single 256-byte “cache line”, which is a 256-byte-aligned block of memory.

    Each box corresponds to a cache entry, which can contain a 256-byte cache line. Since the cache
    lines must be 256-byte aligned, the low eight bits of each address are zero, and the choice of
    hardware hash function means that the next-higher four bits match the hash line number.

    What happens when it does a write? Because it is important that all CPUs agree on the value of a
    given data item, before a given CPU writes to that data item, it must first cause it to be
    removed, or “invalidated”, from other CPUs’ caches. Once this invalidation has completed, the
    CPU may safely modify the data item. If the data item was present in this CPU’s cache, but was
    read-only, this process is termed a “write miss”. Once a given CPU has completed invalidating a
    given data item from other CPUs’ caches, that CPU may repeatedly write (and read) that data
    item.

    Later, if one of the other CPUs attempts to access the data item, it will incur a cache miss,
    this time because the first CPU invalidated the item in order to write to it. This type of cache
    miss is termed a “communication miss”, since it is usually due to several CPUs using the data
    items to communicate (for example, a lock is a data item that is used to communicate among CPUs
    using a mutual-exclusion algorithm).

    Clearly, much care must be taken to ensure that all CPUs maintain a coherent view of the data.
    With all this fetching, invalidating, and writing, it is easy to imagine data being lost or
    (perhaps worse) different CPUs having conflicting values for the same data item in their
    respective caches.
*** Cache-Coherence Protocols
**** MESI States
    MESI stands for "modified", "exclusive", "shared", and "invalid", the four states a given cache
    line can take on using this protocol.
* Problems
        1. [[P1]]: don't quite understand the words about non-preemptive kernel
