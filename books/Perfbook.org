#+title: Is Parallel Programming Hard, And, If So, @@latex:\\@@What Can You Do About It?

#+AUTHOR: Paul E. McKenny
#+EXPORT_FILE_NAME: ../latex/perfbook/perfbook.tex
#+LATEX_HEADER: \input{/Users/wu/notes/preamble.tex}
#+LATEX_HEADER: \graphicspath{{../../books/}}
#+LATEX_HEADER: \makeindex
#+STARTUP: shrink

#+LATEX_HEADER: \definecolor{mintedbg}{rgb}{0.99,0.99,0.99}
#+LATEX_HEADER: \usepackage[cachedir=\detokenize{~/miscellaneous/trash}]{minted}
#+LATEX_HEADER: \setminted{breaklines,
#+LATEX_HEADER:   mathescape,
#+LATEX_HEADER:   bgcolor=mintedbg,
#+LATEX_HEADER:   fontsize=\footnotesize,
#+LATEX_HEADER:   frame=single,
#+LATEX_HEADER:   linenos}
* Deferred Processing
** Running Example
        The value looked up and returned will also be a simple integer, so that the data structure is as shown
        in Figure ref:9.1, which directs packets with address 42 to interface 1, address 56 to interface 3,
        and address 17 to interface 7.

        #+ATTR_LATEX: :width .99\textwidth :float nil
        #+NAME: 9.1
        #+CAPTION: Pre-BSD Packet Routing List
        [[../images/perfbook/3.png]]

        #+NAME: l9.1
        #+CAPTION: Sequential Pre-BSD Routing Table
        #+begin_src c
struct route_entry {
    struct cds_list_head re_next;
    unsigned long addr;
    unsigned long iface;
};
CDS_LIST_HEAD(route_list);

unsigned long route_lookup(unsigned long addr)
{
    struct route_entry *rep;
    unsigned long ret;

    cds_list_for_each_entry(rep, &route_list, re_next) {
        if (rep->addr == addr) {
            ret = rep->iface;
            return ret;
        }
    }
    return ULONG_MAX;
}

int route_add(unsigned long addr, unsigned long interface)
{
    struct route_entry *rep;

    rep = malloc(sizeof(*rep));
    if (!rep)
        return -ENOMEM;
    rep->addr = addr;
    rep->iface = interface;
    cds_list_add(&rep->re_next, &route_list);
    return 0;
}

int route_del(unsigned long addr)
{
    struct route_entry *rep;

    cds_list_for_each_entry(rep, &route_list, re_next) {
        if (rep->addr == addr) {
            cds_list_del(&rep->re_next);
            free(rep);
            return 0;
        }
    }
    return -ENOENT;
}
        #+end_src

        Listing ref:l9.1 (~route_seq.c~) shows a simple single-threaded implementation corresponding to Figure ref:9.1.
** Reference Counting
        #+NAME: l9.2
        #+CAPTION: Reference-Counted Pre-BSD Routing Table Lookup (BUGGY)
        #+begin_src c
struct route_entry {
    atomic_t re_refcnt;
    struct route_entry *re_next;
    unsigned long addr;
    unsigned long iface;
    int re_freed;
};
struct route_entry route_list;
DEFINE_SPINLOCK(routelock);

static void re_free(struct route_entry *rep)
{
    WRITE_ONCE(rep->re_freed, 1);
    free(rep);
}

unsigned long route_lookup(unsigned long addr)
{
    int old;
    int new;
    struct route_entry *rep;
    struct route_entry **repp;
    unsigned long ret;

retry:
    repp = &route_list.re_next;
    rep = NULL;
    do {
        if (rep && atomic_dec_and_test(&rep->re_refcnt))
            re_free(rep);
        rep = READ_ONCE(*repp);
        if (rep == NULL)
            return ULONG_MAX;
        do {
            if (READ_ONCE(rep->re_freed))
                abort();
            old = atomic_read(&rep->re_refcnt);
            if (old <= 0)
                goto retry;
            new = old + 1;
        } while (atomic_cmpxchg(&rep->re_refcnt,
                                old, new) != old);
        repp = &rep->re_next;
    } while (rep->addr != addr);
    ret = rep->iface;
    if (atomic_dec_and_test(&rep->re_refcnt))
        re_free(rep);
    return ret;
}
        #+end_src

        Starting with Listing ref:l9.2, line 2 adds the actual reference counter, line 6 adds a ~->re_freed~
        use-after-free check field, line 9 adds the ~routelock~ that will be used to synchronize concurrent
        updates, and lines 11–15 add ~re_free()~, which sets ~->re_freed~, enabling ~route_lookup()~ to check for
        use-after-free bugs. In ~route_lookup()~ itself, lines 29–30 release the reference count of the prior
        element and free it if the count becomes zero, and lines 34–42 acquire a reference on the new element,
        with lines 35 and 36 performing the use-after-free check.

        #+NAME: l9.3
        #+CAPTION: Reference-Counted Pre-BSD Routing Table Add/Delete (BUGGY)
        #+begin_src c
int route_add(unsigned long addr, unsigned long interface)
{
    struct route_entry *rep;

    rep = malloc(sizeof(*rep));
    if (!rep)
        return -ENOMEM;
    atomic_set(&rep->re_refcnt, 1);
    rep->addr = addr;
    rep->iface = interface;
    spin_lock(&routelock);
    rep->re_next = route_list.re_next;
    rep->re_freed = 0;
    route_list.re_next = rep;
    spin_unlock(&routelock);
    return 0;
}

int route_del(unsigned long addr)
{
    struct route_entry *rep;
    struct route_entry **repp;

    spin_lock(&routelock);
    repp = &route_list.re_next;
    for (;;) {
        rep = *repp;
        if (rep == NULL)
            break;
        if (rep->addr == addr) {
            ,*repp = rep->re_next;
            spin_unlock(&routelock);
            if (atomic_dec_and_test(&rep->re_refcnt))
                re_free(rep);
            return 0;
        }
        repp = &rep->re_next;
    }
    spin_unlock(&routelock);
    return -ENOENT;

}
        #+end_src

        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        Why bother with a use-after-free check?

        To greatly increase the probability of finding bugs
        #+END_remark

        In Listing ref:l9.3, lines 11, 15, 24, 32, and 39 introduce locking to synchronize concurrent updates.
        Line 13 initializes the ~->re_freed~ use-after-free-check field, and finally lines 33–34 invoke
        ~re_free()~ if the new value of the reference count is zero.

        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        Why doesn't ~route_del()~ in Listing ref:l9.3 use reference counts to protect the traversal to the
        element to be freed?

        Because the traversal is already protected by the lock, so no additional protection is required.
        #+END_remark

        #+ATTR_LATEX: :width .99\textwidth :float nil
        #+NAME: 9.2
        #+CAPTION: Pre-BSD Routing Table Protected by Reference Counting
        [[../images/perfbook/4.png]]

        Ideal is from Listing ref:l9.1. Refcnt performance is abysmal.
        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        Why the break in the “ideal” line at 224 CPUs in Figure 9.2? Shouldn’t it be a straight line?

        The break is due to hyperthreading. On this particular system, the first hardware thread in each core
        within a socket have consecutive CPU numbers, followed by the first hardware threads in each core for
        the other sockets, and finally followed by the second hardware thread in each core on all the sockets.
        On this particular system, CPU numbers 0–27 are the first hardware threads in each of the 28 cores in
        the first socket, numbers 28–55 are the first hardware threads in each of the 28 cores in the second
        socket, and so on, so that numbers 196–223 are the first hardware threads in each of the 28 cores in
        the eighth socket. Then CPU numbers 224–251 are the second hardware threads in each of the 28 cores of
        the first socket, numbers 252–279 are the second hardware threads in each of the 28 cores of the
        second socket, and so on until numbers 420–447 are the second hardware threads in each of the 28 cores
        of the eighth socket.

        Why does this matter?

        Because the two hardware threads of a given core share resources, and this workload seems to allow a
        single hardware thread to consume more than half of the relevant resources within its core. Therefore,
        adding the second hardware thread of that core adds less than one might hope. Other workloads might
        gain greater benefit from each core’s second hardware thread, but much depends on the details of both
        the hardware and the workload.
        #+END_remark

        One sequence of events leading to the use-after-free bug is as follows, given the list shown in Figure ref:l9.1:
        1. Thread A looks up address 42, reaching line 32 of ~route_lookup()~ in Listing ref:l9.2. In other
           words, Thread A has a pointer to the first element, but has not yet acquired a reference to it.
        2. Thread B invokes ~route_del()~ in Listing ref:l9.2 to delete the route entry for address 42. It
           completes successfully, and because this entry’s ~->re_refcnt~ field was equal to the value one, it
           invokes ~re_free()~ to set the ~->re_freed~ field and to free the entry.
        3. Thread A continues execution of ~route_lookup()~. Its rep pointer is non-~NULL~, but line 35 sees that
           its ~->re_freed~ field is non-zero, so line 36 invokes ~abort()~
** Hazard Pointers
        One way of avoiding problems with concurrent reference counting is to implement the reference counters
        inside out, that is, rather than incrementing an integer stored in the data element, instead store a
        pointer to that data element in per-CPU (or per-thread) lists. Each element of these lists is called a
        *hazard pointer*.

        The value of a given data element’s “virtual reference counter” can then be obtained by counting the
        number of hazard pointers referencing that element. Therefore, if that element has been rendered
        inaccessible to readers, and there are no longer any hazard pointers referencing it, that element may
        safely be freed.

        #+begin_src c
/* Parameters to the algorithm:
 ,*  K: Number of hazard pointers per thread.
 ,*  H: Number of hazard pointers required.
 ,*  R: Chosen such that R = H + Omega(H).
 ,*/
#define K 2
#define H (K * NR_THREADS)
#define R (100 + 2*H)

/* Must be the first field in the hazard-pointer-protected structure. */
/* It is illegal to nest one such structure inside another. */
typedef struct hazptr_head {
	struct hazptr_head *next;
} hazptr_head_t;

typedef struct hazard_pointer_s {
	void *  __attribute__ ((__aligned__ (CACHE_LINE_SIZE))) p;
} hazard_pointer;

/* Must be dynamically initialized to be an array of size H. */
hazard_pointer *HP;

void hazptr_init(void);
void hazptr_thread_exit(void);
void hazptr_scan();
void hazptr_free_later(hazptr_head_t *);
void hazptr_free(void *ptr); /* supplied by caller. */

#define HAZPTR_POISON 0x8

static hazptr_head_t __thread *rlist;
static unsigned long __thread rcount;
static hazptr_head_t __thread **gplist;
        #+end_src

        #+NAME: l9.4
        #+CAPTION: Hazard-Pointer Recording and Clearing
        #+begin_src c
static inline void *_h_t_r_impl(void **p,
                                hazard_pointer *hp)
{
    void *tmp;

    tmp = READ_ONCE(*p);
    if (!tmp || tmp == (void *)HAZPTR_POISON)
        return tmp;
    WRITE_ONCE(hp->p, tmp);
    smp_mb();
    if (tmp == READ_ONCE(*p))
        return tmp;
    return (void *)HAZPTR_POISON;
}

#define hp_try_record(p, hp) _h_t_r_impl((void **)(p), hp)

static inline void *hp_record(void **p,
                              hazard_pointer *hp)
{
    void *tmp;

    do {
        tmp = hp_try_record(p, hp);
    } while (tmp == (void *)HAZPTR_POISON);
    return tmp;
}


static inline void hp_clear(hazard_pointer *hp)
{
    smp_mb();
    WRITE_ONCE(hp->p, NULL);
}
        #+end_src

        The ~hp_try_record()~ macro on line 16 is simply a casting wrapper for the ~_h_t_r_impl()~ function, which
        attempts to store the pointer referenced by ~p~ into the hazard pointer referenced by ~hp~. If successful,
        it returns the value of the stored pointer. If it fails due to that pointer being ~NULL~, it returns
        ~NULL~. Finally, if it fails due to racing with an update, it returns a special ~HAZPTR_POISON~ token.

        #+ATTR_LATEX: :options []
        #+BEGIN_remark
        Given that papers on hazard pointers use the bottom bits of each pointer to mark deleted elements,
        what is up with ~HAZPTR_POISON~?
        #+END_remark

        Line 6 reads the pointer to the object to be protected. If line 8 finds that this pointer was either
        ~NULL~ or the special ~HAZPTR_POISON~ deleted-object token, it returns the pointer’s value to inform the
        caller of the failure. Otherwise, line 9 stores the pointer into the specified hazard pointer, and
        line 10 forces full ordering of that store with the reload of the original pointer on line 11. If the
        value of the original pointer has not changed, then the hazard pointer protects the pointed-to object,
        and in that case, line 12 returns a pointer to that object, which also indicates success to the
        caller. Otherwise, if the pointer changed between the two ~READ_ONCE()~ invocations, line 13 indicates
        failure.

        The ~hp_clear()~ function is even more straightforward, with an ~smp_mb()~ to force full ordering between
        the caller’s uses of the object protected by the hazard pointer and the setting of the hazard pointer
        to ~NULL~.

        Once a hazard-pointer-protected object has been removed from its linked data structure, so that it is
        now inaccessible to future hazard-pointer readers, it is passed to ~hazptr_free_later()~, which is shown
        on lines 48–56 of Listing ref:l9.5. Lines 50 and 51 enqueue the object on a per-thread list rlist and
        line 52 counts the object in rcount. If line 53 sees that a sufficiently large number of objects are
        now queued, line 54 invokes ~hazptr_scan()~ to attempt to free some of them.

        #+NAME: l9.5
        #+CAPTION: Hazard-Pointer Scanning and Freeing
        #+begin_src c
int compare(const void *a, const void *b)
{
    return ( *(hazptr_head_t **)a - *(hazptr_head_t **)b );
}

void hazptr_scan()
{
    hazptr_head_t *cur;
    int i;
    hazptr_head_t *tmplist;
    hazptr_head_t **plist = gplist;
    unsigned long psize;

    if (plist == NULL) {
        psize = sizeof(hazptr_head_t *) * K * NR_THREADS;
        plist = (hazptr_head_t **)malloc(psize);
        BUG_ON(!plist);
        gplist = plist;
    }
    smp_mb();
    psize = 0;
    for (i = 0; i < H; i++) {
        uintptr_t hp = (uintptr_t)READ_ONCE(HP[i].p);

        if (!hp)
            continue;
        plist[psize++] = (hazptr_head_t *)(hp & ~0x1UL);

    }
    smp_mb();
    qsort(plist, psize, sizeof(hazptr_head_t *), compare);
    tmplist = rlist;
    rlist = NULL;
    rcount = 0;
    while (tmplist != NULL) {
        cur = tmplist;
        tmplist = tmplist->next;
        if (bsearch(&cur, plist, psize,
                    sizeof(hazptr_head_t *), compare)) {
            cur->next = rlist;
            rlist = cur;
            rcount++;
        } else {
            hazptr_free(cur);
        }
    }
}

void hazptr_free_later(hazptr_head_t *n)
{
    n->next = rlist;
    rlist = n;
    rcount++;
    if (rcount >= R) {
        hazptr_scan();
    }
}
        #+end_src


        The ~hazptr_scan()~ function is shown on lines 6–46 of the listing. This function relies on a fixed
        maximum number of threads (~NR_THREADS~) and a fixed maximum number of hazard pointers per thread (~K~),
        which allows a fixed-size array of hazard pointers to be used. Because any thread might need to scan
        the hazard pointers, each thread maintains its own array, which is referenced by the per-thread
        variable ~gplist~. If line 14 determines that this thread has not yet allocated its gplist, lines 15–18
        carry out the allocation. The memory barrier on line 20 ensures that _all threads see the removal of
        all objects by this thread_ before lines 22–28 scan all of the hazard pointers, accumulating non-~NULL~
        pointers into the ~plist~ array and counting them in ~psize~. The memory barrier on line 29 ensures that
        the reads of the hazard pointers happen before any objects are freed. Line 30 then sorts this array to
        enable use of binary search below.

        Lines 31 and 32 remove all elements from this thread’s list of to-be-freed objects, placing them on
        the local tmplist and line 33 zeroes the count. Each pass through the loop spanning lines 34–45
        processes each of the to-be-freed objects. Lines 35 and 36 remove the first object from tmplist, and
        if lines 37 and 38 determine that there is a hazard pointer protecting this object, lines 39–41 place
        it back onto rlist. Otherwise, line 43 frees the object.

        The Pre-BSD routing example can use hazard pointers
as shown in Listing 9.6 for data structures and route_
lookup(), and in Listing 9.7 for route_add() and
route_del() (route_hazptr.c). As with reference
counting, the hazard-pointers implementation is quite sim-
ilar to the sequential algorithm shown in Listing 9.1 on
page 130, so only differences will be discussed.
Starting with Listing 9.6, line 2 shows the ->hh field
used to queue objects pending hazard-pointer free, line 6
shows the ->re_freed field used to detect use-after-free
bugs, and line 21 invokes hp_try_record() to attempt
to acquire a hazard pointer. If the return value is NULL,
line 23 returns a not-found indication to the caller. If the
call to hp_try_record() raced with deletion, line 25
branches back to line 18’s retry to re-traverse the list
from the beginning. The do–while loop falls through
when the desired element is located, but if this element
has already been freed, line 29 terminates the program.
* Appendices :ignore:
#+LATEX: \appendix
** Why Memory Barriers
*** Cache Structure
    #+ATTR_LATEX: :width .7\textwidth :float nil
    #+NAME:
    #+CAPTION: Modern Computer System Cache Structure
    [[../images/perfbook/1.png]]

    Data flows among the CPUs’ caches and memory in fixed-length blocks called “cache lines”, which
    are normally a power of two in size, ranging from 16 to 256 bytes. When a given data item is
    first accessed by a given CPU, it will be absent from that CPU’s cache, meaning that a “cache
    miss” (or, more specifically, a “startup” or “warmup” cache miss) has occurred. The cache miss
    means that the CPU will have to wait (or be “stalled”) for hundreds of cycles while the item is
    fetched from memory. However, the item will be loaded into that CPU’s cache, so that subsequent
    accesses will find it in the cache and therefore run at full speed.

    #+ATTR_LATEX: :width .8\textwidth :float nil
    #+NAME:
    #+CAPTION: CPU Cache Structure
    [[../images/perfbook/2.png]]

    This cache has sixteen “sets” and two “ways” for a total of 32 “lines”, each entry containing a
    single 256-byte “cache line”, which is a 256-byte-aligned block of memory.

    Each box corresponds to a cache entry, which can contain a 256-byte cache line. Since the cache
    lines must be 256-byte aligned, the low eight bits of each address are zero, and the choice of
    hardware hash function means that the next-higher four bits match the hash line number.

    What happens when it does a write? Because it is important that all CPUs agree on the value of a
    given data item, before a given CPU writes to that data item, it must first cause it to be
    removed, or “invalidated”, from other CPUs’ caches. Once this invalidation has completed, the
    CPU may safely modify the data item. If the data item was present in this CPU’s cache, but was
    read-only, this process is termed a “write miss”. Once a given CPU has completed invalidating a
    given data item from other CPUs’ caches, that CPU may repeatedly write (and read) that data
    item.

    Later, if one of the other CPUs attempts to access the data item, it will incur a cache miss,
    this time because the first CPU invalidated the item in order to write to it. This type of cache
    miss is termed a “communication miss”, since it is usually due to several CPUs using the data
    items to communicate (for example, a lock is a data item that is used to communicate among CPUs
    using a mutual-exclusion algorithm).

    Clearly, much care must be taken to ensure that all CPUs maintain a coherent view of the data.
    With all this fetching, invalidating, and writing, it is easy to imagine data being lost or
    (perhaps worse) different CPUs having conflicting values for the same data item in their
    respective caches.
*** Cache-Coherence Protocols
**** MESI States
    MESI stands for "modified", "exclusive", "shared", and "invalid", the four states a given cache
    line can take on using this protocol.
